<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>c123ian.github.io - Finetuning the phi-1.5 Language Model üìù</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">c123ian.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title"><strong>Finetuning the phi-1.5 Language Model</strong> üìù</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Fine-tuning a model, especially one as complex as phi-1.5, is an intricate endeavor demanding a judicious blend of techniques, optimizations, and a profound grasp of the model‚Äôs architecture. As illustrated in this notebook, each phase‚Äîfrom environment setup to the utilization of the fine-tuned model‚Äîis a delicate dance of precision and understanding.</p>
<p>This journey was driven by the <a href="https://www.kaggle.com/competitions/kaggle-llm-science-exam">Kaggle Science Exam competition</a>, where the challenge was to enable a language model to answer multiple-choice questions. These questions were intriguingly generated by GPT 3.5 from a mix of STEM and History Wikipedia articles, offering a deep dive into the realms of synthetic data and knowledge distillation.</p>
<p>My model of choice was <a href="https://huggingface.co/microsoft/phi-1_5">phi-1.5</a>. With only 1.3 billion parameters, it stands as a compact yet potent transformer. Its allure was further amplified by its unique training methodology using synthetic ‚Äútextbooks‚Äù, which embodies the essence of a perfect ‚Äútextbook‚Äù as described in the <a href="https://arxiv.org/pdf/2306.11644.pdf">Textbooks all you need</a> research.</p>
<p>My strategy was multifaceted:</p>
<ol type="1">
<li>Substitute synthetic textbooks or Wikipedia articles with informative articles from Khan Academy.</li>
<li>Harness the GPT 3.5 API to mold MCQs from a sample of 100 Khan Academy ‚Äútextbooks.‚Äù</li>
<li>Fine-tune phi 1_5 using this dataset to craft MCQs from the remaining textbooks.</li>
<li>Employ another LLM to respond to MCQs using <a href="https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-1">the open-book method</a>.</li>
</ol>
<p>However, challenges arose, particularly at step 3. The limitations, I conjecture, were twofold: the constrained capacity of the Phi 1_ model and the limited training dataset. A larger model, like <a href="https://huggingface.co/docs/transformers/main/en/model_doc/mistral">Mistral 7B</a>, might have been a more potent alternative.</p>
<p>With the aid of libraries like transformers and methodologies such as LoRA, the phi-1.5 model‚Äôs fine-tuning capabilities offer a promising path for NLP tasks, be it in research or real-world applications. This endeavor, though challenging, was enriched by insights from contributors like <a href="https://github.com/Vasanthengineer4949/NLP-Projects-NHV">Vasanthengineer</a>, whose <a href="https://m.youtube.com/watch?v=R8CKx5yNEDo">video tutorial</a> proved invaluable.</p>
<p>Now, let‚Äôs delve into the process of finetuning Phi 1_5 for generating MCQs based on a textbook context.</p>
<hr>
<p>This adjusted conclusion should provide a cohesive recap and set the stage for the rest of the content.</p>
<hr>
<section id="installing-dependencies" class="level3">
<h3 class="anchored" data-anchor-id="installing-dependencies"><strong>Installing Dependencies</strong></h3>
<p>To begin, we need to install a few Python packages which are crucial for our finetuning process:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> pip install accelerate transformers einops datasets peft bitsandbytes</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p>Once the required packages are installed, the next step is to import them:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> notebook_login</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>notebook_login()</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, Dataset</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here, we are using the Hugging Face‚Äôs <code>transformers</code> library for language model finetuning and <code>datasets</code> for data handling.</p>
<hr>
</section>
<section id="setting-up-the-tokenizer" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-the-tokenizer"><strong>Setting Up the Tokenizer</strong></h3>
<p>Before we can start training, we need to set up the tokenizer:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"microsoft/phi-1_5"</span>, trust_remote_code<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The tokenizer will be responsible for converting our text data into tokens that the model can understand.</p>
<hr>
</section>
<section id="configuring-the-model" class="level3">
<h3 class="anchored" data-anchor-id="configuring-the-model"><strong>Configuring the Model</strong></h3>
<p>The phi-1.5 model leverages the <code>BitsAndBytesConfig</code> for efficient quantization:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>bnb_config <span class="op">=</span> BitsAndBytesConfig(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_use_double_quant<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_quant_type<span class="op">=</span><span class="st">"nf4"</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_compute_dtype<span class="op">=</span>torch.float16</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"microsoft/phi-1_5"</span>,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span>{<span class="st">""</span>:<span class="dv">0</span>},</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    trust_remote_code<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    quantization_config<span class="op">=</span>bnb_config</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This configuration ensures the model uses less memory during training, making it more efficient. Lets dicuss the above code in a little more detail.</p>
<p>Alright! Let‚Äôs delve deeper into the more technical aspects.</p>
<hr>
</section>
<section id="device-map" class="level3">
<h3 class="anchored" data-anchor-id="device-map"><strong>Device Map</strong></h3>
<p>The Phi 1_5 model does not currently support <code>device_map="auto"</code>. If you‚Äôre using just one GPU for a model, you don‚Äôt necessarily need a device map since the entire model will reside on that single GPU. However, if you still want to set up a device map for one GPU, you can specify all the layers to reside on GPU 0 <code>device_map={"":0</code>.</p>
</section>
<section id="bitsandbytes" class="level3">
<h3 class="anchored" data-anchor-id="bitsandbytes"><strong>BitsAndBytes</strong></h3>
<p>The <code>bitsandbytes</code> library provides a lightweight wrapper around CUDA custom functions, especially 8-bit optimizers, matrix multiplication (<code>LLM.int8()</code>), and quantization functions. This is crucial for memory efficiency during training, allowing for faster computations.</p>
<p>Here‚Äôs how it‚Äôs configured for the phi-1.5 model:</p>
<ul>
<li><code>load_in_4bit</code>: Loads the model in 4-bit precision.</li>
<li><code>bnb_4bit_use_double_quant</code>: Allows for using double quantization.</li>
<li><code>bnb_4bit_quant_type</code>: Specifies the type of quantization, in this case, ‚Äúnf4‚Äù.</li>
<li><code>bnb_4bit_compute_dtype</code>: Sets the data type for computation to half-precision floating point (float16).</li>
</ul>
<hr>
</section>
<section id="finding-target-modules-for-lora" class="level3">
<h3 class="anchored" data-anchor-id="finding-target-modules-for-lora"><strong>Finding Target Modules for LoRA</strong></h3>
<p>LoRA (Layer-wise Relevance Analysis) is a technique used to enhance and adapt specific layers of a neural network. By attaching LoRA to layers, you can introduce low-rank adaptations to them, enabling the model to better capture task-specific information without retraining the entire network. This makes fine-tuning more efficient and targeted.</p>
<p>An essential aspect of LoRA is determining which layers or modules of the model should be targeted. Instead of printing the entire model to find these, there‚Äôs a handy function (which I got from <a href="https://www.reddit.com/r/LocalLLaMA/comments/15sgg4m/what_modules_should_i_target_when_training_using/">here</a>) below:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_target_modules(model):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize a Set to Store Unique Layers</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    unique_layers <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate Over All Named Modules in the Model</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, module <span class="kw">in</span> model.named_modules():</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if the Module Type Contains 'Linear8bitLt' or 'Linear4bit'</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        module_type <span class="op">=</span> <span class="bu">str</span>(<span class="bu">type</span>(module))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">"Linear8bitLt"</span> <span class="kw">in</span> module_type <span class="kw">or</span> <span class="st">"Linear4bit"</span> <span class="kw">in</span> module_type:</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract the Type of the Layer</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>            layer_type <span class="op">=</span> name.split(<span class="st">'.'</span>)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Add the Layer Type to the Set of Unique Layers</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            unique_layers.add(layer_type)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the Set of Unique Layers Converted to a List</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">list</span>(unique_layers)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>target_modules <span class="op">=</span> find_target_modules(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>find_target_modules(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>['Wqkv', 'fc2', 'out_proj', 'fc1']</code></pre>
<p>The function extracts modules of type <code>Linear8bitLt</code> or <code>Linear4bit</code> which are likely the primary focus for LoRA.</p>
<p>Either using the <code>find_target_modules</code> function above, or simple printing the model <code>print(model)</code> to display the models architecture:</p>

<table border="1" cellspacing="0" cellpadding="5" style="border-collapse: collapse; border: 1px solid black;">
<thead>
<tr>
<th style="border: 1px solid black;">
Component
</th>
<th style="border: 1px solid black;">
Description
</th>
<th style="border: 1px solid black;">
Belongs to
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="border: 1px solid black;">
<code>MHA (Multi-Head Attention)</code>
</td>
<td style="border: 1px solid black;">
This is essentially a mixer. The inclusion of <code>RotaryEmbedding()</code> suggests that this component can scale effectively.
</td>
<td style="border: 1px solid black;">
-
</td>
</tr>
<tr>
<td style="border: 1px solid black;">
<code>Embedding Layer</code>
</td>
<td style="border: 1px solid black;">
Converts discrete tokens into continuous embeddings, with dropout for regularization.
</td>
<td style="border: 1px solid black;">
-
</td>
</tr>
<tr>
<td style="border: 1px solid black;">
<code>LayerNorm</code>
</td>
<td style="border: 1px solid black;">
Stabilizes layer activations.
</td>
<td style="border: 1px solid black;">
ParallelBlock
</td>
</tr>
<tr>
<td style="border: 1px solid black;">
<code>resid_dropout</code>
</td>
<td style="border: 1px solid black;">
Dropout on residual connections to combat overfitting.
</td>
<td style="border: 1px solid black;">
ParallelBlock
</td>
</tr>
<tr>
<td style="border: 1px solid black;">
<code>MHA</code>
</td>
<td style="border: 1px solid black;">
Weighs the significance of various positions in an input sequence. Uses rotary embeddings and linear transformations.
</td>
<td style="border: 1px solid black;">
ParallelBlock
</td>
</tr>
<tr>
<td style="border: 1px solid black;">
<code>out_proj</code>
</td>
<td style="border: 1px solid black;">
A linear transformation post-MHA.
</td>
<td style="border: 1px solid black;">
ParallelBlock
</td>
</tr>
<tr>
<td style="border: 1px solid black;">
<code>inner_attn</code>
</td>
<td style="border: 1px solid black;">
Self-attention within MHA.
</td>
<td style="border: 1px solid black;">
ParallelBlock
</td>
</tr>
<tr>
<td style="border: 1px solid black;">
<code>inner_cross_attn</code>
</td>
<td style="border: 1px solid black;">
Cross-attention within MHA.
</td>
<td style="border: 1px solid black;">
ParallelBlock
</td>
</tr>
<tr>
<td style="border: 1px solid black;">
<code>MLP (Multi-Layer Perceptron)</code>
</td>
<td style="border: 1px solid black;">
Contains two dense layers and uses the GELU activation function. It processes the data post-MHA.
</td>
<td style="border: 1px solid black;">
-
</td>
</tr>
</tbody>

</table>
<hr>
</section>
<section id="configuring-lora" class="level3">
<h3 class="anchored" data-anchor-id="configuring-lora"><strong>Configuring LoRA</strong></h3>
<p>With the target modules identified above, we can set up the LoRA configuration:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> LoraConfig(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"Wqkv"</span>, <span class="st">"out_proj"</span>],</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span><span class="st">"CAUSAL_LM"</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, config)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>model.print_trainable_parameters()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For our target modules will be will use weights of key value (<code>Wqkv</code>) and <code>out_proj</code> but, we could experiment with the addition of <code>inner_attn</code> and <code>inn_cross_attn</code>. If you‚Äôre incorporating LoRA, ensure it‚Äôs applied across all layers, not just to the Key and Value matrices (<code>Wqkv</code>), to maximize model performance <a href="https://lightning.ai/pages/community/lora-insights/?utm_medium=social&amp;utm_source=twitter&amp;utm_campaign=Education_10132023">(Raschka, 2023)</a>.</p>
<blockquote class="blockquote">
<p>The description of this LoRA is very careful to not state that the output is objectively better than not using LoRA, because everything is subjective and there are use cases where vibrant output is not desired. For most use cases, the output should be better desired however (<a href="https://huggingface.co/minimaxir/sdxl-wrong-lora">source</a>).</p>
</blockquote>
<p>Here‚Äôs a breakdown of the LorA parameters:</p>
<ul>
<li><code>r</code>: The rank for low-rank matrices. Reducing it can result in a loss of information, but in some cases, we might not need all of it.</li>
<li>Great rank explanation (in the context of stable diffusion) <a href="https://github.com/cloneofsimo/lora/discussions/37">here</a>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Updated_Phi_blog_files/figure-html/485f8f0c-58f9-4447-9db8-f9561fd6616b-1-aa481f05-5e44-47f4-8df6-0e62759ed7a2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Adjusting r value for images</figcaption>
</figure>
</div>
<ul>
<li><code>lora_alpha</code>: A scaling factor for the weight matrices. A higher value assigns more weight to the LoRA activations.</li>
</ul>
<p>Adjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank‚Äôs value <a href="https://lightning.ai/pages/community/lora-insights/?utm_medium=social&amp;utm_source=twitter&amp;utm_campaign=Education_10132023">(Raschka, 2023)</a>.</p>
<ul>
<li><code>target_modules</code>: Specifies the modules to be targeted by LoRA.</li>
<li><code>lora_dropout</code>: Dropout probability for the LoRA layers.</li>
<li><code>bias</code>: Specifies which bias parameters to train. Recommendations suggest starting with <code>None</code>.</li>
<li><code>task_type</code>: Specifies the type of task, in this case, ‚ÄúCAUSAL_LM‚Äù (Causal Language Modeling).</li>
</ul>
</section>
<section id="custom-dataset-number-1" class="level3">
<h3 class="anchored" data-anchor-id="custom-dataset-number-1"><strong>Custom dataset number 1:</strong></h3>
<p>Lets import the first dataset, GPT 3.5 geneated MCQ based on 200 Khan academy contexts or ‚Äútextbooks‚Äù.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"c123ian/khan_academy_200"</span>) <span class="co"># generate using gpt 3.5 on pulled Khan academy lessons</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>dataset</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['context', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer'],
        num_rows: 125
    })
})</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dataset[<span class="st">'train'</span>][<span class="dv">0</span>])  <span class="co"># This will print the first row of the train dataset.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>{'context': "Hi, I'm Larry Joe!  What do you work on? I am a Molecular Biologist, working in a lab at the University of California, Berkeley.  Our lab in interested in extending lifespans and preventing age-related diseases.  In my previous job I worked for a Bay Area biotechnology company where I developed DNA analysis instruments and reagents for forensic and agricultural purposes.  Some of my products can be seen on ‚ÄúCSI‚Äù shows or movies. My favorite project there was participating in the gray wolf restoration project in Yellowstone Park.  There is a delicate balance of animal species in the park.  Wolves had been hunted to extinction in Yellowstone.  The loss of the predator disrupted that balance and negatively impacted the other animal populations.  Twenty-one wolves were transported from Canada to Yellowstone.  We analyzed the DNA of the founding members and their offspring to confirm the field biologists‚Äô observations on wolf family behavior as the wolves re-populated the park.  The project was a big success as the wolves continue to thrive there today, and the ecosystem balance has recovered. Larry in Yellowstone How did you get interested in science, and what did you study? When I was a kid, I loved animals and science.  I always wanted to visit the Oakland or San Francisco Zoos, the Exploratorium, Chabot Observatory, or the Academy of Sciences.  I also liked building things and figuring out how things work.  I studied microbiology and immunology in college.  I thought I might pursue a career in one of the health professions, but in the end, I enjoyed conducting experiments as a scientist. What do you do in your free time? My main hobbies are traveling, hiking, wood working, reading, watching movies &amp; nature shows, framing pictures, and gardening.  I also like to play and watch baseball.  My nephew and I have a goal to visit all of the Major League Baseball stadiums. What‚Äôs your one piece of advice for people interested in biology? I have two pieces of advice:1: Don‚Äôt be afraid to ask a lot of questions.  I like it when students ask me questions, for it shows me that they are interested in the project and it gives me confidence that they can execute the experimental protocol.2: Try to find a mentor.  Mentors can be great resources for a scientist as you progress through your career.  They can help prepare you for various situations and provide insight on what to expect in a given job.  Many scientists enjoy the mentoring aspect of the job, including myself.", 'prompt': 'Which of the following hobbies does Larry Joe mention in his free time?', 'A': 'Traveling', 'B': 'Painting', 'C': 'Cooking', 'D': 'Playing chess', 'E': 'Knitting', 'answer': 'A'}</code></pre>
</section>
<section id="targeting-the-mcq-generation" class="level3">
<h3 class="anchored" data-anchor-id="targeting-the-mcq-generation"><strong>Targeting the MCQ generation</strong></h3>
<p>Use the preprocessing function to convert it to source-target pairs:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_for_mcq(example):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    source <span class="op">=</span> example[<span class="st">'context'</span>] <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> example[<span class="st">'prompt'</span>]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    target <span class="op">=</span> example[<span class="st">'prompt'</span>] <span class="op">+</span> <span class="st">" A) "</span> <span class="op">+</span> example[<span class="st">'A'</span>] <span class="op">+</span> <span class="st">" B) "</span> <span class="op">+</span> example[<span class="st">'B'</span>] <span class="op">+</span> <span class="st">" C) "</span> <span class="op">+</span> example[<span class="st">'C'</span>] <span class="op">+</span> <span class="st">" D) "</span> <span class="op">+</span> example[<span class="st">'D'</span>] <span class="op">+</span> <span class="st">" E) "</span> <span class="op">+</span> example[<span class="st">'E'</span>] <span class="op">+</span> <span class="st">" Answer: "</span> <span class="op">+</span> example[<span class="st">'answer'</span>]</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"source"</span>: source, <span class="st">"target"</span>: target}</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>processed_dataset <span class="op">=</span> dataset.<span class="bu">map</span>(preprocess_for_mcq)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(processed_dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['context', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'source', 'target'],
        num_rows: 125
    })
})</code></pre>
<p>So the source will be Khan academy, and the target will be a MCQ generated from the source articles</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Source:"</span>, processed_dataset[<span class="st">"train"</span>][<span class="st">"source"</span>][<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Source[0]: Taj Mahal, Agra, India, 1632‚Äì53 (photo: King of Hearts, CC BY-SA 4.0)Taj Mahal, Agra, India, 1632‚Äì53 (photo: King of Hearts, CC BY-SA 4.0) Taj Mahal, Agra, India, 1632‚Äì53 (photo: King of Hearts, CC BY-SA 4.0) Shah Jahan was the fifth ruler of the Mughal dynasty. During his third regnal year, his favorite wife, known as Mumtaz Mahal, died due to complications arising from the birth of their fourteenth child.  Deeply saddened, the emperor started planning the construction of a suitable, permanent resting place for his beloved wife almost immediately. The result of his efforts.....</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Target:"</span>, processed_dataset[<span class="st">"train"</span>][<span class="st">"target"</span>][<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Target[0]: What is the main reason Shah Jahan built the Taj Mahal? A) To honor his beloved wife, Mumtaz Mahal B) To establish his dominance as the ruler of the Mughal dynasty C) To showcase his wealth and power to the world D) To create a luxurious resting place for himself E) To preserve the Mughal architectural tradition  Answer: A</code></pre>
</section>
<section id="tokensiation" class="level3">
<h3 class="anchored" data-anchor-id="tokensiation"><strong>Tokensiation:</strong></h3>
<p>Due to the fact that the tokenizer returns PyTorch tensors, and when using batched=True with the map function from the datasets library, it expects the function to return lists or numpy arrays, not tensors.</p>
<p>To resolve this, you can convert the tensors to lists within the tokenize function. Here‚Äôs how you can adjust the tokenize function to avoid an error:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_and_convert_to_list(example):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    model_inputs <span class="op">=</span> tokenizer(example[<span class="st">'source'</span>], padding<span class="op">=</span><span class="st">'max_length'</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">512</span>, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert tensors to lists</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key, value <span class="kw">in</span> model_inputs.items():</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        model_inputs[key] <span class="op">=</span> value.tolist()</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model_inputs</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Since we can't execute the map function in this environment, we'll just return the function definition</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>tokenize_and_convert_to_list</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now we tokenize the dataset:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(example):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(example[<span class="st">'source'</span>], padding<span class="op">=</span><span class="st">'max_length'</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">512</span>, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> processed_dataset.<span class="bu">map</span>(tokenize_and_convert_to_list, batched<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['context', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'source', 'target', 'input_ids', 'attention_mask'],
        num_rows: 125
    })
})</code></pre>
<p>Notice we now have context (the articles) saved to <code>source</code>and combined the prompt (the question) and subsequent answer options and correct answer into <code>target</code> column.</p>
</section>
<section id="attention-masks" class="level3">
<h3 class="anchored" data-anchor-id="attention-masks"><strong>Attention Masks</strong></h3>
<p>If we print <code>tokenized_dataset</code> we can see <code>attention_mask</code>, which aren‚Äôt supported during trainin. Attention masks are typically used in transformers to specify which tokens should be attended to and which should be ignored. This is especially important for sequences of different lengths, to ensure padding tokens don‚Äôt influence the model‚Äôs output.</p>
<p>However, while finetuning I constantly got the below error:</p>
<p><code>Attention mask is not supported during training. Using it might lead to unexpected results.</code></p>
<p>The problem here is that phi-1.5 was pre-trained without padding and the implementation of ‚ÄúMixFormerSequentialForCausalLM‚Äù released by Microsoft with the model doesn‚Äôt support attention masking during training.</p>
<blockquote class="blockquote">
<p>In the generation function, our model currently does not support beam search (num_beams &gt;1) and ‚Äúattention_mask‚Äù parameters. Furthermore, in the forward pass of the model, we currently do not support outputing hidden states or attention values, or using custom input embeddings (instead of the model‚Äôs).</p>
</blockquote>
<p>In the <code>Trainer</code>, set <code>mlm=False</code> to prevent masked language modelling, though the warning still appeared during finetuning!</p>
</section>
<section id="fine-tuning-the-model-for-mcq-generation" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-the-model-for-mcq-generation"><strong>Fine-tuning the Model for MCQ Generation</strong></h3>
<p>Once the data is prepared and the model is set up, the next step is to fine-tune the model on the data. This typically involves specifying training parameters, such as:</p>

<style>
    table {
        border-collapse: collapse;
        width: 100%;
    }

    table, th, td {
        border: 1px solid black;
    }

    th, td {
        padding: 8px;
        text-align: left;
    }
</style>
<table>
<tbody><tr>
<th>
Parameter
</th>
<th>
Description
</th>
</tr>
<tr>
<td>
Learning rate
</td>
<td>
Smaller learning rates might lead to more stable training but might require more training steps to converge. <a href="https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.2-learning-rates-and-learning-rate-schedulers/">Learn more</a>.
</td>
</tr>
<tr>
<td>
Batch size
</td>
<td>
You can experiment with different batch sizes (<code>per_device_train_batch_size</code>). Smaller batch sizes can sometimes lead to more stable training, but they may require more training steps.
</td>
</tr>
<tr>
<td>
Number of epochs
</td>
<td>
For static datasets, iterating multiple times as done in multi-epoch training might not be beneficial. It often deteriorates the results, probably due to overfitting <a href="https://lightning.ai/pages/community/lora-insights/?utm_medium=social&amp;utm_source=twitter&amp;utm_campaign=Education_10132023">[(Raschka, 2023)]</a>.
</td>
</tr>
<tr>
<td>
Optimizer
</td>
<td>
When finetuning LLMs, the choice of optimizer shouldn‚Äôt be a major concern. While Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn‚Äôt significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters <a href="https://lightning.ai/pages/community/lora-insights/?utm_medium=social&amp;utm_source=twitter&amp;utm_campaign=Education_10132023">[(Raschka, 2023)]</a>.
</td>
</tr>
<tr>
<td>
Monitor Performance
</td>
<td>
Will use Weights &amp; Bias to monitor performance. You can view my run at <a href="https://wandb.ai/c123ian/phi_target_training/runs/sxm5030n">here</a>.
</td>
</tr>

</tbody></table>
<hr>
<p>We download the phi 1_5 tokenizer again:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"microsoft/phi-1_5"</span>, trust_remote_code<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Recall, the <code>bitsandbytes</code> is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions.</p>
<p>Also, recall that the Phi model does not support <code>device_map=auto</code> so we manually set <code>device_map={"":0},</code> meaning, load tensors to CUDA 0 (THe only GPU I am running this notebook on via Kaggle)</p>
<p>More resources here:</p>
<ul>
<li><a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">LLM.int8() and Emergent Features (Tim Dettemers)</a></li>
<li><a href="https://huggingface.co/blog/hf-bitsandbytes-integration">Software Blog post</a></li>
</ul>
<p>The <code>transformers</code> library provides a <code>Trainer</code> class, which simplifies the training process. You would specify the model, the tokenized data, and the training arguments to this class, and then call its <code>train</code> method to start the fine-tuning. In the <code>Trainer</code>below, we set <code>mlm=False</code> to prevent masked language modelling.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>training_arguments <span class="op">=</span> TrainingArguments(</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>        output_dir<span class="op">=</span><span class="st">"."</span>,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        evaluation_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        eval_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        per_device_train_batch_size<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        gradient_accumulation_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">3e-4</span>, <span class="co">#2e-4,</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        lr_scheduler_type<span class="op">=</span><span class="st">"cosine"</span>,</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        logging_first_step<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        logging_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        max_steps<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        num_train_epochs<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        optim<span class="op">=</span><span class="st">"adamw_bnb_8bit"</span>,</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        report_to<span class="op">=</span><span class="st">"wandb"</span>,</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        push_to_hub<span class="op">=</span><span class="va">True</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized_data,</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_arguments,</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>DataCollatorForLanguageModeling(tokenizer, mlm<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>repo_id <span class="op">=</span> <span class="ss">f'c123ian/phi_test_target'</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>trainer.model.push_to_hub(repo_id)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>tokenizer.push_to_hub(repo_id)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="saving-merging-and-inference" class="level3">
<h3 class="anchored" data-anchor-id="saving-merging-and-inference"><strong>Saving, Merging, and Inference</strong></h3>
<p>For faster inference, can merge LorA layers with base. The merged model calls both the base model <code>config.json</code> file and the LoRA modules <code>adapter_config.json</code>, instead of just the <code>adapter_module</code>.</p>
<p>For inference, certain configurations like <code>return_tensors="pt"</code> and turning off attention masks with <code>return_attention_mask=False</code> are used to get predictions from the model.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> PeftModel</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(<span class="st">"microsoft/phi-1_5"</span>, trust_remote_code<span class="op">=</span><span class="va">True</span>, torch_dtype<span class="op">=</span>torch.float32) <span class="co"># base model 32 NOT 16!</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>peft_model <span class="op">=</span> PeftModel.from_pretrained(model, <span class="st">"c123ian/phi_test_target"</span>, from_transformers<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> peft_model.merge_and_unload()</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">#model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then we push the new merged model to the hub:</p>
<pre class="oython"><code>model.push_to_hub("c123ian/phi_test_target") # push merged model</code></pre>
<p>Now that we have merged the models, it will call base model <code>config.json</code> file and the LoRA modules <code>adapter_config.json</code>, rather then just the <code>adapter_module</code>.</p>
<hr>
</section>
<section id="inference-generating-mcq-from-textbooks-test" class="level3">
<h3 class="anchored" data-anchor-id="inference-generating-mcq-from-textbooks-test"><strong>Inference: Generating MCQ from Textbooks Test:</strong></h3>
<p>So next lets load the new mereged model, finetuned on MCQ 200 sample dataset, and the original phi 1_5 tokens:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model and tokenizer</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(<span class="st">"c123ian/phi_test_target"</span>, trust_remote_code<span class="op">=</span><span class="va">True</span>, torch_dtype<span class="op">=</span>torch.float32)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"microsoft/phi-1_5"</span>, trust_remote_code<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Lets do a quick test:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the input context</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>context <span class="op">=</span> <span class="st">'''RMS Titanic was a British passenger liner, operated by the White Star Line, that sank in the North Atlantic Ocean on 15 April 1912 after striking an iceberg during her maiden voyage from Southampton, England, to New York City, United States. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making it the deadliest sinking of a single ship up to that time. '''</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the instruction for generating an MCQ</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>instruction <span class="op">=</span> <span class="st">'''"Based on the provided text, create a multiple-choice question with 5 options labeled A), B), C), D), and E), and provide the correct answer. Ensure that the answer options are of similar length and closely aligned wording."'''</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize the input</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(instruction <span class="op">+</span> context, return_tensors<span class="op">=</span><span class="st">"pt"</span>, return_attention_mask<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the output</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs, max_length<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> tokenizer.batch_decode(outputs)[<span class="dv">0</span>]</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>"Based on the provided text, create a multiple-choice question with 5 options labeled A), B), C), D), and E), and provide the correct answer. Ensure that the answer options are of similar length and closely aligned wording."RMS Titanic was a British passenger liner, operated by the White Star Line, that sank in the North Atlantic Ocean on 15 April 1912 after striking an iceberg during her maiden voyage from Southampton, England, to New York City, United States. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making it the deadliest sinking of a single ship up to that time. 
Answer: The question is labeled A), B), C), D), and E). The correct answer is D). The question is labeled A), B), C), D), and E), and the correct answer is D). The correct answer is dead. The questions and answers provided in this lesson are not vetted by VB. If you need more questions, or would like to learn more about this lesson, please contact us. Using VB. Add labels to the question "The Titanic was a ‚Ä¶" label, A), B), C), D), and E), and the correct answer is labeled E). The question is labeled A), B), C), D), and E), and the correct answer is E). Thank you for using VB. Our lessons We start with the tragic Titanic, as mentioned in the opening paragraph. As we've seen before, the Titanic was a passenger liner (not an ferry [boat]) operated by the White Star Line (a company based in Southampton, England); it sank in the North Atlantic Ocean (in New York City, United States) in 1912 after striking an iceberg during its first voyage from Southampton (to New York).  Of the estimated 1,224 passengers and crew aboard, more than 1,500 died; the Titanic is one of the most tragic (and deadliest) shipping disasters up to that time. 
Question: What was the object of the ship the text describes in this lesson?
Answer: The objects of the text describe "the Titanic." (B) The objects of the text describe "the Titanic," and the correct answer is
Question: According to the text, what was one "feature" of the Titanic that was "not fully understood at the design stage?," B), C), D), and E), and the correct answer is E)? The question is labeled A), B), C), D), and E), and</code></pre>
<p>Hmmm.. I can sort of see the attempt to generate MCQ. I also tried to instill Alpaca style formatting for the instruction - to try and standerdize the MCQ <code>text</code> generation.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_instruction(sample):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ss">f"""### Instruction:</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="ss">Based on the provided text, create a multiple-choice question with 5 options labeled A), B), C), D), and E), and provide the correct answer. Ensure that the answer options are of similar length and closely aligned wording.</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="ss">### Input:</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>sample[<span class="st">'context'</span>]<span class="sc">}</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="ss">#### MCQ Question:</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>sample[<span class="st">'prompt'</span>]<span class="sc">}</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="ss">A) </span><span class="sc">{</span>sample[<span class="st">'A'</span>]<span class="sc">}</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="ss">B) </span><span class="sc">{</span>sample[<span class="st">'B'</span>]<span class="sc">}</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="ss">C) </span><span class="sc">{</span>sample[<span class="st">'C'</span>]<span class="sc">}</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="ss">D) </span><span class="sc">{</span>sample[<span class="st">'D'</span>]<span class="sc">}</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="ss">E) </span><span class="sc">{</span>sample[<span class="st">'E'</span>]<span class="sc">}</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="ss">#### Correct Answer:</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>sample[<span class="st">'answer'</span>]<span class="sc">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can inspect each part:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>{context}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>{'RMS Titanic was a British passenger liner, operated by the White Star Line, that sank in the North Atlantic Ocean on 15 April 1912 after striking an iceberg during her maiden voyage from Southampton, England, to New York City, United States. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making it the deadliest sinking of a single ship up to that time. '}</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>{prompt}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>{'Answer: The Titanic sank in the North Atlantic Ocean on 15 April 1912 and died of over 1,500 deaths. '}
</code></pre>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>{sample[<span class="st">'A'</span>]}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>{'16. "Which of the following best describes the Titanic?'}</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>{answer}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>{'Answer: D. All of the above. '}
</code></pre>
<p>From above, we can see things dont lne up (for example, <code>sample['A']</code> which should be A) is in fact the question. I also tested more specific instructions (prompt) to try make the output <code>text</code> structire be more consistent, but to no avail. You may notice the gernartion also appears to end mid-sentence, this is due to the <code>max_length</code> parametr, which will adjust later.</p>
</section>
<section id="custom-dataset-number-2" class="level3">
<h3 class="anchored" data-anchor-id="custom-dataset-number-2"><strong>Custom dataset number 2:</strong></h3>
<p>The next step is to load the larger dataset, which is just a collection of Khan academy articles <a href="https://c123ian.github.io/posts/Web%20Scraping/blog_v2.html">that I scraped</a>.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>dataset_context <span class="op">=</span> load_dataset(<span class="st">"c123ian/khan_academy_context"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['context'],
        num_rows: 2167
    })
})</code></pre>
</section>
<section id="max-token-sequence-length" class="level3">
<h3 class="anchored" data-anchor-id="max-token-sequence-length"><strong>Max token sequence length:</strong></h3>
<p>Error message was recieved when running an inference test on a longer khan article. <code>Token indices sequence length is longer than the specified maximum sequence length for this model (2200 &gt; 2048). Running this sequence through the model will result in indexing errors</code></p>
<p>The error you‚Äôre encountering indicates that some of your tokenized sequences are longer than the model‚Äôs maximum allowed sequence length (2048 tokens for models like BERT, GPT-2, etc.).</p>
<p>If we inspect the large khan dataset we can see that the percentage of texts over 2048 tokens: 47.76%</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Updated_Phi_blog_files/figure-html/3f0558c3-1926-4ec2-8eed-58113f2c307b-1-aea38601-970c-415b-aee0-b29c7f86788f.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">dataset token length</figcaption>
</figure>
</div>
<p>To address this, you can take the following steps:</p>
<ol type="1">
<li><p><strong>Truncate Sequences During Tokenization</strong>: Ensure that your tokenization function truncates sequences that exceed the model‚Äôs maximum length.</p></li>
<li><p><strong>Filter Out Long Sequences</strong>: Another approach would be to filter out examples from your dataset that exceed the model‚Äôs maximum sequence length after tokenization.</p></li>
</ol>
<p>We opted to filter out examples where the number of tokens exceeds the model‚Äôs limit:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create column for text_length</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'text_length'</span>] <span class="op">=</span> df[<span class="st">'context'</span>].<span class="bu">str</span>.<span class="bu">len</span>()</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter rows where text_length &lt;= 2048</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>filtered_df <span class="op">=</span> df[df[<span class="st">'text_length'</span>] <span class="op">&lt;=</span> <span class="dv">2048</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To convert a pandas DataFrame back to a DatasetDict using the datasets library, you can use the from_pandas() method. Let‚Äôs convert the filtered_df‚Äôs ‚Äúcontext‚Äù column back into a DatasetDict object. Here‚Äôs how you can do it:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> Dataset, DatasetDict</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the 'context' column of filtered_df to a Dataset</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>context_dataset <span class="op">=</span> Dataset.from_pandas(filtered_df[[<span class="st">'context'</span>]])</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the Dataset to a DatasetDict with a 'train' split</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>context_dataset_dict <span class="op">=</span> DatasetDict({<span class="st">"train"</span>: context_dataset})</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co"># renam</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> context_dataset_dict</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['context', '__index_level_0__'],
        num_rows: 1132
    })
}</code></pre>
<p>The <code>'__index_level_0__'</code> is the old index, incase we wanted to reverse engineer. We can remove it with:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Code to remove the '__index_level_0__' column</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.<span class="bu">map</span>(<span class="kw">lambda</span> example: {<span class="st">'context'</span>: example[<span class="st">'context'</span>]}, remove_columns<span class="op">=</span>[<span class="st">'__index_level_0__'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Lets make a random small subset for testing the model.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 5 random indices</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>random_indices <span class="op">=</span> random.sample(<span class="bu">range</span>(<span class="bu">len</span>(data[<span class="st">'train'</span>])), <span class="dv">5</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the random rows</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> data[<span class="st">'train'</span>].select(random_indices)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>test_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>While we have handled the max input tokens issue by filtering the dataset, we also want to ensure that the total token count (input tokens + generated tokens) does not exceed a specific threshold (e.g., 2048 tokens), and you‚Äôre aiming for a generated MCQ that won‚Äôt be more than 500 tokens, you can dynamically set the <code>max_new_tokens</code> based on the input length.</p>
<ol type="1">
<li>Calculate the number of tokens in the input.</li>
<li>Subtract this number from the desired maximum total tokens (e.g., 2048) to determine how many new tokens can be generated.</li>
<li>Set this calculated value as the max_new_tokens parameter.</li>
</ol>
</section>
<section id="inference-generating-mcq-from-textbooks-from-dataset" class="level3">
<h3 class="anchored" data-anchor-id="inference-generating-mcq-from-textbooks-from-dataset"><strong>Inference: Generating MCQ from Textbooks from dataset </strong></h3>
<p>Also for faster inference, were making sure the model <code>device</code> is set to the GPU.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_mcqs(test_data):</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check for GPU availability</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load the model and tokenizer</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#model = AutoModelForCausalLM.from_pretrained("c123ian/phi_test_target", trust_remote_code=True, torch_dtype=torch.float32).to(device)</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">#tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5")</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the instruction for generating an MCQ</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>    instruction <span class="op">=</span> <span class="st">'''"Based on the provided text, create a multiple-choice question with 5 options labeled A), B), C), D), and E), and provide the correct answer. Ensure that the answer options are of similar length and closely aligned wording."'''</span></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create an empty list to store the results</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> []</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over each context and generate MCQs with tqdm progress bar</span></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> context <span class="kw">in</span> tqdm(test_data[<span class="st">"context"</span>], desc<span class="op">=</span><span class="st">"Generating MCQs"</span>):</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Tokenize only the instruction for input</span></span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> tokenizer(instruction, return_tensors<span class="op">=</span><span class="st">"pt"</span>, return_attention_mask<span class="op">=</span><span class="va">False</span>).to(device)</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate the output</span></span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model.generate(<span class="op">**</span>inputs, max_length<span class="op">=</span><span class="dv">912</span>)  <span class="co"># Note: Adjust max_length as per your model's limitations</span></span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>        generated_mcq <span class="op">=</span> tokenizer.batch_decode(outputs)[<span class="dv">0</span>]</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add the result to the results list</span></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>        results.append({<span class="st">'Context'</span>: context, <span class="st">'Generated Text'</span>: generated_mcq})</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert the results list to a dataframe</span></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(results)</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage example</span></span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> generate_mcqs(test_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion"><strong>Conclusion</strong></h3>
<p>In this journey of fine-tuning the phi-1.5 model, we‚Äôve traversed through the myriad facets of large language model optimization. Fine-tuning is far from a straightforward process, and as our experience with the <a href="https://www.kaggle.com/competitions/kaggle-llm-science-exam">Kaggle Science Exam competition</a> highlighted, unforeseen challenges can emerge, especially when dealing with synthetic data and distinct training methodologies.</p>
<p>The choice of phi-1.5, with its unique synthetic ‚Äútextbook‚Äù training approach, offered both opportunities and challenges. While its compact nature made it manageable, its limitations became apparent when applied to specific tasks like generating MCQs from Khan Academy articles. However, these challenges underscore the importance of model selection, especially in light of available alternatives like <a href="https://huggingface.co/docs/transformers/main/en/model_doc/mistral">Mistral 7B</a>.</p>
<p>Through this process, we‚Äôve gained invaluable insights into the world of NLP tasks. The tools and techniques, from transformers to LoRA, have showcased the potential of fine-tuning and adaptation. Even though we encountered hurdles, the learnings derived are profound. They serve as a testament to the rapidly evolving nature of AI and the importance of continuous exploration.</p>
<p>Special thanks to contributors in the community, like <a href="https://github.com/Vasanthengineer4949/NLP-Projects-NHV">Vasanthengineer</a>, whose shared knowledge significantly enriched this endeavor. As we wrap up, it‚Äôs essential to remember that in the realm of AI, every challenge is an opportunity, and every setback a lesson. The path to mastering fine-tuning is iterative, and with each iteration, we come closer to harnessing the full potential of large language models.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>