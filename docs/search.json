[
  {
    "objectID": "posts/ULMFiT/Dell_LSTM_Blog.html",
    "href": "posts/ULMFiT/Dell_LSTM_Blog.html",
    "title": "Dell Technical Support Email Classification: A Deep Dive",
    "section": "",
    "text": "by Cian Prendergast\n\nIntroduction\nThis is an old project but, seeing how a lot of ULMFiT tecniques are being redicovered, I thought I’d share it!\n\n\n\nimage.png\n\n\nIts essentially the older fastai NLP lesson from fastai course/book. ULMFit is both a training method and a type of model, where the model is tasked with predicting the next word in a sentence, and through this process learns about world concepts.\n\n\n\nimage.png\n\n\nTransformers like BERT (Bidirectional Encoder Representations from Transformers), don’t allow for this “predict the next word” method, instead they delete a word at random (mask it from the model) and ask the model to predict the deleted words. Other then that the concepts between ULMFiT and Transormers are essentially the same.\nThis project is phrased as a binary classification problem, the model is tasked to classify Dell Technical Support emails into ‘software’ or ‘hardware’ categories. Sourced from an in-house case scraping tool at Dell, we have a dataset comprising 1324 emails. Of these, 662 are labeled as software-related (‘sw’) and 662 as hardware-related (‘hw’). This labeling was done based on the domain knowledge of the researchers, and the data is stored in a CSV file with each email occupying one row.\nThe approach we’ve taken in this notebook is based on ULMFiT method. We start with a pretrained language model, Wikitext103, which is already proficient in understanding the structure of the English language. This model is then tasked to predict whether a given email pertains to ‘sw’ or ‘hw’.\n\n\n\nimage.png\n\n\nBut how do we feed these emails to the model? They undergo tokenization, where sentences are split into individual words. Additionally, to ensure each batch fed to the model is of uniform size, we utilize special tokens. For shorter emails, padding is done using a reserved token termed xxunk. This process also involves numericalizing the words to form a vocabulary, prioritizing words based on their frequency of occurrence.\nYet, there’s always room for improvement. We further train our own language model tailored to Dell-specific vocabulary, built upon the LSTM architecture. This model is trained to predict subsequent words in a sentence.\nLastly, we retrain our newly minted language model to classify the emails. We gradually unfreeze each layer, aiming for enhanced performance.\n\nNow, let’s extract the content related to the dataset and see how it’s structured.\n\n\nDataset\nBefore diving into the intricacies of our model, it’s essential to get a firsthand look at the dataset we’re dealing with.\nThe dataset is imported from a CSV file, and here’s how we load it into a Pandas dataframe:\ndf = pd.read_csv('Dataset_CSV.csv')\nTo get a glimpse of the dataset, let’s inspect the first 20 rows:\ndf.head(20)\n\n\n\n\n\n\n\n\n\nlabel\n\n\ntext\n\n\n\n\n\n\n0\n\n\ns\n\n\nHi Technical support, We had an issue with the active controller today where emails as i will be the point of contact for this. Thanks.\n\n\n\n\n1\n\n\ns\n\n\nI can’t open web console\n\n\n\n\n2\n\n\ns\n\n\nI want update Dell blade server M910 Bios and iDRAC Firmware\n\n\n\n\n3\n\n\ns\n\n\niDRAC card on this server appears to be non-functional. New one has been ordered but I am unable to locate the Dell iDRAC Enterprise License that needs to be loaded up to it, so they the card will offer iDRAC functions. I am requesting assistance locating that file.\n\n\n\n\n4\n\n\ns\n\n\nOur network switch has crashed and needs its software to be re-installed. However, we have no copys of the software - OS10\n\n\n\n\n5\n\n\ns\n\n\nPlease provide iDRAC Enterprise license for PowerEdge M620 with after MB replacement.\n\n\n\n\n6\n\n\ns\n\n\nSystem crashed, purple screen of death. Please assist with troubleshooting root cause.\n\n\n\n\n7\n\n\ns\n\n\nTeam, we have installed ESXi6.5U3 and need help for NVIDIA supported drives.\n\n\n\n\n8\n\n\nh\n\n\nTwo drives in the array are showing solid amber lights but otherwise appear to be functioning correctly. Can’t get the GUI console to work. Appears that the software is too old to run on the version of Windows Server 2008R2. Trying to find\n\n\n\n\n9\n\n\nh\n\n\nVMWare showed a PSOD. Looks like a RAM Module died. TSR is attached to the ticket.\n\n\n\n\n10\n\n\nh\n\n\nWe are currently experiencing an issue with the memory on this server, in the logs we are gettings the following error “Correctable memory error rate exceeded for DIMM_B8”. Please see the TSR logs attached.\n\n\n\n\n11\n\n\ns\n\n\nWe received an error message while trying to update the firmware for our\n\n\n\n\n12\n\n\ns\n\n\nWe understand that the server is not under warranty, but we just need to get the iDRAC license.\n\n\n\n\n13\n\n\ns\n\n\nWhere can I find and download the latest version of SCOS for Dell Compellent SC4020\n\n\n\n\n14\n\n\nh\n\n\n2 HDD Failure and Server not Booting up\n\n\n\n\n15\n\n\nh\n\n\nA block on Physical Disk 0:0:0 was punctured by the controller. A block on Physical Disk 0:0:1 was punctured by the controller. Physical Disk 0:0:3 was reset.Communication with Chassis Integrated Controller 1 has been lost.\n\n\n\n\n16\n\n\ns\n\n\nA Dell software utility called lifecycle controoler from within windows has made the system unable to boot - It may be missing its dell service profile if that makes sense. The files and folder of the system are still intact and on the SSDisk.\n\n\n\n\n17\n\n\nh\n\n\nA fatal error was detected on a component at bus 23 device 2 function 0. The NDC is absent.\n\n\n\n\n18\n\n\ns\n\n\nA fibre channel adapter attached to this host started reporting errors, causing high latency and preventing access to VMs hosted on this system. Errors reported by our fibre channel adapter is the cause of the errors or not. Please advise which resources we can send you (e. g. logs) to help with the diagnosis.\n\n\n\n\n19\n\n\nh\n\n\nA system board fan keep reporting “less than the lower critical threshold” and then going into “Fan redundancy is lost”. It will then come back on line and all green. It has done this multiple times over the weekend. I have attached the log file.\n\n\n\n\n\n\n\nNow that we have a clearer idea about the dataset, let’s move on to the process of creating a TextDataLoader, which is a crucial step in preparing our data for training the model. Shall we?\n\n\nTextDataLoader: Prepping the Data\nAfter acquainting ourselves with the dataset, the next pivotal step involves preparing this data for the deep learning model. The Fastai library offers an incredibly handy tool for this – the TextDataLoader.\nSo, what does the TextDataLoader do?\nSimply put, it loads our dataset, identifies our label and text columns, and performs transformative operations on our email samples, which includes tokenization and numericalization.\nHere’s how we employ TextDataLoader:\ndls = TextDataLoaders.from_csv(path=path, csv_fname='Dataset_CSV.csv', text_col='text', label_col='label')\ndls.show_batch(max_n=3)\nWhen you execute the above code, you’ll witness the Fastai library automatically processing the texts, tokenizing them, and adding some special tokens. Here are a couple of examples:\n\nxxbos: Indicates the beginning of a text.\nxxmaj: Denotes that the subsequent word was initially capitalized.\n\nWith our data neatly formatted and ready, the subsequent move is defining a Learner apt for text classification.\n\nLet’s proceed and understand how we can create a learner tailored for our text classification task.\n\n\nThe First Attempt at Classification\nWith our data primed, it’s time to forge ahead and train our first classifier. In this endeavor, we’re leveraging the text_classifier_learner method provided by Fastai. This method is tailored for training text classification models.\nHere’s the code snippet that sets up our learner:\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nA few points to elucidate:\n\ndls: This is our data loaded via the TextDataLoader.\nAWD_LSTM: The architecture we’re utilizing. AWD_LSTM is a type of recurrent neural network known for its prowess in natural language processing tasks.\ndrop_mult=0.5: A multiplier applied to all dropouts in the AWD_LSTM model to regularize the model and prevent overfitting.\nmetrics=accuracy: We’re gauging the performance of our model based on its accuracy.\n\nNow that our learner is set up, we can train it and evaluate its performance. But this is merely the beginning. As we delve deeper into this notebook, we’ll explore more advanced techniques and iterations to refine our model.\n\nLet’s continue this exploration and uncover the subsequent steps taken in the notebook. Shall we?\n\n\nFine-Tuning and Initial Evaluation\nBuilding upon our foundational learner, we implement the power of fine-tuning to enhance its performance. Fine-tuning is a strategy wherein a pre-trained model is further trained (typically with a smaller learning rate) on a new dataset. This approach is incredibly beneficial, especially when the new dataset is relatively small.\nHere’s how we proceed with fine-tuning:\nlearn.fine_tune(4, 1e-2)\nThe number ‘4’ represents the epochs, indicating that we train our model for four iterations. The 1e-2 is our learning rate, determining the step size taken during optimization.\nPost fine-tuning, it’s essential to evaluate our model. One intuitive way to do this is by visualizing the results:\nlearn.show_results()\nFor a more quantitative perspective, we could also analyze the confusion matrix, which provides insights into false positives, false negatives, true positives, and true negatives.\n\n\nTesting on Unseen Data\nA trained model’s real mettle is tested when it encounters unseen data. Let’s see how our model fares:\nlearn.predict(\"please help me update my server software\")\nAdditionally, we test a more intricate sentence:\nlearn.predict(\"I am having issues after software update, I believe it is a hardware issue and need a DIMM replaced\")\nInterestingly, our model, in the second instance, places a strong emphasis on the keyword ‘software’, misclassifying the issue with a whopping 96% confidence. This highlights the nuances and challenges in natural language processing and underscores the need for continuous refinement.\n\nAs we journey further into this notebook, we’ll uncover more advanced strategies to enhance our model. Stay tuned!\n\n\nDiving Deeper: Fine-Tuning a Language Model\nWhile our initial classifier provided some insights, the world of natural language processing is filled with nuances. To tackle these complexities, one strategy is to train a language model specific to the domain or context in question. In our case, this means a language model tailored to Dell’s vocabulary.\nBefore embarking on this, let’s test our model on another sentence to gauge its current performance:\nlearn.predict(\"please help this is not a software issue this is a hardware problem\")\n\nCrafting the Language Model\nTo further refine our model, we harness the power of Fastai’s DataBlock API, which provides a flexible and intuitive way to structure our data:\ndls = TextDataLoaders.from_csv(path=path, csv_fname='Dataset_CSV.csv', text_col='text', label_col='label')\nThe parameter valid_pct=0.1 implies that a random 10% of the reviews will form the validation set. To get a sense of how the data looks after loading, you can execute:\ndell_lm.show_batch(max_n=5)\nFor our language model, we stick to the tried and tested AWD_LSTM architecture. Our metrics of choice are accuracy and perplexity (the latter being the exponential of the loss). We also incorporate weight decay and mixed precision for optimization and faster training:\nlearn = language_model_learner(dell_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16()\n\n\nEmbracing the Power of Freezing\nOne technique that’s invaluable during training is “freezing”. This involves fixing the weights of certain layers (typically the earlier ones) while training the others. This can be especially useful when fine-tuning pre-trained models:\nlearn.fit_one_cycle(1, 1e-2)\nBy iteratively freezing and unfreezing layers, we can achieve a delicate balance, preserving the knowledge gained by the pre-trained model while customizing it to our specific task.\n\nThe exciting journey of refining our model continues. Let’s delve further into the notebook’s subsequent steps.\n\n\n\nSaving Your Training Epochs\nIn deep learning, especially when working with vast datasets and models, training can be a time-consuming process. So, it’s essential to save our progress. Imagine training a model for several hours, only to lose all that effort due to a sudden crash or interruption!\nTo save an epoch after training, use:\nlearn.save('1epoch')\nThis command creates a file in learn.path/models/ named “1epoch.pth”. If you wish to resume training later or load your model on another machine, use:\nlearn = learn.load('1epoch')\nAnd voilà! Your model’s state is back.\n\n\nUnfreezing: Fine-tuning Your Model\nOnce you’ve trained your model with the frozen layers, it’s time to “unfreeze” them. This allows you to fine-tune the entire model for better performance.\nAfter unfreezing, you might notice remarkable improvements in your model’s accuracy:\nlearn.unfreeze()\nlearn.fit_one_cycle(15, 1e-3)\n\n\nText Generation: Let Your Model Speak\nWhat’s the fun of a language model if it can’t generate text? Let’s play a bit:\nTEXT = \"Hi, I need help with\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)]\nprint(\"\\n\".join(preds))\nThe results? Randomly generated customer emails that might look eerily similar to real ones. This shows how our model has learned the language structure from the training data.\nprint(\"\\n\".join(preds))\nHi , i need help with the support & support support to restore idrac Enterprise software to the idrac and the OS version in the process . Nutanix ME4024 has failed in Firmware . The firmware version on screen\nHi , i need help with the server Hi , i tried to update Firmware , but the issue followed the original firmware . But it 's not responding , I have tried to install Firmware , but i am\n\n\nGradual Unfreezing: One Layer at a Time\nOne of the secrets of fine-tuning a model effectively is the concept of “gradual unfreezing”. Instead of unfreezing all layers at once, we do it one by one, from the last layer to the first. This approach helps in achieving better results without compromising the earlier layers’ knowledge:\n# Unfreeze the last two layers and train\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n\n# Progressively unfreeze more layers and train\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n\n# Finally, unfreeze the entire model and train\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n\n\nWrapping Up with a Confusion Matrix\nA confusion matrix provides a snapshot of how well your model is performing in terms of true positives, true negatives, false positives, and false negatives. It’s an invaluable tool for understanding your model’s strengths and weaknesses:\ninterp = ClassificationInterpretation.from_learner(learn)\nprint(\"After Fine Tune Language Model\")\ninterp.plot_confusion_matrix()\n\n\n\nimage.png\n\n\n\n\nConclusion\nIn this journey, we took a dataset of Dell technical support emails and, using deep learning, classified them into ‘hardware’ and ‘software’ categories. With the power of pretrained models, tokenization, and gradual unfreezing, we achieved impressive results.\nUnfortantly I can’t share this specific dataset, but the full code is here on my GitHub.\nRemember, deep learning is as much an art as it is a science. Keep experimenting, fine-tuning, and most importantly, have fun!"
  },
  {
    "objectID": "posts/House/House_Blog_v2.html",
    "href": "posts/House/House_Blog_v2.html",
    "title": "Dublin’s Property Prices: A Spatial Perspective 🏠",
    "section": "",
    "text": "By Cian Prendergast"
  },
  {
    "objectID": "posts/House/House_Blog_v2.html#geocoding-with-the-google-api",
    "href": "posts/House/House_Blog_v2.html#geocoding-with-the-google-api",
    "title": "Dublin’s Property Prices: A Spatial Perspective 🏠",
    "section": "Geocoding with the Google API",
    "text": "Geocoding with the Google API\nWith our dataset ready, we proceeded with geocoding each address using the Google Maps API.\nFirst, we set up the Google Maps client:\nimport googlemaps\ngmaps = googlemaps.Client(key='xxxxxx')  # Note: actual API key is masked for security reasons.\nThen, for each address in our dataset, we sent a request to the Google Maps Geocoding API:\nfor x in range(len(df)):\n    try:\n        geocode_result = gmaps.geocode(df['Address'][x])\n        df['lat'][x] = geocode_result[0]['geometry']['location']['lat']\n        df['long'][x] = geocode_result[0]['geometry']['location']['lng']\n    except IndexError:\n        print(\"Address was wrong...\")\n    except Exception as e:\n        print(\"Unexpected error occurred.\", e)\nAfter the geocoding process, we saved the updated dataset with spatial coordinates:\ndf.to_csv('output.csv', encoding='unicode_escape')\n\n\n\n\n\n\n\n\n\nDate of Sale\n\n\nAddress\n\n\nPostal Code\n\n\nCounty\n\n\nPrice\n\n\nVAT Exclusive\n\n\nDescription of Property\n\n\nProperty Size Description\n\n\nlong\n\n\nlat\n\n\n\n\n\n\n55590\n\n\n21/01/2010\n\n\n7 Hawthorn Terrace, Eastwall, Dublin , Dublin\n\n\nNaN\n\n\nDublin\n\n\n210,000.0\n\n\nNo\n\n\nSecond-Hand Dwelling house /Apartment\n\n\nNaN\n\n\n-6.2375688552856445\n\n\n53.35382843017578\n\n\n\n\n55591\n\n\n21/01/2010\n\n\n76 Royston, Kimmage Road West, Dublin 12 , Dublin\n\n\nNaN\n\n\nDublin\n\n\n297,500.0\n\n\nNo\n\n\nSecond-Hand Dwelling house /Apartment\n\n\nNaN\n\n\n-6.312779426574707\n\n\n53.315460205078125\n\n\n\n\n55592\n\n\n21/01/2010\n\n\n9 Woodstown Gardens, Ballycullen Road, Knockly…\n\n\nNaN\n\n\nDublin\n\n\n409,000.0\n\n\nNo\n\n\nSecond-Hand Dwelling house /Apartment\n\n\nNaN\n\n\n-6.325483798980713\n\n\n53.275978088378906\n\n\n\n\n55593\n\n\n21/01/2010\n\n\n94, Loreto Avenue, Rathfarnham , Dublin\n\n\nDublin 14\n\n\nDublin\n\n\n271,000.0\n\n\nNo\n\n\nSecond-Hand Dwelling house /Apartment\n\n\nNaN\n\n\n-6.274948596954346\n\n\n53.28966522216797\n\n\n\n\n55594\n\n\n21/01/2010\n\n\nApt. 49 Hunters Walk, Hunterswood, Ballycullen…\n\n\nNaN\n\n\nDublin\n\n\n225,000.0\n\n\nNo\n\n\nSecond-Hand Dwelling house /Apartment\n\n\nNaN\n\n\n-6.3307037353515625\n\n\n53.27122497558594\n\n\n\n\n\n\n\nVisualizing the Spatial Distribution\nTo visualize the spatial distribution of the properties in our newly generated dataset, we can plot their latitude and longitude values:\nplt.figure(figsize=(10,10))\nsns.jointplot(x=df.lat.values, y=df.long.values, size=10)\nplt.ylabel('Longitude', fontsize=12)\nplt.xlabel('Latitude', fontsize=12)\nplt.show()\nThis visualization offers a bird’s-eye view of the property distribution across Dublin. It can also help in identifying any outliers or anomalies in our geocoded data.\n\n\n\nimage.png\n\n\nWe can remove outliers based on the percentiles, repeat by generating the visulisation (we want our datapoints to be within the boundary/shape of Dublin.\ndf[['long', 'lat']].describe(percentiles=[.01,.05,.1,.25,.5,.9,.95,.99]) # maybe dipslay by two decomal point\n\n\n\n\n\n\n\n\n\nlong\n\n\nlat\n\n\n\n\n\n\ncount\n\n\n55526.00\n\n\n55526.00\n\n\n\n\nmean\n\n\n-6.29\n\n\n53.33\n\n\n\n\nstd\n\n\n1.89\n\n\n0.67\n\n\n\n\nmin\n\n\n-121.95\n\n\n-35.91\n\n\n\n\n1%\n\n\n-6.47\n\n\n53.24\n\n\n\n\n5%\n\n\n-6.44\n\n\n53.26\n\n\n\n\n10%\n\n\n-6.41\n\n\n53.28\n\n\n\n\n25%\n\n\n-6.32\n\n\n53.30\n\n\n\n\n50%\n\n\n-6.26\n\n\n53.34\n\n\n\n\n90%\n\n\n-6.15\n\n\n53.44\n\n\n\n\n95%\n\n\n-6.13\n\n\n53.49\n\n\n\n\n99%\n\n\n-6.10\n\n\n53.61\n\n\n\n\nmax\n\n\n146.91\n\n\n56.44\n\n\n\n\n\n\n\n\nBasic Folium Map\nWe import folium and use our dataset mean to help follium find Dublin.\n\nmap1 = folium.Map(\n    # lat, long\n    location=[53.34,-6.27],\n    # use default OpenStreetMap title\n    zoom_start=12,\n)\ndf.apply(lambda row:folium.Circle(location=[row[\"lat\"], row[\"long\"]]).add_to(map1), axis=1)\nmap1\nAdd some colour scale using LinearColormap class. It gives us the ability to assign a color to the Circle objects added to our map:\ncenter = get_center_latlong(df3)\n\n# create a LinearColorMap and assign colors, vmin, and vmax\n# the colormap will show green for $100,000 homes all the way up to red for $1,500,000 homes\ncolormap = cm.LinearColormap(colors=['green', 'yellow', 'red'], vmin=230000, vmax=460000) # price range 25% and 75% quartile for 2018 - 2020\n\n# create our map again.  This time I am using a different tileset for a new look\nm = folium.Map(location=center, zoom_start=10, tiles='Stamen Toner')\n\n# Same as before... go through each home in set, make circle, and add to map.\n# This time we add a color using price and the colormap object\nfor i in range(len(df3)):\n    folium.Circle(\n        location=[df3.iloc[i]['lat'], df3.iloc[i]['long']],\n        radius=10,\n        fill=True,\n        color=colormap(df3.iloc[i]['Price']),# colour by price\n        fill_opacity=0.2\n    ).add_to(m)\n\n# the following line adds the scale directly to our map\nm.add_child(colormap)\n\n# Save map \nm.save('price_colormap.html')\n\nm # to display\n\n\n\nimage.png\n\n\n\n\nSegmenting the Map into Parishes using GeoJSON\nDublin is divided into numerous parishes. Segmenting our data by these parishes allows us to understand property prices at a more localized level. The segmentation process involves:\n\nObtaining Parish Boundaries: This involves sourcing a map that clearly defines the boundaries of each parish.\nOverlaying Property Data: Once we have the boundaries, we overlay our property data, ensuring each property is correctly located within its respective parish.\nAggregating Data: For each parish, we can now aggregate data, calculating metrics like average price, highest sale, and more.\n\nDublin parish boundaries in GeoJSON format, originally pulled from the 2011 Census, see source here. GeoJSON is a data format tailored for encoding geographical structures, extending the simplicity of JSON to spatial data. It uses polygons to represent the enclosed parish areas.\ndublin_map = folium.Map(location=[53.3302,  -6.3106],zoom_start= 11)\nurl = (\"https://raw.githubusercontent.com/ihuston/dublin_parishes/master/data/cleaned_dublin_parishes.geojson\")\ndublin_parishes_edge = f\"{url}\"\n\nf = folium.Choropleth(\n    geo_data= dublin_parishes_edge,\n    data = parish,\n    columns=[\"parishes_name\",\"Price\"],\n    key_on=\"feature.properties.Parish Name\",\n    name=\"choropleth\",\n    bins = 8,\n    fill_color = \"BuPu\",\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    highlight=True\n).add_to(dublin_map)\n\nf.geojson.add_child(\n    folium.features.GeoJsonTooltip(['Parish Name'],labels=False)\n)\ndublin_map\n\n\n\nimage.png\n\n\n\n\n\nFuture Steps: Deepening the Analysis\nWith our geocoded data ready and refined, we can deepen our analysis. Potential directions include:\n\nSpatial Clustering: Grouping properties based on their spatial proximity can help identify hotspots or areas with similar price trends.\nTime-based Analysis: Exploring how property prices have evolved over time, combined with the spatial data, can offer insights into emerging real estate trends in specific areas.\nIncorporating External Data: Overlaying our property data with other datasets, like amenities, public transport routes, or school districts, can help identify factors influencing property prices.\n\n\n\nConclusion\nSpatial analysis, especially when combined with segmentation like parishes, provides a depth of insight that’s hard to achieve otherwise. It allows stakeholders to make decisions based on localized trends rather than broad strokes. As we’ve showcased in this analysis, the value of such a project isn’t just in the data, but in the nuanced understanding it provides.\nlocal version &lt;iframe src=\"file:///C:/Users/Cian/Documents/DCU/YEAR1/Data%20Mining%20CA683/price_colormap.html\" width=\"100%\" height=\"600\"&gt;&lt;/iframe&gt;"
  },
  {
    "objectID": "posts/Bee/bee_blog_refined_v2.html",
    "href": "posts/Bee/bee_blog_refined_v2.html",
    "title": "Using Deep Learning to Classify Bee Species: An In-Depth Guide 🐝",
    "section": "",
    "text": "Bees play a pivotal role in maintaining our ecosystem. Yet, their diversity can be a challenge for even the most avid enthusiasts to discern. With over 20,000 recognized species (99 species in Ireland), how can we harness the power of deep learning to identify them from images? In this blog post, we’ll embark on a journey from collecting bee images to deploying a classifier in the wild. Let’s dive in!"
  },
  {
    "objectID": "posts/Bee/bee_blog_refined_v2.html#challenges-in-bee-identification",
    "href": "posts/Bee/bee_blog_refined_v2.html#challenges-in-bee-identification",
    "title": "Using Deep Learning to Classify Bee Species: An In-Depth Guide 🐝",
    "section": "Challenges in Bee Identification",
    "text": "Challenges in Bee Identification\nDid you know that distinguishing between different bee species, especially those closely related, can be quite challenging? The key to telling these species apart often lies in minute anatomical details. Take a look at the hind tibia (leg) for instance:\n\n\n\nimage.png\n\n\nFor the untrained eye, spotting such subtle differences from a distance is nearly impossible. While there are tools and guides that can assist in identification, for this project, we’ll explore how machine learning can aid in this endeavor. For those keen on traditional methods, this guide offers a deep dive into bee identification."
  },
  {
    "objectID": "posts/Bee/bee_blog_refined_v2.html#data-collection-building-the-bee-database",
    "href": "posts/Bee/bee_blog_refined_v2.html#data-collection-building-the-bee-database",
    "title": "Using Deep Learning to Classify Bee Species: An In-Depth Guide 🐝",
    "section": "Data Collection: Building the Bee Database",
    "text": "Data Collection: Building the Bee Database\nOur dataset was sourced from Biodiversity Ireland, which lists 99 known bee species in Ireland.\n\n\n\nimage.png\n\n\nEach species listing provides scientific and common names. Moreover, images for these species can be downloaded via GBIF or EOL Profile.\nA noteworthy point: I manually navigated the site to gather image URLs instead of using automated tools like BeautifulSoup. This hands-on approach had two advantages: 1. It enabled me to identify and rectify malfunctioning links, which arose due to variations in the scientific naming within the URLs. 2. In classes with fewer images, I observed some pictures showing bees in atypical settings, like test tubes. Avoiding such images ensures our model trains on relevant, real-world data."
  },
  {
    "objectID": "posts/Bee/bee_blog_refined_v2.html#creating-a-list-of-bee-classifications",
    "href": "posts/Bee/bee_blog_refined_v2.html#creating-a-list-of-bee-classifications",
    "title": "Using Deep Learning to Classify Bee Species: An In-Depth Guide 🐝",
    "section": "Creating a List of Bee Classifications",
    "text": "Creating a List of Bee Classifications\nBefore diving into data collection, it’s essential to know which bee species we are targeting. Here’s a list of the species we’re focusing on:\nbees_list = [\n    \"Andrena (Andrena) apicata\",\n    \"Andrena (Andrena) clarkella\",\n    \"Andrena (Andrena) fucata\",\n    \"Andrena (Andrena) fulva - Tawny Mining Bee\",\n    \"Andrena (Andrena) lapponica\",\n    \"Andrena (Andrena) praecox\",\n    \"Andrena (Chlorandrena) humilis\",\n    \"Andrena (Cnemidandrena) denticulata\",\n    \"Andrena (Cnemidandrena) fuscipes\",\n    \"Andrena (Euandrena) bicolor - Gwynne's Mining Bee\",\n    \"Andrena (Hoplandrena) rosae\",\n    \"Andrena (Hoplandrena) scotica\",\n    \"Andrena (Hoplandrena) trimmerana - Trimmer's Mining Bee\",\n    \"Andrena (Leucandrena) barbilabris\",\n    \"Andrena (Margandrena) marginata\",\n    \"Andrena (Melandrena) nigroaenea\",\n    \"Andrena (Micrandrena) minutula\",\n    \"Andrena (Micrandrena) semilaevis\",\n    \"Andrena (Micrandrena) subopaca\",\n    \"Andrena (Plastandrena) pilipes\",\n    \"Andrena (Poliandrena) tarsata - Tormentil Mining Bee\",\n    \"Andrena (Ptilandrena) angustior\",\n    \"Andrena (Taeniandrena) wilkella\",\n    \"Andrena (Trachandrena) haemorrhoa - Early Mining Bee\",\n    \"Andrena cineraria (Linnaeus) - Grey Mining Bee\",\n    \"Andrena coitana (Kirby)\",\n    \"Andrena stragulata\",\n    \"Anthidium (Anthidium) manicatum\",\n    \"Bombus (Bombus) cryptarum\",\n    \"Bombus (Bombus) lucorum\",\n    \"Bombus (Bombus) magnus\",\n    \"Bombus (Megabombus) hortorum - Small Garden Bumble Bee\",\n    \"Bombus (Melanobombus) lapidarius - Large Red Tailed Bumble Bee\",\n    \"Bombus (Psithyrus) barbutellus - Barbut's Cuckoo Bee\",\n    \"Bombus (Psithyrus) bohemicus - Gipsy Cuckoo Bee\",\n    \"Bombus (Psithyrus) campestris - Field Cuckoo Bee\",\n    \"Bombus (Psithyrus) rupestris - Hill Cuckoo Bee\",\n    \"Bombus (Psithyrus) sylvestris - Four Coloured Cuckoo Bee\",\n    \"Bombus (Psithyrus) vestalis - Vestal Cuckoo Bee\",\n    \"Bombus (Pyrobombus) hypnorum\",\n    \"Bombus (Pyrobombus) jonellus - Heath Bumble Bee\",\n    \"Bombus (Pyrobombus) monticola - Mountain Bumble Bee\",\n    \"Bombus (Pyrobombus) pratorum - Early Bumble Bee\",\n    \"Bombus (Subterraneobombus) distinguendus - Great Yellow Bumble Bee\",\n    \"Bombus (Thoracombus) muscorum - Moss Carder bee\",\n    \"Bombus (Thoracombus) pascuorum - Common Carder Bee\",\n    \"Bombus (Thoracombus) ruderarius - Red-tailed Carder Bee\",\n    \"Bombus (Thoracombus) sylvarum - Shrill Carder Bee\",\n    \"Coelioxys (Coelioxys) elongata\",\n    \"Coelioxys (Coelioxys) inermis\",\n    \"Colletes (Colletes) daviesanus\",\n    \"Colletes (Colletes) floralis - Northern Colletes\",\n    \"Colletes (Colletes) hederae\",\n    \"Colletes (Colletes) similis\",\n    \"Colletes (Colletes) succinctus\",\n    \"Halictus (Halictus) rubicundus\",\n    \"Halictus (Seladonia) tumulorum\",\n    \"Hylaeus (Hylaeus) communis - Common Yellow Face Bee\",\n    \"Hylaeus (Prosopis) brevicornis\",\n    \"Hylaeus (Prosopis) confusus\",\n    \"Hylaeus (Spatulariella) hyalinatus\",\n    \"Lasioglossum (Dialictus) cupromicans\",\n    \"Lasioglossum (Dialictus) leucopus\",\n    \"Lasioglossum (Dialictus) smeathmanellum\",\n    \"Lasioglossum (Evylaeus) albipes\",\n    \"Lasioglossum (Evylaeus) calceatum - Slender Mining Bee\",\n    \"Lasioglossum (Evylaeus) fratellum\",\n    \"Lasioglossum (Evylaeus) nitidiusculum - Neat Mining Bee\",\n    \"Lasioglossum (Evylaeus) punctatissimum\",\n    \"Lasioglossum (Evylaeus) rufitarse\",\n    \"Lasioglossum (Evylaeus) villosulum - Shaggy Mining Bee\",\n    \"Lasioglossum (Lasioglossum) lativentre\",\n    \"Megachile (Delomegachile) willughbiella\",\n    \"Megachile (Megachile) centuncularis\",\n    \"Megachile (Megachile) ligniseca\",\n    \"Megachile (Megachile) versicolor\",\n    \"Megachile (Xanthosarus) maritima\",\n    \"Nomada argentata\",\n    \"Nomada fabriciana Fabricius - Nomad Bee\",\n    \"Nomada flavoguttata\",\n    \"Nomada goodeniana - Gooden's Nomad Bee\",\n    \"Nomada leucophthalma\",\n    \"Nomada marshamella - Marsham's Nomad Bee\",\n    \"Nomada obtusifrons\",\n    \"Nomada panzeri\",\n    \"Nomada ruficornis\",\n    \"Nomada rufipes\",\n    \"Nomada sheppardana - Dark Nomad Bee\",\n    \"Nomada striata\",\n    \"Osmia (Helicosmia) aurulenta\",\n    \"Osmia (Osmia) rufa - Red Mason Bee\",\n    \"Sphecodes ephippius\",\n    \"Sphecodes ferruginatus\",\n    \"Sphecodes geoffrellus\",\n    \"Sphecodes gibbus\",\n    \"Sphecodes hyalinatus\",\n    \"Sphecodes monilicornis\",\n    \"Sphecodes pellucidus\",\n    \"Xylocopa (Xylocopa) violacea - Violet Carpenter Bee\"\n]\nThis list, bees_list, contains the scientific names of various bee species. By defining our target species upfront, we ensure that our data collection process remains focused and streamlined.Here’s how we can automate the process of downloading images from their respective URLs:\n\nimport urllib.request\nimport pandas as pd\nimport csv\nimport re\n\ndef download_images_from_urls(url_path, image_path):\n    \"\"\"Download images from the provided URLs and save them to the specified path.\"\"\"\n    urls_df = pd.read_csv(url_path)\n    for idx, link in enumerate(urls_df.values):\n        file_name = f'image{idx}.jpg'\n        full_image_path = f'{image_path}{file_name}'\n        try: \n            urllib.request.urlretrieve(link[0], full_image_path)\n            print(f'{file_name} saved.')\n        except:\n            print(f\"Failed to save {file_name}\")\n            continue\n\n# Main loop for processing each bee species\nfor bee_name in bees_list:\n    # Extract URLs from the multimedia text file\n    multimedia_file = fr\"DUMMY_PATH\\Bee_Images\\{bee_name}\\multimedia.txt\"\n    with open(multimedia_file, encoding=\"utf8\") as file:\n        urls_img = [re.findall('(https?://[^\\s]+?\\.jpg)', line) for line in file if re.findall('(https?://[^\\s]+?\\.jpg)', line)]\n\n    # Save the URLs to a CSV\n    url_csv_path = fr\"DUMMY_PATH\\Test_Bee_Images\\{bee_name}.csv\"\n    with open(url_csv_path, 'w', encoding='UTF8', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(urls_img)\n\n    # Define the path where you want to save the images\n    images_save_path = fr\"DUMMY_PATH\\Dataset_Bees\\{bee_name}\\data\\\\\"\n    \n    # Download the images using the URLs\n    download_images_from_urls(url_csv_path, images_save_path)\n\nIn the code above, replace DUMMY_PATH with the actual base directory path where you wish to perform these operations. This function takes a CSV file path containing image URLs and a path where the images should be saved. It then iterates over each URL, downloads the image, and saves it to the specified path.\nThe main loop processes each bee species. For each species:\n\nIt extracts image URLs from a multimedia.txt file associated with the bee species.\nThese URLs are then saved to a CSV file.\nThe images are downloaded from these URLs using the download_images_from_urls function and saved to a designated directory."
  },
  {
    "objectID": "posts/Bee/bee_blog_refined_v2.html#prepare-dataset-for-training",
    "href": "posts/Bee/bee_blog_refined_v2.html#prepare-dataset-for-training",
    "title": "Using Deep Learning to Classify Bee Species: An In-Depth Guide 🐝",
    "section": "Prepare Dataset for Training",
    "text": "Prepare Dataset for Training\nTraining a model requires a structured dataset, and the fastai library provides tools to streamline this process. Here’s how we prepared our dataset for training:\n\nSetting the Base Path\nThe Path.BASE_PATH is set to ensure that all paths are displayed relative to a specific directory, making it easier to understand and navigate:\nPath.BASE_PATH = path\nWe have data pertaining to 99 bee species. Let’s take a look at a few of them:\npath.ls(n_max=None)\n(#99) [Path('Andrena (Andrena) apicata'),Path('Andrena (Andrena) clarkella'),...]\nThe get_image_files(path) function allows us to recursively retrieve all the image files from our specified directory, ensuring we have all our data points:\nfns = get_image_files(path)\n\n\nCreating a Data Pipeline with DataBlock\nFastai’s DataBlock API provides a high-level abstraction to define the steps necessary for loading, splitting, and transforming a dataset:\nbees = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=1),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n    \ndls = bees.dataloaders(path)\nTo understand the structure and flow of this data processing pipeline, we can use:\nbees.summary(path)\n\n\nData Augmentation\nData augmentation artificially increases the size of the training dataset by creating modified versions of images in the dataset. This helps in making the model more robust and generalizable:\nbees = bees.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = bees.dataloaders(path)\nWe can visualize some of the augmented images:\ndls.train.show_batch(max_n=4, nrows=1, unique=True) # figsize=(30,30)\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/Bee/bee_blog_refined_v2.html#training-process-using-fastai",
    "href": "posts/Bee/bee_blog_refined_v2.html#training-process-using-fastai",
    "title": "Using Deep Learning to Classify Bee Species: An In-Depth Guide 🐝",
    "section": "Training Process Using Fastai",
    "text": "Training Process Using Fastai\nTraining a deep learning model is a multi-step process. Here’s how we approached it using the fastai library:\n\nSetting Up the Environment\nFastai provides a streamlined setup for deep learning projects. The following code sets up the required environment and imports necessary modules:\nimport fastbook\nfastbook.setup_book()\nfrom fastbook import *\nfrom fastai.vision.widgets import *\nfrom pathlib import Path\n\n\nBasic Training\nFor our initial training, we utilized the ResNet-18 architecture, which is a relatively lightweight model but performs exceptionally well for various vision tasks:\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\nThe model was trained for three epochs, and here’s a summary of its performance:\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.480083\n1.318795\n0.360347\n2:44:32\n\n\n0\n0.923098\n0.842963\n0.238649\n2:47:05\n\n\n1\n0.727613\n0.642357\n0.182362\n2:45:23\n\n\n2\n0.606020\n0.566509\n0.160040\n2:44:23\n\n\n\nThese values indicate that the model’s performance improved with each epoch. By the third epoch, the model achieved an accuracy of 84%, which suggests it has learned to identify a significant portion of the bee species correctly.\nAfter basic training, we saved our model for future reference:\nlearn.export('bee_init.pkl')\n\n\n\nAdvanced Techniques\n\nUsing Callbacks\nCallbacks in fastai provide an avenue to incorporate custom behaviors into the training process. Callbacks like SaveModelCallback, ReduceLROnPlateau, ShowGraphCallback, and EarlyStoppingCallback offer enhanced control over the training process. They enable functionalities like saving the best model, adjusting the learning rate dynamically, and early stopping to prevent overfitting. More examples of using Callbacks can be found here.\n\n\nImplementing the Learning Rate Finder\nChoosing an appropriate learning rate is crucial. Fastai’s Learning Rate Finder aids in this selection:\nlr_min, lr_steep = learn.lr_find()\nprint(f\"minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\nThe visual plot generated helps in choosing a learning rate just before the loss starts to rise.\n\n\nAdvanced Training with Callbacks\nWe further trained our model using a combination of callbacks to enhance performance:\nlearn_es_sm = cnn_learner(dls, resnet18, metrics=error_rate)\nkeep_path = learn_es_sm.path\nlearn_es_sm.path = Path(path)\nlearn_es_sm.fit_one_cycle(10, cbs=[EarlyStoppingCallback(monitor='error_rate', patience=2), SaveModelCallback(monitor='error_rate', min_delta=0.01), ShowGraphCallback()])\nlearn_es_sm.path = keep_path\nDuring this phase, the model’s performance was tracked, and the best model was saved based on the lowest error rate achieved.\nFinally, after advanced training, we exported our enhanced model:\nlearn_es_sm.export('bee_advanced.pkl')\nA piece of good advice from Jeremy Howard, is to run a basic training loop as a means to clean the dataset. Fastai provides a handy tool called ImageClassifierCleaner() to visually review the predictions of our model. It’s especially useful for identifying images that might be mislabeled or don’t belong in our dataset.\nTo use the cleaner:\ncleaner = ImageClassifierCleaner(learn)\ncleaner\nThis tool presents a GUI where you can see the top losses (images where the model’s predictions were most incorrect).\nFor each image, you can either: - Delete the image from the dataset if it’s irrelevant. - Relabel the image if it’s misclassified.\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/Bee/bee_blog_refined_v2.html#deploying-the-bee-classifier-as-a-web-app-with-streamlit",
    "href": "posts/Bee/bee_blog_refined_v2.html#deploying-the-bee-classifier-as-a-web-app-with-streamlit",
    "title": "Using Deep Learning to Classify Bee Species: An In-Depth Guide 🐝",
    "section": "Deploying the Bee Classifier as a Web App with Streamlit",
    "text": "Deploying the Bee Classifier as a Web App with Streamlit\nOnce our model is trained, it’s time to put it to use! One of the most user-friendly ways to showcase our bee classifier is through a web application. Using Streamlit, we can quickly create an interactive app where users upload bee images, and the app classifies the bee species in real-time.\nimport json\nimport platform\nimport pathlib\nimport streamlit as st\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\nfrom streamlit_lottie import st_lottie\n\n# Fix for 'WindowsPath' error on non-Windows systems\nplt = platform.system()\nif plt == 'Linux':\n    pathlib.WindowsPath = pathlib.PosixPath\n\n# Customise App UI\nst.set_page_config(\n    page_title=\"Bee Classifier\",\n    page_icon=\"🐝\",\n    initial_sidebar_state=\"expanded\",\n    menu_items={\n        'About': \"# This is a personal project to identify bee species.\"\n    }\n)\n\n# Header\nst.markdown(\"&lt;h1 style='text-align: center;'&gt;Welcome, I am your personal Bee species classifier.&lt;/h1&gt;\", unsafe_allow_html=True)\n\n# Lottie Animation\ndef load_lottiefile(filepath: str):\n    with open(filepath, \"r\") as f:\n        return json.load(f)\n\nlottie_coding = load_lottiefile(\"images/bee-flying.json\")\ncol1, col2, col3 = st.columns(3)\nwith col2:\n    st_lottie(\n        lottie_coding,\n        speed=1,\n        reverse=False,\n        loop=True,\n        quality=\"medium\",\n        key=None,\n    )\n\n# Load the Model\n@st.cache(allow_output_mutation=True)\ndef load_model():\n    return load_learner(\"bee_init.pkl\")\n\nwith st.spinner(\"I am collecting my thoughts...\"):\n    model = load_model()\n\n# Image Upload\nuploaded_image = st.file_uploader(\n    \"Upload your image and I'll give it a try.\", type=[\"png\", \"jpg\"])\n\n# Predict and Display Results\nif uploaded_image is not None:\n    st.image(uploaded_image)\n    try:\n        pred, pred_idx, probs = model.predict(uploaded_image.getvalue())\n        st.success(f\"{[pred]} I am {probs[pred_idx]*100:.0f}% confident.\")\n        st.caption(\"Caution: I have only been trained on a small set of images. I may also be wrong.\")\n    except:\n        st.write(\"Sorry, I don't know that bee\")\nThis code sets up a basic Streamlit app. Users can upload their bee images, and our trained model predicts the bee species on the fly.\nTo run this app locally, use the command:\nstreamlit run your_script_name.py\n\n\n\nimage.png\n\n\nFor those keen on making their app accessible to a wider audience, Streamlit also provides easy deployment options. Check out the final version of our bee classifier app here."
  },
  {
    "objectID": "posts/Bee/bee_blog_refined_v2.html#wrapping-up-the-power-of-deep-learning-in-species-identification",
    "href": "posts/Bee/bee_blog_refined_v2.html#wrapping-up-the-power-of-deep-learning-in-species-identification",
    "title": "Using Deep Learning to Classify Bee Species: An In-Depth Guide 🐝",
    "section": "Wrapping Up: The Power of Deep Learning in Species Identification",
    "text": "Wrapping Up: The Power of Deep Learning in Species Identification\nWe’ve journeyed through the intricate process of classifying bee species using deep learning, covering everything from the initial data collection to deploying a real-world application. This endeavor underscores the immense potential of machine learning in revolutionizing traditional tasks, such as species identification.\nHowever, it’s essential to remember that in practice, species identification often hinges not just on visual observations but also on other contextual information like habitat, substrate, location, and time. Incorporating such multi-modal data could be an exciting avenue for future projects.\nFor those intrigued by the possibilities of deep learning in ecology, a Kaggle competition on fungi identification using image metadata provides further insights.\nFinally, for enthusiasts and developers alike, the complete code for this project is available on GitHub, and the dataset can be accessed on the Huggingface Hub.\nThank you for joining us on this exploration. Deep learning continues to open new frontiers in various domains, and with tools like fastai and Streamlit, the barrier to entry is lower than ever. Whether you’re a budding data scientist or an ecology enthusiast, there’s no better time to dive in and make a difference.\nHappy coding, and keep buzzing! 🐝"
  },
  {
    "objectID": "posts/a_Phi/Updated_Phi_blog.html",
    "href": "posts/a_Phi/Updated_Phi_blog.html",
    "title": "Finetuning the phi-1.5 Language Model 📝",
    "section": "",
    "text": "Fine-tuning a model, especially one as complex as phi-1.5, is an intricate endeavor demanding a judicious blend of techniques, optimizations, and a profound grasp of the model’s architecture. As illustrated in this notebook, each phase—from environment setup to the utilization of the fine-tuned model—is a delicate dance of precision and understanding.\nThis journey was driven by the Kaggle Science Exam competition, where the challenge was to enable a language model to answer multiple-choice questions. These questions were intriguingly generated by GPT 3.5 from a mix of STEM and History Wikipedia articles, offering a deep dive into the realms of synthetic data and knowledge distillation.\nMy model of choice was phi-1.5. With only 1.3 billion parameters, it stands as a compact yet potent transformer. Its allure was further amplified by its unique training methodology using synthetic “textbooks”, which embodies the essence of a perfect “textbook” as described in the Textbooks all you need research.\nMy strategy was multifaceted:\n\nSubstitute synthetic textbooks or Wikipedia articles with informative articles from Khan Academy.\nHarness the GPT 3.5 API to mold MCQs from a sample of 100 Khan Academy “textbooks.”\nFine-tune phi 1_5 using this dataset to craft MCQs from the remaining textbooks.\nEmploy another LLM to respond to MCQs using the open-book method.\n\nHowever, challenges arose, particularly at step 3. The limitations, I conjecture, were twofold: the constrained capacity of the Phi 1_ model and the limited training dataset. A larger model, like Mistral 7B, might have been a more potent alternative.\nWith the aid of libraries like transformers and methodologies such as LoRA, the phi-1.5 model’s fine-tuning capabilities offer a promising path for NLP tasks, be it in research or real-world applications. This endeavor, though challenging, was enriched by insights from contributors like Vasanthengineer, whose video tutorial proved invaluable.\nNow, let’s delve into the process of finetuning Phi 1_5 for generating MCQs based on a textbook context.\n\nThis adjusted conclusion should provide a cohesive recap and set the stage for the rest of the content.\n\n\nInstalling Dependencies\nTo begin, we need to install a few Python packages which are crucial for our finetuning process:\n! pip install accelerate transformers einops datasets peft bitsandbytes\n\nOnce the required packages are installed, the next step is to import them:\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\nimport os\nHere, we are using the Hugging Face’s transformers library for language model finetuning and datasets for data handling.\n\n\n\nSetting Up the Tokenizer\nBefore we can start training, we need to set up the tokenizer:\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\nThe tokenizer will be responsible for converting our text data into tokens that the model can understand.\n\n\n\nConfiguring the Model\nThe phi-1.5 model leverages the BitsAndBytesConfig for efficient quantization:\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/phi-1_5\",\n    device_map={\"\":0},\n    trust_remote_code=True,\n    quantization_config=bnb_config\n)\nThis configuration ensures the model uses less memory during training, making it more efficient. Lets dicuss the above code in a little more detail.\nAlright! Let’s delve deeper into the more technical aspects.\n\n\n\nDevice Map\nThe Phi 1_5 model does not currently support device_map=\"auto\". If you’re using just one GPU for a model, you don’t necessarily need a device map since the entire model will reside on that single GPU. However, if you still want to set up a device map for one GPU, you can specify all the layers to reside on GPU 0 device_map={\"\":0.\n\n\nBitsAndBytes\nThe bitsandbytes library provides a lightweight wrapper around CUDA custom functions, especially 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions. This is crucial for memory efficiency during training, allowing for faster computations.\nHere’s how it’s configured for the phi-1.5 model:\n\nload_in_4bit: Loads the model in 4-bit precision.\nbnb_4bit_use_double_quant: Allows for using double quantization.\nbnb_4bit_quant_type: Specifies the type of quantization, in this case, “nf4”.\nbnb_4bit_compute_dtype: Sets the data type for computation to half-precision floating point (float16).\n\n\n\n\nFinding Target Modules for LoRA\nLoRA (Layer-wise Relevance Analysis) is a technique used to enhance and adapt specific layers of a neural network. By attaching LoRA to layers, you can introduce low-rank adaptations to them, enabling the model to better capture task-specific information without retraining the entire network. This makes fine-tuning more efficient and targeted.\nAn essential aspect of LoRA is determining which layers or modules of the model should be targeted. Instead of printing the entire model to find these, there’s a handy function (which I got from here) below:\ndef find_target_modules(model):\n    # Initialize a Set to Store Unique Layers\n    unique_layers = set()\n    \n    # Iterate Over All Named Modules in the Model\n    for name, module in model.named_modules():\n        # Check if the Module Type Contains 'Linear8bitLt' or 'Linear4bit'\n        module_type = str(type(module))\n        if \"Linear8bitLt\" in module_type or \"Linear4bit\" in module_type:\n            # Extract the Type of the Layer\n            layer_type = name.split('.')[-1]\n            \n            # Add the Layer Type to the Set of Unique Layers\n            unique_layers.add(layer_type)\n\n    # Return the Set of Unique Layers Converted to a List\n    return list(unique_layers)\n\ntarget_modules = find_target_modules(model)\nfind_target_modules(model)\n['Wqkv', 'fc2', 'out_proj', 'fc1']\nThe function extracts modules of type Linear8bitLt or Linear4bit which are likely the primary focus for LoRA.\nEither using the find_target_modules function above, or simple printing the model print(model) to display the models architecture:\n\n\n\n\n\nComponent\n\n\nDescription\n\n\nBelongs to\n\n\n\n\n\n\nMHA (Multi-Head Attention)\n\n\nThis is essentially a mixer. The inclusion of RotaryEmbedding() suggests that this component can scale effectively.\n\n\n-\n\n\n\n\nEmbedding Layer\n\n\nConverts discrete tokens into continuous embeddings, with dropout for regularization.\n\n\n-\n\n\n\n\nLayerNorm\n\n\nStabilizes layer activations.\n\n\nParallelBlock\n\n\n\n\nresid_dropout\n\n\nDropout on residual connections to combat overfitting.\n\n\nParallelBlock\n\n\n\n\nMHA\n\n\nWeighs the significance of various positions in an input sequence. Uses rotary embeddings and linear transformations.\n\n\nParallelBlock\n\n\n\n\nout_proj\n\n\nA linear transformation post-MHA.\n\n\nParallelBlock\n\n\n\n\ninner_attn\n\n\nSelf-attention within MHA.\n\n\nParallelBlock\n\n\n\n\ninner_cross_attn\n\n\nCross-attention within MHA.\n\n\nParallelBlock\n\n\n\n\nMLP (Multi-Layer Perceptron)\n\n\nContains two dense layers and uses the GELU activation function. It processes the data post-MHA.\n\n\n-\n\n\n\n\n\n\n\n\nConfiguring LoRA\nWith the target modules identified above, we can set up the LoRA configuration:\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"Wqkv\", \"out_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\nFor our target modules will be will use weights of key value (Wqkv) and out_proj but, we could experiment with the addition of inner_attn and inn_cross_attn. If you’re incorporating LoRA, ensure it’s applied across all layers, not just to the Key and Value matrices (Wqkv), to maximize model performance (Raschka, 2023).\n\nThe description of this LoRA is very careful to not state that the output is objectively better than not using LoRA, because everything is subjective and there are use cases where vibrant output is not desired. For most use cases, the output should be better desired however (source).\n\nHere’s a breakdown of the LorA parameters:\n\nr: The rank for low-rank matrices. Reducing it can result in a loss of information, but in some cases, we might not need all of it.\nGreat rank explanation (in the context of stable diffusion) here.\n\n\n\n\nAdjusting r value for images\n\n\n\nlora_alpha: A scaling factor for the weight matrices. A higher value assigns more weight to the LoRA activations.\n\nAdjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank’s value (Raschka, 2023).\n\ntarget_modules: Specifies the modules to be targeted by LoRA.\nlora_dropout: Dropout probability for the LoRA layers.\nbias: Specifies which bias parameters to train. Recommendations suggest starting with None.\ntask_type: Specifies the type of task, in this case, “CAUSAL_LM” (Causal Language Modeling).\n\n\n\nCustom dataset number 1:\nLets import the first dataset, GPT 3.5 geneated MCQ based on 200 Khan academy contexts or “textbooks”.\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"c123ian/khan_academy_200\") # generate using gpt 3.5 on pulled Khan academy lessons\n\ndataset\n    \nDatasetDict({\n    train: Dataset({\n        features: ['context', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer'],\n        num_rows: 125\n    })\n})\nprint(dataset['train'][0])  # This will print the first row of the train dataset.\n{'context': \"Hi, I'm Larry Joe!  What do you work on? I am a Molecular Biologist, working in a lab at the University of California, Berkeley.  Our lab in interested in extending lifespans and preventing age-related diseases.  In my previous job I worked for a Bay Area biotechnology company where I developed DNA analysis instruments and reagents for forensic and agricultural purposes.  Some of my products can be seen on “CSI” shows or movies. My favorite project there was participating in the gray wolf restoration project in Yellowstone Park.  There is a delicate balance of animal species in the park.  Wolves had been hunted to extinction in Yellowstone.  The loss of the predator disrupted that balance and negatively impacted the other animal populations.  Twenty-one wolves were transported from Canada to Yellowstone.  We analyzed the DNA of the founding members and their offspring to confirm the field biologists’ observations on wolf family behavior as the wolves re-populated the park.  The project was a big success as the wolves continue to thrive there today, and the ecosystem balance has recovered. Larry in Yellowstone How did you get interested in science, and what did you study? When I was a kid, I loved animals and science.  I always wanted to visit the Oakland or San Francisco Zoos, the Exploratorium, Chabot Observatory, or the Academy of Sciences.  I also liked building things and figuring out how things work.  I studied microbiology and immunology in college.  I thought I might pursue a career in one of the health professions, but in the end, I enjoyed conducting experiments as a scientist. What do you do in your free time? My main hobbies are traveling, hiking, wood working, reading, watching movies & nature shows, framing pictures, and gardening.  I also like to play and watch baseball.  My nephew and I have a goal to visit all of the Major League Baseball stadiums. What’s your one piece of advice for people interested in biology? I have two pieces of advice:1: Don’t be afraid to ask a lot of questions.  I like it when students ask me questions, for it shows me that they are interested in the project and it gives me confidence that they can execute the experimental protocol.2: Try to find a mentor.  Mentors can be great resources for a scientist as you progress through your career.  They can help prepare you for various situations and provide insight on what to expect in a given job.  Many scientists enjoy the mentoring aspect of the job, including myself.\", 'prompt': 'Which of the following hobbies does Larry Joe mention in his free time?', 'A': 'Traveling', 'B': 'Painting', 'C': 'Cooking', 'D': 'Playing chess', 'E': 'Knitting', 'answer': 'A'}\n\n\nTargeting the MCQ generation\nUse the preprocessing function to convert it to source-target pairs:\n\ndef preprocess_for_mcq(example):\n    source = example['context'] + \" \" + example['prompt']\n    target = example['prompt'] + \" A) \" + example['A'] + \" B) \" + example['B'] + \" C) \" + example['C'] + \" D) \" + example['D'] + \" E) \" + example['E'] + \" Answer: \" + example['answer']\n    return {\"source\": source, \"target\": target}\n\n\nprocessed_dataset = dataset.map(preprocess_for_mcq)\n\nprint(processed_dataset)\nDatasetDict({\n    train: Dataset({\n        features: ['context', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'source', 'target'],\n        num_rows: 125\n    })\n})\nSo the source will be Khan academy, and the target will be a MCQ generated from the source articles\nprint(\"Source:\", processed_dataset[\"train\"][\"source\"][1])\nSource[0]: Taj Mahal, Agra, India, 1632–53 (photo: King of Hearts, CC BY-SA 4.0)Taj Mahal, Agra, India, 1632–53 (photo: King of Hearts, CC BY-SA 4.0) Taj Mahal, Agra, India, 1632–53 (photo: King of Hearts, CC BY-SA 4.0) Shah Jahan was the fifth ruler of the Mughal dynasty. During his third regnal year, his favorite wife, known as Mumtaz Mahal, died due to complications arising from the birth of their fourteenth child.  Deeply saddened, the emperor started planning the construction of a suitable, permanent resting place for his beloved wife almost immediately. The result of his efforts.....\nprint(\"Target:\", processed_dataset[\"train\"][\"target\"][1])\nTarget[0]: What is the main reason Shah Jahan built the Taj Mahal? A) To honor his beloved wife, Mumtaz Mahal B) To establish his dominance as the ruler of the Mughal dynasty C) To showcase his wealth and power to the world D) To create a luxurious resting place for himself E) To preserve the Mughal architectural tradition  Answer: A\n\n\nTokensiation:\nDue to the fact that the tokenizer returns PyTorch tensors, and when using batched=True with the map function from the datasets library, it expects the function to return lists or numpy arrays, not tensors.\nTo resolve this, you can convert the tensors to lists within the tokenize function. Here’s how you can adjust the tokenize function to avoid an error:\n\ndef tokenize_and_convert_to_list(example):\n    model_inputs = tokenizer(example['source'], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n    \n    # Convert tensors to lists\n    for key, value in model_inputs.items():\n        model_inputs[key] = value.tolist()\n    \n    return model_inputs\n\n# Since we can't execute the map function in this environment, we'll just return the function definition\ntokenize_and_convert_to_list\nNow we tokenize the dataset:\ndef tokenize(example):\n    return tokenizer(example['source'], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n\ntokenized_dataset = processed_dataset.map(tokenize_and_convert_to_list, batched=True)\nDatasetDict({\n    train: Dataset({\n        features: ['context', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'source', 'target', 'input_ids', 'attention_mask'],\n        num_rows: 125\n    })\n})\nNotice we now have context (the articles) saved to sourceand combined the prompt (the question) and subsequent answer options and correct answer into target column.\n\n\nAttention Masks\nIf we print tokenized_dataset we can see attention_mask, which aren’t supported during trainin. Attention masks are typically used in transformers to specify which tokens should be attended to and which should be ignored. This is especially important for sequences of different lengths, to ensure padding tokens don’t influence the model’s output.\nHowever, while finetuning I constantly got the below error:\nAttention mask is not supported during training. Using it might lead to unexpected results.\nThe problem here is that phi-1.5 was pre-trained without padding and the implementation of “MixFormerSequentialForCausalLM” released by Microsoft with the model doesn’t support attention masking during training.\n\nIn the generation function, our model currently does not support beam search (num_beams &gt;1) and “attention_mask” parameters. Furthermore, in the forward pass of the model, we currently do not support outputing hidden states or attention values, or using custom input embeddings (instead of the model’s).\n\nIn the Trainer, set mlm=False to prevent masked language modelling, though the warning still appeared during finetuning!\n\n\nFine-tuning the Model for MCQ Generation\nOnce the data is prepared and the model is set up, the next step is to fine-tune the model on the data. This typically involves specifying training parameters, such as:\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\nLearning rate\n\n\nSmaller learning rates might lead to more stable training but might require more training steps to converge. Learn more.\n\n\n\n\nBatch size\n\n\nYou can experiment with different batch sizes (per_device_train_batch_size). Smaller batch sizes can sometimes lead to more stable training, but they may require more training steps.\n\n\n\n\nNumber of epochs\n\n\nFor static datasets, iterating multiple times as done in multi-epoch training might not be beneficial. It often deteriorates the results, probably due to overfitting [(Raschka, 2023)].\n\n\n\n\nOptimizer\n\n\nWhen finetuning LLMs, the choice of optimizer shouldn’t be a major concern. While Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn’t significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters [(Raschka, 2023)].\n\n\n\n\nMonitor Performance\n\n\nWill use Weights & Bias to monitor performance. You can view my run at here.\n\n\n\n\n\nWe download the phi 1_5 tokenizer again:\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\nRecall, the bitsandbytes is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions.\nAlso, recall that the Phi model does not support device_map=auto so we manually set device_map={\"\":0}, meaning, load tensors to CUDA 0 (THe only GPU I am running this notebook on via Kaggle)\nMore resources here:\n\nLLM.int8() and Emergent Features (Tim Dettemers)\nSoftware Blog post\n\nThe transformers library provides a Trainer class, which simplifies the training process. You would specify the model, the tokenized data, and the training arguments to this class, and then call its train method to start the fine-tuning. In the Trainerbelow, we set mlm=False to prevent masked language modelling.\ntraining_arguments = TrainingArguments(\n        output_dir=\".\",\n        evaluation_strategy=\"steps\",\n        eval_steps=100,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=1,\n        learning_rate=3e-4, #2e-4,\n        lr_scheduler_type=\"cosine\",\n        save_strategy=\"epoch\",\n        logging_first_step=True,\n        logging_steps=100,\n        max_steps=1000,\n        num_train_epochs=1,\n        optim=\"adamw_bnb_8bit\",\n        report_to=\"wandb\",\n        push_to_hub=True\n    )\ntrainer = Trainer(\n    model=model,\n    train_dataset=tokenized_data,\n    args=training_arguments,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\ntrainer.train()\nrepo_id = f'c123ian/phi_test_target'\ntrainer.model.push_to_hub(repo_id)\ntokenizer.push_to_hub(repo_id)\n\n\n\nSaving, Merging, and Inference\nFor faster inference, can merge LorA layers with base. The merged model calls both the base model config.json file and the LoRA modules adapter_config.json, instead of just the adapter_module.\nFor inference, certain configurations like return_tensors=\"pt\" and turning off attention masks with return_attention_mask=False are used to get predictions from the model.\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM\nimport torch\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=torch.float32) # base model 32 NOT 16!\npeft_model = PeftModel.from_pretrained(model, \"c123ian/phi_test_target\", from_transformers=True)\nmodel = peft_model.merge_and_unload()\n#model\nThen we push the new merged model to the hub:\nmodel.push_to_hub(\"c123ian/phi_test_target\") # push merged model\nNow that we have merged the models, it will call base model config.json file and the LoRA modules adapter_config.json, rather then just the adapter_module.\n\n\n\nInference: Generating MCQ from Textbooks Test:\nSo next lets load the new mereged model, finetuned on MCQ 200 sample dataset, and the original phi 1_5 tokens:\n# Load the model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"c123ian/phi_test_target\", trust_remote_code=True, torch_dtype=torch.float32)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\nLets do a quick test:\n# Define the input context\ncontext = '''RMS Titanic was a British passenger liner, operated by the White Star Line, that sank in the North Atlantic Ocean on 15 April 1912 after striking an iceberg during her maiden voyage from Southampton, England, to New York City, United States. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making it the deadliest sinking of a single ship up to that time. '''\n\n# Define the instruction for generating an MCQ\ninstruction = '''\"Based on the provided text, create a multiple-choice question with 5 options labeled A), B), C), D), and E), and provide the correct answer. Ensure that the answer options are of similar length and closely aligned wording.\"'''\n\n# Tokenize the input\ninputs = tokenizer(instruction + context, return_tensors=\"pt\", return_attention_mask=False)\n\n# Generate the output\noutputs = model.generate(**inputs, max_length=512)\ntext = tokenizer.batch_decode(outputs)[0]\n\nprint(text)\n\"Based on the provided text, create a multiple-choice question with 5 options labeled A), B), C), D), and E), and provide the correct answer. Ensure that the answer options are of similar length and closely aligned wording.\"RMS Titanic was a British passenger liner, operated by the White Star Line, that sank in the North Atlantic Ocean on 15 April 1912 after striking an iceberg during her maiden voyage from Southampton, England, to New York City, United States. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making it the deadliest sinking of a single ship up to that time. \nAnswer: The question is labeled A), B), C), D), and E). The correct answer is D). The question is labeled A), B), C), D), and E), and the correct answer is D). The correct answer is dead. The questions and answers provided in this lesson are not vetted by VB. If you need more questions, or would like to learn more about this lesson, please contact us. Using VB. Add labels to the question \"The Titanic was a …\" label, A), B), C), D), and E), and the correct answer is labeled E). The question is labeled A), B), C), D), and E), and the correct answer is E). Thank you for using VB. Our lessons We start with the tragic Titanic, as mentioned in the opening paragraph. As we've seen before, the Titanic was a passenger liner (not an ferry [boat]) operated by the White Star Line (a company based in Southampton, England); it sank in the North Atlantic Ocean (in New York City, United States) in 1912 after striking an iceberg during its first voyage from Southampton (to New York).  Of the estimated 1,224 passengers and crew aboard, more than 1,500 died; the Titanic is one of the most tragic (and deadliest) shipping disasters up to that time. \nQuestion: What was the object of the ship the text describes in this lesson?\nAnswer: The objects of the text describe \"the Titanic.\" (B) The objects of the text describe \"the Titanic,\" and the correct answer is\nQuestion: According to the text, what was one \"feature\" of the Titanic that was \"not fully understood at the design stage?,\" B), C), D), and E), and the correct answer is E)? The question is labeled A), B), C), D), and E), and\nHmmm.. I can sort of see the attempt to generate MCQ. I also tried to instill Alpaca style formatting for the instruction - to try and standerdize the MCQ text generation.\ndef format_instruction(sample):\n    return f\"\"\"### Instruction:\nBased on the provided text, create a multiple-choice question with 5 options labeled A), B), C), D), and E), and provide the correct answer. Ensure that the answer options are of similar length and closely aligned wording.\n\n### Input:\n{sample['context']}\n\n#### MCQ Question:\n{sample['prompt']}\n\nA) {sample['A']}\nB) {sample['B']}\nC) {sample['C']}\nD) {sample['D']}\nE) {sample['E']}\n\n#### Correct Answer:\n{sample['answer']}\nWe can inspect each part:\n{context}\n{'RMS Titanic was a British passenger liner, operated by the White Star Line, that sank in the North Atlantic Ocean on 15 April 1912 after striking an iceberg during her maiden voyage from Southampton, England, to New York City, United States. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making it the deadliest sinking of a single ship up to that time. '}\n{prompt}\n{'Answer: The Titanic sank in the North Atlantic Ocean on 15 April 1912 and died of over 1,500 deaths. '}\n\n{sample['A']}\n{'16. \"Which of the following best describes the Titanic?'}\n{answer}\n{'Answer: D. All of the above. '}\n\nFrom above, we can see things dont lne up (for example, sample['A'] which should be A) is in fact the question. I also tested more specific instructions (prompt) to try make the output text structire be more consistent, but to no avail. You may notice the gernartion also appears to end mid-sentence, this is due to the max_length parametr, which will adjust later.\n\n\nCustom dataset number 2:\nThe next step is to load the larger dataset, which is just a collection of Khan academy articles that I scraped.\n\nfrom datasets import load_dataset\n\ndataset_context = load_dataset(\"c123ian/khan_academy_context\") \nDatasetDict({\n    train: Dataset({\n        features: ['context'],\n        num_rows: 2167\n    })\n})\n\n\nMax token sequence length:\nError message was recieved when running an inference test on a longer khan article. Token indices sequence length is longer than the specified maximum sequence length for this model (2200 &gt; 2048). Running this sequence through the model will result in indexing errors\nThe error you’re encountering indicates that some of your tokenized sequences are longer than the model’s maximum allowed sequence length (2048 tokens for models like BERT, GPT-2, etc.).\nIf we inspect the large khan dataset we can see that the percentage of texts over 2048 tokens: 47.76%\n\n\n\ndataset token length\n\n\nTo address this, you can take the following steps:\n\nTruncate Sequences During Tokenization: Ensure that your tokenization function truncates sequences that exceed the model’s maximum length.\nFilter Out Long Sequences: Another approach would be to filter out examples from your dataset that exceed the model’s maximum sequence length after tokenization.\n\nWe opted to filter out examples where the number of tokens exceeds the model’s limit:\n# Create column for text_length\ndf['text_length'] = df['context'].str.len()\n# Filter rows where text_length &lt;= 2048\nfiltered_df = df[df['text_length'] &lt;= 2048]\nTo convert a pandas DataFrame back to a DatasetDict using the datasets library, you can use the from_pandas() method. Let’s convert the filtered_df’s “context” column back into a DatasetDict object. Here’s how you can do it:\nfrom datasets import Dataset, DatasetDict\n\n# Convert the 'context' column of filtered_df to a Dataset\ncontext_dataset = Dataset.from_pandas(filtered_df[['context']])\n\n# Convert the Dataset to a DatasetDict with a 'train' split\ncontext_dataset_dict = DatasetDict({\"train\": context_dataset})\n\n# renam\ndataset = context_dataset_dict\ndataset\nDatasetDict({\n    train: Dataset({\n        features: ['context', '__index_level_0__'],\n        num_rows: 1132\n    })\n}\nThe '__index_level_0__' is the old index, incase we wanted to reverse engineer. We can remove it with:\n# Code to remove the '__index_level_0__' column\ndata = data.map(lambda example: {'context': example['context']}, remove_columns=['__index_level_0__'])\nLets make a random small subset for testing the model.\nimport random\n\n# Generate 5 random indices\nrandom_indices = random.sample(range(len(data['train'])), 5)\n\n# Select the random rows\ntest_data = data['train'].select(random_indices)\ntest_data\nWhile we have handled the max input tokens issue by filtering the dataset, we also want to ensure that the total token count (input tokens + generated tokens) does not exceed a specific threshold (e.g., 2048 tokens), and you’re aiming for a generated MCQ that won’t be more than 500 tokens, you can dynamically set the max_new_tokens based on the input length.\n\nCalculate the number of tokens in the input.\nSubtract this number from the desired maximum total tokens (e.g., 2048) to determine how many new tokens can be generated.\nSet this calculated value as the max_new_tokens parameter.\n\n\n\nInference: Generating MCQ from Textbooks from dataset \nAlso for faster inference, were making sure the model device is set to the GPU.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef generate_mcqs(test_data):\n    # Check for GPU availability\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    # Load the model and tokenizer\n    #model = AutoModelForCausalLM.from_pretrained(\"c123ian/phi_test_target\", trust_remote_code=True, torch_dtype=torch.float32).to(device)\n    #tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n    \n    # Define the instruction for generating an MCQ\n    instruction = '''\"Based on the provided text, create a multiple-choice question with 5 options labeled A), B), C), D), and E), and provide the correct answer. Ensure that the answer options are of similar length and closely aligned wording.\"'''\n\n    # Create an empty list to store the results\n    results = []\n\n    # Iterate over each context and generate MCQs with tqdm progress bar\n    for context in tqdm(test_data[\"context\"], desc=\"Generating MCQs\"):\n        # Tokenize only the instruction for input\n        inputs = tokenizer(instruction, return_tensors=\"pt\", return_attention_mask=False).to(device)\n\n        # Generate the output\n        outputs = model.generate(**inputs, max_length=912)  # Note: Adjust max_length as per your model's limitations\n        generated_mcq = tokenizer.batch_decode(outputs)[0]\n\n        # Add the result to the results list\n        results.append({'Context': context, 'Generated Text': generated_mcq})\n\n    # Convert the results list to a dataframe\n    df = pd.DataFrame(results)\n    return df\n\n# Usage example\ndf = generate_mcqs(test_data)\n\n\n\nConclusion\nIn this journey of fine-tuning the phi-1.5 model, we’ve traversed through the myriad facets of large language model optimization. Fine-tuning is far from a straightforward process, and as our experience with the Kaggle Science Exam competition highlighted, unforeseen challenges can emerge, especially when dealing with synthetic data and distinct training methodologies.\nThe choice of phi-1.5, with its unique synthetic “textbook” training approach, offered both opportunities and challenges. While its compact nature made it manageable, its limitations became apparent when applied to specific tasks like generating MCQs from Khan Academy articles. However, these challenges underscore the importance of model selection, especially in light of available alternatives like Mistral 7B.\nThrough this process, we’ve gained invaluable insights into the world of NLP tasks. The tools and techniques, from transformers to LoRA, have showcased the potential of fine-tuning and adaptation. Even though we encountered hurdles, the learnings derived are profound. They serve as a testament to the rapidly evolving nature of AI and the importance of continuous exploration.\nSpecial thanks to contributors in the community, like Vasanthengineer, whose shared knowledge significantly enriched this endeavor. As we wrap up, it’s essential to remember that in the realm of AI, every challenge is an opportunity, and every setback a lesson. The path to mastering fine-tuning is iterative, and with each iteration, we come closer to harnessing the full potential of large language models."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello my name is Cian Prendergast, I am from Dublin, Ireland and love to gather my own datasets for machine learning and AI exploration :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cian Prendergast",
    "section": "",
    "text": "Finetuning the phi-1.5 Language Model 📝\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nA Unified Approach to Web Scraping 🌐\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing Deep Learning to Classify Bee Species: An In-Depth Guide 🐝\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing Folium’s HeatMapWithTime to Unearth Geo-Data Insights: An Applied Walkthrough 🦅\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDublin’s Property Prices: A Spatial Perspective 🏠\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nExtractive Question Answering with Data from Dell Forums ⛏️\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDell Technical Support Email Classification: A Deep Dive\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/a_Web Scraping/blog_v2.html",
    "href": "posts/a_Web Scraping/blog_v2.html",
    "title": "A Unified Approach to Web Scraping 🌐",
    "section": "",
    "text": "1. Laying the Groundwork:\nEvery mission needs a clear objective. Khan Academy is a reservoir of educational content. Imagine its vast library as a series of interconnected textbooks. Each lesson is like a chapter, and our goal is to digitally ‘link’ these textbooks, extracting their essence for our data needs.\n\n\n2. Reconnaissance - The Digital Marco Polo:\nMuch like the game of Marco Polo, where one player calls out “Marco!” and others respond with “Polo!”, web scraping is about sending a probe and waiting for a response. Using browser developer tools, we ‘call out’ to the website, inspecting elements and deciphering its structure. Often, elements like the ‘Load More’ button are hidden gems that we need to uncover for effective scraping.\nHere is a quick video showing the method in action:\n\n\n\n3. Crafting the Toolset:\nWith insights from manual inspection, we turn to ChatGPT. By providing specific HTML elements, we get back Python functions, tailor-made for data extraction.\nOnce we have the HMTL elements of interest copied: \nWe can feed them into ChatGPT:\n!\n\n\n4. Iterative Building - Traversing the Digital Tree:\nWeb scraping is akin to exploring a vast digital forest. Each website represents a unique tree within this forest, and the goal is to traverse from the leaves (finest details) to the root (landing or home page), gathering valuable data along the way.\nFor an effective traversal:\n\nStart Small, Think Big: Begin by targeting a specific, granular detail on the website. This could be a single article, a forum post, or a lesson. By mastering the extraction process at this micro-level, you lay the groundwork for larger, more complex scraping tasks.\nExample from Khan Academy:\ndef get_lesson_details(url):\n    driver.get(url)\n    content = driver.find_element_by_xpath('//div[@class=\"perseus-renderer\"]').text\n    return content\n\nBranch Out: Once you’ve successfully extracted details from the ‘leaf’ level, start branching out. Explore related content within the same category or section. This could involve scraping all lessons within a particular topic or all posts within a forum thread.\nExample from Khan Academy:\ndef get_topic_details(url):\n    driver.get(url)\n    lesson_links = driver.find_elements_by_xpath('//a[@class=\"link_1uvuyao-o_O-nodeStyle_j7os3g-o_O-nodeStyleHighlight_aacz5b\"]')\n    topic_content = [get_lesson_details(link.get_attribute('href')) for link in lesson_links]\n    return topic_content\n\n\n\n\n/science/high-school-biology/hs-biology-foundations\n\n\n\nConnect the Dots: As you move further up the ‘tree’, look for patterns and connections. Understand how different sections or categories are linked. This interconnectedness will guide your scraping journey, ensuring you capture a holistic snapshot of the website’s content.\nExample from Dell IT Support:\ndef extract_thread_details(url):\n    driver.get(url)\n    post_links = driver.find_elements_by_xpath('//div[@class=\"lia-thread-topic\"]//a[@class=\"page-link lia-link-navigation lia-custom-event\"]')\n    thread_content = [extract_post_text(link.get_attribute('href')) for link in post_links]\n    return thread_content\nReach the Root: With detailed insights from the branches and leaves, navigate back to the landing or home page. This final step should encapsulate all the data extraction methods you’ve developed, offering a comprehensive view of the website’s content.\nExample from Khan Academy:\ndef navigate_to_khan_landing(url):\n    driver.get(url)\n    topic_links = driver.find_elements_by_xpath('//a[@class=\"link_1uvuyao-o_O-nodeStyle_j7os3g-o_O-nodeStyleHighlight_aacz5b\"]')\n    landing_content = [get_topic_details(link.get_attribute('href')) for link in topic_links]\n    return landing_content\n\n\n\n\n/science\n\n\n\nIterate and Refine: Web scraping is rarely a one-shot process. As you traverse the digital tree, you’ll likely encounter challenges or discover more efficient paths. Continuously iterate on your methods, refining and optimizing as you delve deeper into the forest.\n\n\nBy adopting an iterative building approach, you ensure thoroughness and efficiency in your web scraping endeavors. It’s about mastering the micro to effectively navigate the macro. Each step, each function, is a stepping stone, guiding you through the vast digital terrain.\n\n\n5. Harvest, Refine, and Polish:\nPost extraction, data needs cleansing. This includes handling placeholders like [URL], removing Personal Identifiable Information (PII) such as names (often found adjacent to “@” symbols), and dealing with missing values.\n# Data cleaning code\n\ndef clean_data(data_list):\n    cleaned_data = []\n\n    for data in data_list:\n        # Remove URL placeholders\n        data = data.replace('[URL]', '')\n\n        # Remove PII (names associated with '@')\n        if '@' in data:\n            continue\n        \n        # Remove NaN or empty entries\n        if not data:\n            continue\n\n        cleaned_data.append(data)\n\n    return cleaned_data\n\n# Assuming 'raw_data' is the list of extracted data\ncleaned_data = clean_data(raw_data)\n\n# Convert the cleaned data into a CSV format\nimport pandas as pd\ndf = pd.DataFrame(cleaned_data)\ndf.to_csv('cleaned_data.csv', index=False)\n\n\n6. Challenges - The Twists and Turns of Web Scraping:\nNavigating the digital realm in pursuit of data treasures is not without its share of challenges. These hurdles test the resilience of your scraper, and by extension, your problem-solving mettle. Here are some of the challenges we encountered and the lessons they taught:\n\n1. Ever-Changing Web Landscapes:\nWebsites evolve. Whether it’s a design overhaul or a restructuring of content, these changes can render a previously efficient scraper ineffective. A prime example from our journey was with the Dell IT Support site. When I last scraped the site, all forum posts, including the “solved” ones, were housed together. Fast forward to now, and the “solved posts” have been segregated into their own section. A scraper tailored for the earlier structure would be obsolete today. Such changes emphasize the importance of regular scraper maintenance and adaptability.\nCheck out this earlier scraping approach which is now obsolete due to site changes.\n\n\n2. The Wild Card of User-Generated Content:\nWhen dealing with platforms that allow user contributions, you’re diving into unpredictable waters. Without stringent formatting rules, users might introduce content that throws a wrench in your scraping logic. From embedding multimedia to using a whirlwind of emojis and special characters, user-generated content is a wild card. During our scraping mission, one such unexpected content format almost derailed a nearly complete scraping method!\n\n\n\nExample, users marks question as solved answer\n\n\n\n\n\nExample 2, having two solved posts\n\n\n\n\n3. The Delicate Art of Data Cleaning:\nPost extraction, the challenge shifts to refining this raw harvest. Decisions made during this phase profoundly impact the usability and quality of the data. For instance, URLs in the content: Are they useful metadata, or do they risk confusing a machine learning model? And what about images? Should they be omitted entirely or replaced with a placeholder like [IMAGE]?\nName extraction poses its own set of challenges. Using the “@” character as a heuristic to remove names is handy but not exhaustive. Turning to a list of real names for cleaning is tempting but fraught with pitfalls. Common words that double as names, like “Will”, might get erroneously removed, distorting the data’s meaning.\n\n\n\n\nWrapping Up:\nIn web scraping, it’s often wise to cast a wide net. For instance, even though the community Q&A on Khan Academy wasn’t the primary focus, we extracted it—because, why not? Always gather more data than you think you’ll need.\n\n\n\nkhan academy comments section\n\n\ndef extract_community_qa(url):\n    \"\"\"\n    Extract community Q&A content from a given Khan Academy page.\n    \n    Args:\n    - url (str): The URL of the Khan Academy page with community Q&A.\n    \n    Returns:\n    - list[dict]: A list of dictionaries, where each dictionary contains a question and its answer(s).\n    \"\"\"\n    driver.get(url)\n    \n    # Find all question elements on the page. The specific XPath would depend on the actual structure.\n    question_elements = driver.find_elements_by_xpath('//div[@class=\"qa-question\"]')  # Placeholder XPath\n    \n    # Initialize a list to store Q&A pairs.\n    qa_content = []\n    \n    for q_elem in question_elements:\n        # Extract the text of the question.\n        question_text = q_elem.find_element_by_xpath('.//div[@class=\"question-text\"]').text  # Placeholder XPath\n        \n        # Extract corresponding answer(s).\n        answer_elements = q_elem.find_elements_by_xpath('.//div[@class=\"answer-text\"]')  # Placeholder XPath\n        answers = [answer_elem.text for answer_elem in answer_elements]\n        \n        # Append the Q&A pair to our content list.\n        qa_content.append({\n            'question': question_text,\n            'answers': answers\n        })\n    \n    return qa_content\nHaving data reserves can be invaluable for unforeseen projects down the line. The digital realm is vast and filled with treasures; dive in, gather generously, and let your data trove be the cornerstone of future innovations."
  },
  {
    "objectID": "posts/Bird/Bird_Visualisation_ver_HeatMap.html",
    "href": "posts/Bird/Bird_Visualisation_ver_HeatMap.html",
    "title": "Using Folium’s HeatMapWithTime to Unearth Geo-Data Insights: An Applied Walkthrough 🦅",
    "section": "",
    "text": "By Cian Prendergast\n\nIntroduction:\nThere’s a haunting beauty in the songs of the birds of Ireland. However, recent studies point towards a rather bleak reality. A whopping 54 species of birds, which equates to over 26% of the studied population, now find themselves on the endangered Red list. This is a clarion call for action.\n\n\n\nimage.png\n\n\nTo better understand and advocate for these birds, we delve into a dataset provided by Biodiversity Ireland. In this article, we will process and analyze this dataset, presenting our findings to highlight the importance of conserving these species.\n\n\nData Retrieval and Preparation:\nOur dataset, initially a .txt file, was converted into a more manageable .csv format. Before diving into the data, let’s load our essential libraries:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nWhile briefly reviewing the dataset, I observed that many longitude/latitude values were missing. These were predominantly from older entries (1990’s) that utilized the Irish OSI Map references. While converting these was beyond the scope of my available time, you can learn more at th OSI Co-ordinate Converter. Once the older OSI references were removed we were left with 48,172 rows!\nThe dataset was read into a DataFrame. Pay attention to the delimiter used to split the data, given the tab-separated nature of our dataset:\ndf = pd.read_csv(r\"C:\\Users\\Cian\\Workstation\\Datasets\\Bird_dataset.txt\", \"\\t+|\\t+\", encoding='UTF-8', engine='python')\nWe can have a quick look at the dataset using df.head()\nRecordKey   StartDate   EndDate DateType    Date    TaxonVersionKey TaxonName   Longitude   Latitude    Grid Reference  ... Habitat description Abundance   Determiner name Survey name Common name Site description    Source  Habitat code (Fossitt, 2000)    Type of sighting    Activity\n0   9.97173E+16 28/02/2022  28/02/2022  D   28/02/2022  NHMSYS0000530420    Lagopus lagopus -9.859652   53.883251   WGS84   ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n1   9.97173E+16 21/02/2022  21/02/2022  D   21/02/2022  NHMSYS0000530420    Lagopus lagopus -6.299050   53.037785   WGS84   ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n2   9.97173E+16 16/02/2022  16/02/2022  D   16/02/2022  NHMSYS0000530420    Lagopus lagopus -6.221776   53.156033   WGS84   ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n3   9.97173E+16 04/02/2022  04/02/2022  D   04/02/2022  NHMSYS0000530420    Lagopus lagopus -6.461017   52.940109   WGS84   ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n4   9.97173E+16 04/02/2022  04/02/2022  D   04/02/2022  NHMSYS0000530420    Lagopus lagopus -6.460975   52.940112   WGS84   ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n\n\nData Cleaning:\nThe initial exploration showed some rows with missing longitude and latitude data, which were removed for the sake of accurate geospatial analysis:\ndf.dropna(subset = ['Longitude', 'Latitude'], inplace=True)\nTo ensure that our data truly represents bird sightings in Ireland, we plotted our latitude and longitude data. Our aim was to identify outliers that fall outside the geographical boundaries of Ireland:\n# reference: https://github.com/Shreyas3108/house-price-prediction/blob/master/housesales.ipynb\nplt.figure(figsize=(10,10))\nsns.jointplot(x=df.Latitude.values, y=df.Longitude.values, size=10)\nplt.ylabel('Longitude', fontsize=12)\nplt.xlabel('Latitude', fontsize=12)\nplt.show()\nsns.despine\n\n\n\nimage.png\n\n\nFrom this visual, outliers were particularly evident along the latitude, likely coming from UK locations. Using percentiles, we pruned the dataset to fall within the 1% to 99% range for both longitude and latitude:\n# returns percentiles as a table\ndf[['Longitude', 'Latitude']].describe(percentiles=[.01,.05,.1,.25,.5,.9,.95,.99]) # maybe dipslay by two decomal point\n\n\n\nimage.png\n\n\n# Filter out outliers if needed by taking range between 1% - 99%\ndf_f = df[df.apply(lambda row: (-10.13&lt;=row['Longitude']&lt;= -5.61) and (51.43&lt;=row['Latitude']&lt;=54.82), axis=1)]\nNow, our dataset is pruned to better reflect the geolocation of bird sightings in Ireland.\n\n\n\nimage.png\n\n\n\n\nSingle Count Per Occurrence - A Dive into Data Processing\nNext we’re going to walk through the process of converting cumulative counts into single counts per occurrence, a critical preprocessing step for more meaningful visualizations and statistical analyses!\n\n\nEnsuring Date is Correctly Formatted\nFirst and foremost, we need our dates to be consistently formatted so that we can perform temporal analyses later. In the Python world, this is achieved using the fantastic Pandas library:\ndf['Date'] = pd.to_datetime(df['Date'])\nHowever, you might encounter the SettingWithCopyWarning. This warning is Pandas’ way of saying “Be cautious, you might be changing a slice from a DataFrame.” It arises when dealing with views vs. copies. You can find more about this in the Pandas documentation.\n\n\nCreating Count Column\nWe’re moving away from cumulative counts and focusing on single occurrences. This simplification will aid in creating heatmaps later.\ndf['my_count'] = 1\nAgain, you might see the SettingWithCopyWarning. Just a gentle reminder to tread carefully.\n\n\nTaxonName Check\nIt’s essential to understand the unique counts of species (TaxonNames) to get a sense of the diversity in the dataset:\npd.set_option(\"display.max_rows\", None)\nprint(df['TaxonName'].value_counts(ascending=True))\n\n\n\nimage.png\n\n\n\nAlso, displaying the total count per unique bird species:\nfor bird in df['TaxonName'].unique():\n    print('TaxonName {}; has a total count of {:.0f}'.format(bird, (df[df['TaxonName'] == bird]['Total_Count'].max())))\n\n\n\nimage.png\n\n\n\n\nGrouping Species\nAccording to the Red List, bird species are classified into Breeding, Wintering, Passage, and Breeding&Wintering. Grouping them this way can provide valuable insights.\nYou can filter rows in a dataframe based on a column’s values. This link provides comprehensive ways to do this.\nFor instance:\ndf_b = df[df['TaxonName'].isin(['Coturnix coturnix', 'Perdix perdix', ... ])]\nBy repeating this for all categories, we can get a count of the total number of bird species:\nprint(\"The number of bird species from Red List found in the dataset is: \", amount)\n\n\nVisualization - HeatMaps with Time\nVisualization is an integral component in the data analysis pipeline, offering a unique avenue to present complex datasets in a digestible format. But when we have temporal and spatial dimensions in the mix, the challenge arises: How do we efficiently display both? The answer is quite simply: Heatmaps.\nHeatmaps are a fantastic way to visualize dense data. But when combined with time, they provide an animated sequence that’s both visually appealing and informative.\nWe will use the Folium library. With a combination of other plugins like HeatMap and HeatMapWithTime, we can create dynamic visualizations:\nimport folium\nfrom folium.plugins import HeatMap, HeatMapWithTime\n...\nm = folium.Map(location=center, zoom_start=7, tiles='Stamen Toner')\n...\nhm = HeatMap(data=df_b[['Latitude','Longitude','my_count']]).add_to(m)\nIn the end, this preprocessing and visualization process gives us a vivid picture of the dataset’s richness. We can see not only the spatial density of bird species but also track their movements and concentrations over time.\n\n\nOrganizing Your Data\nFirst, we need our data in the right format. Given the dates from various dataframes df_b, df_bw, df_w, and df_p, you’d want to sort them:\ndf_b['Date'].sort_values().unique()\nAfter sorting dates from various datasets, we obtained a unique array of dates. The mentioned process is crucial because we intend to visualize data for each specific date.\narray(['2003-03-10T00:00:00.000000000', '2004-08-29T00:00:00.000000000',\n       '2004-12-02T00:00:00.000000000', '2006-07-01T00:00:00.000000000',\n       '2006-09-21T00:00:00.000000000', '2008-12-07T00:00:00.000000000',\n       '2009-03-08T00:00:00.000000000', '2009-08-26T00:00:00.000000000',\n       '2009-09-23T00:00:00.000000000', '2010-03-08T00:00:00.000000000',\n       '2010-04-08T00:00:00.000000000', '2010-07-08T00:00:00.000000000',\n       '2010-08-09T00:00:00.000000000', '2010-09-13T00:00:00.000000000',\n       '2010-09-16T00:00:00.000000000', '2010-09-17T00:00:00.000000000',\n       '2010-09-21T00:00:00.000000000', '2010-10-17T00:00:00.000000000',\n       '2010-10-23T00:00:00.000000000', '2011-01-09T00:00:00.000000000',\n       '2011-03-09T00:00:00.000000000', '2011-08-19T00:00:00.000000000',\n       '2011-08-20T00:00:00.000000000', '2011-08-21T00:00:00.000000000',\n       '2011-08-31T00:00:00.000000000', '2011-09-13T00:00:00.000000000',\n       '2011-09-17T00:00:00.000000000', '2011-10-10T00:00:00.000000000',\n       '2011-12-09T00:00:00.000000000', '2016-11-19T00:00:00.000000000',\n       '2017-02-12T00:00:00.000000000', '2017-10-15T00:00:00.000000000',\n       '2018-01-15T00:00:00.000000000', '2018-04-18T00:00:00.000000000',\n       '2018-05-03T00:00:00.000000000', '2020-08-26T00:00:00.000000000',\n       '2020-10-09T00:00:00.000000000', '2020-12-09T00:00:00.000000000',\n       '2021-05-16T00:00:00.000000000'], dtype='datetime64[ns]')\n\n\nCreating a Time-Series HeatMap\nThe power of folium comes into play when creating time-series heatmaps. First, we break down our data into hours for each category:\ndf_hour_list_b = []\nfor sale_date in df_b['Date'].sort_values().unique():\n    df_hour_list_b.append(df_b.loc[df_b['Date'] == sale_date, ['Latitude', 'Longitude', 'my_count']].groupby(['Latitude', 'Longitude']).sum().reset_index().values.tolist())\nIn the code snippet above, for every unique date in our sorted dataset, we’re gathering data on Latitude, Longitude, and my_count. This helps in aggregating bird counts (or whichever entity is represented by my_count) at each unique geographic location.\n\n\nVisualizing the HeatMap\nHere’s where the magic happens. Using the organized data, we create a heatmap:\nbase_map_breeding = folium.Map(location=[53.305494, -7.737649], tiles='Stamen Toner',  zoom_start=6)\nHeatMapWithTime(df_hour_list_b, radius=25, gradient={0.2: 'blue', 0.4: 'lime', 0.7: 'orange', 1: 'red'}, min_opacity=0.5, max_opacity=0.8, use_local_extrema=True).add_to(base_map_breeding)\nThe HeatMapWithTime function creates a time-series heatmap with a gradient color scale. This means the areas with higher bird counts would appear ‘red’, and those with lower counts will lean towards ‘blue’.\n\n\nExpanding the Analysis\nThe method described above can be expanded for the other categories: df_bw, df_w, and df_p. The end result would be a series of heatmaps for each category.\n\n\nMonthly Grouping for Better Temporal Insights\nOften, daily data can be noisy. To see trends better, you might want to group data by month:\ndf_months = []\nfor sale_date in df_b['Date'].sort_values().unique():\n    df_months.append(df_b.loc[df_b['Date'] == sale_date, ['Latitude', 'Longitude', 'my_count']].groupby(['Latitude', 'Longitude']).sum().reset_index().values.tolist())\n\nbase_map_test\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nConclusion:\nIn the context of the provided code, these heatmaps can provide profound insights into patterns, be it the migration, population density, or any event-based clustering of birds in different geographical locations over time. By visualizing these trends, organizations and researchers can derive actionable insights, such as resource allocation for bird conservation, identifying hotspots for bird-watching, and understanding migration patterns.\nHope you had a great time reading this! 📚 For a deeper dive into heatmap techniques and visualizations, feel free to check out this article 📖 here and the GitHub repo 🖥️ here.\nAlso for the full code and datasets please see my own GitHub repo 🖥️ here."
  },
  {
    "objectID": "posts/SQuAD/squad.html",
    "href": "posts/SQuAD/squad.html",
    "title": "Extractive Question Answering with Data from Dell Forums ⛏️",
    "section": "",
    "text": "By Cian Prendergast\nHello there, data enthusiasts! Today, we’re diving deep into the world of Extractive Question Answering (QA). For those unfamiliar, extractive QA is all about pinpointing exact answers from large text sources. Imagine being able to instantly find the solution to a hardware issue from an ocean of forum posts. Sounds cool, right? Let’s get started!"
  },
  {
    "objectID": "posts/SQuAD/squad.html#tokenization-the-first-step-towards-understanding",
    "href": "posts/SQuAD/squad.html#tokenization-the-first-step-towards-understanding",
    "title": "Extractive Question Answering with Data from Dell Forums ⛏️",
    "section": "Tokenization: The First Step Towards Understanding",
    "text": "Tokenization: The First Step Towards Understanding\nWelcome back! If you remember from our last post, we dived deep into preparing our data. This time, let’s talk about how we make sense of this data: Tokenization.\n\nWhat’s Tokenization Anyway?\nIn simple terms, tokenization is the process of converting our text (questions and contexts) into smaller chunks, called tokens. This is an essential step because our models don’t understand text as we do; they understand numbers. Tokens are a bridge between human-readable text and machine-understandable numbers.\nfrom transformers import AutoTokenizer\n\nmodel_ckpt = \"deepset/minilm-uncased-squad2\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nUsing a sample question and context, we can see tokenization in action:\nquestion = \"How much music can this hold?\"\ncontext = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on file size.\"\"\"\ninputs = tokenizer(question, context, return_tensors=\"pt\")\n\ninput_df = pd.DataFrame.from_dict(tokenizer(question, context), orient=\"index\")\ninput_df\n\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n…\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n25\n\n\n26\n\n\n27\n\n\n\n\n\n\ninput_ids\n\n\n101\n\n\n2129\n\n\n2172\n\n\n2189\n\n\n2064\n\n\n2023\n\n\n2907\n\n\n1029\n\n\n102\n\n\n2019\n\n\n…\n\n\n2061\n\n\n2055\n\n\n25961\n\n\n2847\n\n\n5834\n\n\n2006\n\n\n5371\n\n\n2946\n\n\n1012\n\n\n102\n\n\n\n\ntoken_type_ids\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n…\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nattention_mask\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n…\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n3 rows × 28 columns\n\n\n\n\nSimplifying with Pipelines\nTransformers offer a nifty feature called pipelines, which simplifies our QA process:\nfrom transformers import pipeline\n\npipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\npipe(question=question, context=context, topk=3)\nBut what if our context doesn’t contain the answer? The pipeline gracefully handles such scenarios:\npipe(question=\"Why is there no data?\", context=context, handle_impossible_answer=True)\n{'score': 0.906841516494751, 'start': 0, 'end': 0, 'answer': ''}\n\n\n\nSliding Windows: Seeing the Bigger Picture\nRemember the challenge of tokenizing long passages? The sliding window approach is our knight in shining armor. By using return_overflowing_tokens=True and specifying a stride, we ensure that our model gets overlapping views of a passage without missing any potential answers.\nLet’s see it in action:\nexample = df.iloc[0][[\"question\", \"context\"]]\ntokenized_example = tokenizer(example[\"question\"], example[\"context\"], \n                              return_overflowing_tokens=True, max_length=100, \n                              stride=25)\nEach window has a specific number of tokens:\nfor idx, window in enumerate(tokenized_example[\"input_ids\"]):\n    print(f\"Window #{idx} has {len(window)} tokens\")\nWindow #0 has 89 tokens\n\nWhen we decode these tokens, we can visualize the overlapping windows:\nfor window in tokenized_example[\"input_ids\"]:\n    print(f\"{tokenizer.decode(window)} \\n\")\n[CLS] does v9 license works with v8 platform? when i try to import the v9 license i am having this error. lic017. [SEP] idrac v9 license is not compatible with idrac v8 platforms. idrac license is tied with each system and you will not be able to install on another system other than the server it is intended. thanks, dell - shine k # iwork4dellview solution in original post [SEP] \nWith this setup, our model gets a comprehensive view of the context, ensuring we don’t miss out on potential answers.\n\n\nElasticSearch: Turbocharging Our QA System\nElasticSearch is a powerhouse for search and analytics. It’s our chosen tool to index and retrieve relevant contexts efficiently when posing questions.\nSetting it up involves:\n\nDownloading ElasticSearch:\n\nurl = \"https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.9.0-linux-x86_64.tar.gz\"\n!wget -nc -q {url}\n!tar -xzf elasticsearch-8.9.0-linux-x86_64.tar.gz\n\nPerforming system configurations:\n\n!sudo usermod -aG docker $USER\n!sudo reboot\n\nInstalling required packages:\n\n!pip install --upgrade numexpr\nWith ElasticSearch in place, our QA system is turbocharged, ensuring rapid and accurate retrievals.\n\n\nLaunching ElasticSearch (The Docker Way)\nFor those using Docker, here’s a quick way to get ElasticSearch up and running:\nfrom haystack.utils import launch_es\n\nlaunch_es()\nA quick check to ensure ElasticSearch is running:\n!curl -X GET \"localhost:9200/?pretty\"\n\n\nPopulating ElasticSearch: From DataFrame to DocumentStore\nTo make our data retrievable, we need to format it appropriately and populate our ElasticSearch index:\ndocs = []\n\n# Convert DataFrame rows to dictionary format for the DocumentStore\nfor _, row in df.iterrows():\n    doc = {\n        'content': row['context'], \n        'meta': {}  # You can add additional metadata here if needed.\n    }\n    docs.append(doc)\n\n# Populate the DocumentStore\ndocument_store.write_documents(docs, index=\"document\")\n\n\nBM25: Simple Yet Powerful\nBM25 is a classic technique in information retrieval. Let’s initialize it and see it in action:\nfrom haystack.nodes.retriever import BM25Retriever\n\nbm25_retriever = BM25Retriever(document_store=document_store)\nA quick retrieval test:\nquery = \"How to fix my KVM?\"\nretrieved_docs = bm25_retriever.retrieve(query=query, top_k=3)\n\nprint(retrieved_docs[0])\n&lt;Document: id=1a93a3494d130482abb1d9341f3a4642, content='With a KVM  you really will need to do some deductive troubleshooting.If port one is the only port w...'&gt;\n\n\nDense Passage Retrieval (DPR)\nWhile BM25 is great, there’s another retrieval technique on the horizon: DPR. It’s a more advanced method that uses deep learning to understand context and provide better results. You can learn more about DPR here.\nDPR offers a more granular retrieval technique, diving deeper into our contexts to find the most relevant snippets:\nfrom haystack.nodes import DensePassageRetriever\n\ndpr_retriever = DensePassageRetriever(\n    document_store=document_store,\n    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n    embed_title=False\n)\nWith DPR initialized, it’s time to update our document embeddings:\ndocument_store.update_embeddings(retriever=dpr_retriever)\n\n\nThe Power of Pipelines\nBy combining our retriever and reader, we form a powerful QA pipeline:\nfrom haystack.pipelines import ExtractiveQAPipeline\n\npipe = ExtractiveQAPipeline(reader=reader, retriever=dpr_retriever)\nLet’s see this in action:\nn_answers = 3\npreds = pipe.run(query=query, params={\"Retriever\": {\"top_k\": 3}, \n                                      \"Reader\": {\"top_k\": n_answers}})\n\nprint(f\"Answer {idx+1}: {preds['answers'][idx].answer}\")\n\n\nfor idx in range(n_answers):\n    print(f\"Question {idx+1}: {preds['answers'][idx].answer}\")\n    print(f\"Answer snippet: ...{preds['answers'][idx].context}...\")\n    print(\"\\n\\n\")\nQuestion 3: I want to buy DDR3L HPE 647883-B21 16Gb DIMM ECC Reg PC3-10600 CL9This modules may not be supported.Kingston for Dell (317-6142 370-20147) DDR3 DIMM 16GB (PC3-10600) 1333MHz ECC Registered Low Voltage ModuleThis module can be used.\nAnswer snippet: ....I want to buy DDR3L HPE 647883-B21 16Gb DIMM ECC Reg PC3-10600 CL9This modules may not be supported.Kingston for Dell (317-6142 370-20147) DDR3 DIMM 16GB (PC3-10600) 1333MHz ECC Registered Low Voltage ModuleThis module can be used....\n\n\nFine-Tuning & Evaluation\nWhile our models are pretrained on diverse data, fine-tuning helps adapt them to our specific use case:\nfrom pathlib import Path\nimport torch\n\ndef train_and_evaluate():\n    # Initialize model, optimizer, loss function, etc.\n    train_filename = \"Dell_QA_200_train.json\"\n    dev_filename = \"Dell_QA_200_test.json\"\n    test_filename = \"Dell_QA_200_test.json\"  # or another test file\n    data_dir = Path(\".\")  # using pathlib's Path\n    save_dir = \"./model\"\n\n    # Fine-tuning your model\n    reader.train(data_dir=data_dir, use_gpu=True, n_epochs=1, batch_size=16,\n                 train_filename=train_filename, save_dir=save_dir)\n\n    # Evaluate the model\n    result = reader.eval_on_file(data_dir=data_dir, test_filename=test_filename, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(result)\n\n# Call the function to start the process\ntrain_and_evaluate()\nThe above snippet demonstrates how we fine-tune our reader on the Dell QA dataset, optimizing it for our domain.\n\n\nMetrics: The Scorecards of AI\nEvaluation is crucial in understanding how well our system performs. Some key metrics include:\n\nExact Match (EM): Measures the percentage of predictions that match any one of the ground truth answers exactly.\nF1 Score: Considers both the precision and the recall of the test to compute the score.\nTop_n_accuracy: Evaluates how often the correct answer is within the top n retrieved results.\n\nAfter fine-tuning with train_and_evaluate(), our evaluation showcased impressive results:\n{\n 'EM': 27.50, 'f1': 44.76, 'top_n_accuracy': 67.5, \n 'top_n': 4, 'EM_text_answer': 24.32, 'f1_text_answer': 42.98, \n ...\n}\nThese metrics offer a comprehensive view of our model’s performance, highlighting areas of excellence and potential improvement.\n\n\n\nConclusion\nSo that is my attempt at using Exctractive QA! With Deepset dropping it support for the online version of Haystack Annotation tool, I suspect many people are moving towards Generative QA system - I will be doing a blog post on this shortly!\nMy full code and datasets are here on my GitHub.\nHappy coding!"
  }
]