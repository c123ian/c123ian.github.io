<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Aunt-ificial Intelligence: Learning from Lifeâ€™s Dilemmas âœï¸ â€“ c123ian.github.io</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-ddd961a2510921635943dfbbd19534c4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">c123ian.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title"><strong>Aunt-ificial Intelligence: Learning from Lifeâ€™s Dilemmas</strong> âœï¸</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>The <a href="https://www.sciencedirect.com/science/article/abs/pii/S016503272400661X">therapeutic effectiveness</a> of artificial intelligence-based chatbots led me to look at uisng agony aunts to inform a LLM chatbot on their answers!</p>
<p>This article demonstrates how to create a production-grade LLM that uses RAG to retrieve relevant â€˜Agony Auntâ€™ articles to inform LLMâ€™s life/relationship advice. The aim is to show the relevant components so anyone can swap in their LLM, with or without RAG. Instead of using OpenAI API or Claude, this project allows swapping in/out any open source model (e.g., LLAMA 3.2). This project merges <a href="https://github.com/arihanv/fasthtml-modal">this template</a> and Modal Labs <a href="https://modal.com/docs/examples/vllm_inference">OpenAI-compatible LLM inference with LLaMA 3.1-8B and vLLM turtorial</a> which also links to full code <a href="https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/llm-serving/vllm_inference.py">here</a>.</p>
<p>You can download the full code <a href="https://github.com/c123ian/Agony_Aunt_RAG">here</a>ğŸ”— and the dataset used for RAG <a href="https://huggingface.co/datasets/c123ian/dear_deidre_agony_aunt">here</a>ğŸ“Š.</p>
<p>You can also interact with my own deployment <a href="https://c123ian--rag-chatbot-serve-fasthtml.modal.run">here</a>ğŸ’¬.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/vid_modal.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Side-by-side of GUI and Modal Lab logs</figcaption>
</figure>
</div>
<section id="the-stack" class="level2">
<h2 class="anchored" data-anchor-id="the-stack">The Stack</h2>
<section id="backend" class="level3">
<h3 class="anchored" data-anchor-id="backend">Backend</h3>
<p>For the backend, weâ€™re using <a href="https://modal.com/">Modal Labs</a> for hosting because it enables local development with easy cloud/GPU cluster deployment using Python decorators. Modal Labs scales effectively with user growth through additional GPU container instances and offers transparent pricing with a generous starter free tier providing $30/week of compute. When using smaller LLMs, we can utilize CPUs, which are considerably cheaper than GPUs.</p>
<p>The backend architecture incorporates an OpenAI-compatible serving API structure using industry-standard FastAPI interaction. We utilize SQLite, which comes built into Python, for fast, direct database access and conversation history storage tied to cookie session IDs. For the engine, weâ€™ve chosen vLLM, which leverages Page Attention for all LLAMA-compatible models, ensuring fast output.</p>
</section>
<section id="frontend" class="level3">
<h3 class="anchored" data-anchor-id="frontend">Frontend</h3>
<p>Our frontend implementation uses <a href="https://fastht.ml/">FastHTML</a> (with <a href="https://tailwindcss.com/">Tailwind CSS</a>) for hypermedia-driven app development, rather than JavaScript. While there are excellent <a href="https://www.youtube.com/watch?v=WuipZMUch18">YouTube resources</a> and <a href="https://hypermedia.systems/">books</a> available on this approach, donâ€™t worry if youâ€™re unfamiliar with this development style â€“ Iâ€™ll show you how to modify elements like buttons or colors.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/after.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>The GUI using FastHTML</figcaption>
</figure>
</div>
</section>
</section>
<section id="data-collection-and-context" class="level2">
<h2 class="anchored" data-anchor-id="data-collection-and-context">Data Collection and Context</h2>
<p>The role of an agony aunt is fundamentally about helping people clarify their options and choices. The act of writing to an agony aunt allows individuals to think through their choices and receive reassurance. <a href="https://inews.co.uk/inews-lifestyle/agony-aunt-column-why-still-endures-350-years-later-1905176">Agony aunts</a> provide a unique form of objectivity that we often canâ€™t get from close friends.</p>
<p>A crucial aspect of the agony aunt role is that they donâ€™t act as moral judges of othersâ€™ behavior. They understand that life rarely presents black-and-white situations, instead operating in shades of grey. Our data source for this project comes from <a href="https://www.thesun.co.uk/dear-deidre">here</a>.</p>
<p>The <a href="c123ian/dear_deidre_agony_aunt">Agony Aunt dataset</a> â€˜Questionâ€™ column is a letter written to the agony aunt and their response/advice is saved in â€˜Answerâ€™ column.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 37%">
<col style="width: 21%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>ID</th>
<th>Data Original Text</th>
<th>Data Headline</th>
<th>URL</th>
<th>Question</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>30792701</td>
<td>BIG BULLY</td>
<td>Iâ€™m sick of being bullied and belittled by myâ€¦</td>
<td>https://www.thesun.co.uk/dear-deidre/30792701/</td>
<td>MY dad was a bully and drove me to alcoholismâ€¦.</td>
<td>Your relationship sounds abusive. Find a quietâ€¦</td>
</tr>
<tr class="even">
<td>30764398</td>
<td>NOT APPY</td>
<td>Online dating is a mystery to me and no matteâ€¦</td>
<td>https://www.thesun.co.uk/dear-deidre/30764398/</td>
<td>Iâ€™VE tried so many dating apps but my profile â€¦</td>
<td>Online dating is very much focused on appearanâ€¦</td>
</tr>
<tr class="odd">
<td>30730867</td>
<td>SO HOPELESS</td>
<td>Iâ€™m 25 but my father wonâ€™t allow me any freedâ€¦</td>
<td>https://www.thesun.co.uk/dear-deidre/30730867/</td>
<td>MY father treats me worse than an animal. Iâ€™m â€¦</td>
<td>Family can be the perpetrators of domestic abuâ€¦</td>
</tr>
</tbody>
</table>
<p>Just to note on the dataset, I plan to get more diverse sources but currently its all from a tabloid called The Sun. Sometimes agony aunt writer refers to an e-leaflet for more detailed guidance on a topic, this is not included in the dataset. Also, this is a UK column and there was a <a href="https://www.bmj.com/content/327/Suppl_S1/030722">law going through parliament</a> resulting in agony aunts who give advice to teenagers being liable to prosecution, so this may affect agony aunts reponses to sex advice!</p>
</section>
<section id="prompt-testing" class="level2">
<h2 class="anchored" data-anchor-id="prompt-testing">Prompt Testing</h2>
<p>Our prompt development was grounded in the agony auntâ€™s characteristic neutral stance and the necessity to reference context. We used Anthropicâ€™s <a href="https://docs.anthropic.com/en/release-notes/system-prompts#oct-22nd-2024">Claude prompt</a> as a starting point. For those seeking additional prompt resources, the <a href="https://smith.langchain.com/hub">LangSmith Prompt Hub</a> offers various prompts for different models and use cases, including RAG implementations.</p>
<p>The testing process involved iterative adjustment, deployment, and output verification to ensure the responses maintained the appropriate tone and utility. But didnâ€™t go beyound vibe testing so defiinetly room for improvemet here.</p>
</section>
<section id="token-management" class="level2">
<h2 class="anchored" data-anchor-id="token-management">Token Management</h2>
<p>After signing up to Modal labs the setup process begins with API token configuration in your chosen virtual enviroment. Navigate to Settings &gt; API Token &gt; Generate, where youâ€™ll receive a key to use with a command like <code>modal token set --token-id ak-XXXx --token-secret as-YYYY</code>.Just FYI itâ€™s important to distinguish between API Tokens and Secrets. For secrets, you can either use the web interface at https://modal.com/secrets/c123ian/main/create?secret_nam, choosing custom &gt; input value and name, or use the command line: <code>modal secret create my-custom-secret-2 YOUR_KEY=333</code>. In your code, youâ€™ll reference this using <code>secret = modal.Secret.from_name("irish-chatbot-secret")</code>.</p>
<p>For Hugging Face integration, set up the token using:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">modal</span> secret create huggingface-token-3 HUGGINGFACE_TOKEN=<span class="st">"hf_FxxxxxxxxxxxxxxxxxxxxxXl"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In your code, implement it as follows:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>hf_token_secret <span class="op">=</span> modal.Secret.from_name(<span class="st">"huggingface-token"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="at">@app.function</span>(secrets<span class="op">=</span>[modal.Secret.from_name(<span class="st">"huggingface-token"</span>)])</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> some_function():</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    os.getenv(<span class="st">"HUGGINGFACE_TOKEN"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="selecting-llm-and-dataset" class="level2">
<h2 class="anchored" data-anchor-id="selecting-llm-and-dataset">Selecting LLM and dataset</h2>
<p>Make changes on the main app, but also reflect tthese changes on your <code>download_model.py</code> and <code>download_faiss_data.py</code></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>MODELS_DIR <span class="op">=</span> <span class="st">"/llama_mini"</span> <span class="co"># Name of Volume we created via script </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"Llama-3.2-3B-Instruct"</span> <span class="co"># Matches model name from HuggingFace, so we know where to find weight/tokenizer</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>FAISS_DATA_DIR <span class="op">=</span> <span class="st">"/faiss_data"</span> <span class="co"># Name of Volume we created via script (RAG)</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>EMBEDDING_MODEL_NAME <span class="op">=</span> <span class="st">"BAAI/bge-small-en-v1.5"</span> <span class="co"># Can use any, reduces the dimenional space (RAG)</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>USERNAME <span class="op">=</span> <span class="st">"c123ian"</span> <span class="co"># Modal labs usernaame</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>APP_NAME <span class="op">=</span> <span class="st">"rag-cha"</span><span class="co"># Can use any name here to appear on Modal Labs dashboard</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>DATABASE_DIR <span class="op">=</span> <span class="st">"/db_data"</span>  <span class="co"># Database directory</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>db_path <span class="op">=</span> os.path.join(DATABASE_DIR, <span class="st">'chat_history.db'</span>) <span class="co"># To store conversation history</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="scripts-volume-management" class="level2">
<h2 class="anchored" data-anchor-id="scripts-volume-management">Scripts &amp; Volume Management</h2>
<p>Once you have chosen your model and dataset to use, will run the two scripts that download the dataset onto a <a href="https://modal.com/docs/guide/volumes">Volume</a> for persistant storage and index the dataset uisng FAISS (though I havent tested it you could also try LlamaIndex).</p>
<ul>
<li>(optional) Downloading the database (if exists yet, otherwise its generated in the main app code later)(<code>modal volume get db_data /chat_history.db</code>), or see the <code>inspect_db.ipynb</code> notebook.</li>
<li><ol type="1">
<li>Run the pre-requist scripts for downloading the model (<code>modal run download_model.py</code>) and creating the FAISS index (<code>modal run download_faiss_data.py</code>). Recall my code points to <code>c123ian/dear_deidre_agony_aunt</code> Huggingface dataset to be downloaded and indexed.</li>
</ol></li>
<li>Deploying the app (<code>modal deploy app_name.py</code>) and click the displayed fasthtml URL, for example mine is `https://c123ianâ€“rag-chatbot-serve-fasthâ€Modal Demoâ€/&gt;</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/script.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Modal deploy</figcaption>
</figure>
</div>
<p>To inspect your Volumes after running the scripts, start with:</p>
<ul>
<li><code>modal volume list</code> to see all created volumes.</li>
</ul>
<pre><code>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Name                      â”ƒ Created at                         â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ llama_mini                â”‚ 2024-09-23 12:31 GMT Standard Time â”‚
â”‚ faiss_data                â”‚ 2024-10-04 13:18 GMT Standard Time â”‚
â”‚ db_data                   â”‚ 2024-10-31 09:56 GMT Standard Time â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>
<ul>
<li>Then you can then examine specific volumes with commands like <code>modal volume ls llama_mini</code>, which should show safetensors and config files. Safetensors represents a new format for storing tensors safely compared to pickle, while maintaining speed through zero-copy operations.</li>
</ul>
<pre><code>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Filename                         â”ƒ Type â”ƒ Created/Modified                   â”ƒ Size      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ .cache                           â”‚ dir  â”‚ 2024-09-23 12:31 GMT Standard Time â”‚ 11 B      â”‚
â”‚ tokenizer_config.json            â”‚ file â”‚ 2024-10-01 12:09 GMT Standard Time â”‚ 1.9 KiB   â”‚
â”‚ tokenizer.model                  â”‚ file â”‚ 2024-10-01 12:09 GMT Standard Time â”‚ 545.5 KiB â”‚
â”‚ tokenizer.json                   â”‚ file â”‚ 2024-10-01 12:09 GMT Standard Time â”‚ 2.0 MiB   â”‚
â”‚ special_tokens_map.json          â”‚ file â”‚ 2024-10-01 12:09 GMT Standard Time â”‚ 551 B     â”‚
â”‚ generation_config.json           â”‚ file â”‚ 2024-10-01 12:09 GMT Standard Time â”‚ 183 B     â”‚
â”‚ config.json                      â”‚ file â”‚ 2024-10-01 12:09 GMT Standard Time â”‚ 674 B     â”‚
â”‚ README.md                        â”‚ file â”‚ 2024-10-01 12:09 GMT Standard Time â”‚ 3.0 KiB   â”‚
â”‚ .gitattributes                   â”‚ file â”‚ 2024-10-01 12:09 GMT Standard Time â”‚ 1.5 KiB   â”‚
â”‚ model.safetensors                â”‚ file â”‚ 2024-10-01 12:06 GMT Standard Time â”‚ 4.1 GiB   â”‚
â”‚ TinyLlama_logo.png               â”‚ file â”‚ 2024-10-01 12:06 GMT Standard Time â”‚ 1.8 MiB   â”‚
â”‚ model.safetensors.index.json     â”‚ file â”‚ 2024-09-23 12:35 GMT Standard Time â”‚ 29.2 KiB  â”‚
â”‚ model-00011-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:35 GMT Standard Time â”‚ 2.8 GiB   â”‚
â”‚ model-00010-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:35 GMT Standard Time â”‚ 4.6 GiB   â”‚
â”‚ model-00009-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:34 GMT Standard Time â”‚ 4.6 GiB   â”‚
â”‚ model-00008-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:34 GMT Standard Time â”‚ 4.5 GiB   â”‚
â”‚ model-00007-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:34 GMT Standard Time â”‚ 4.5 GiB   â”‚
â”‚ model-00006-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:33 GMT Standard Time â”‚ 4.5 GiB   â”‚
â”‚ model-00005-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:33 GMT Standard Time â”‚ 4.6 GiB   â”‚
â”‚ model-00004-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:32 GMT Standard Time â”‚ 4.6 GiB   â”‚
â”‚ model-00003-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:32 GMT Standard Time â”‚ 4.6 GiB   â”‚
â”‚ model-00002-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:32 GMT Standard Time â”‚ 4.6 GiB   â”‚
â”‚ model-00001-of-00011.safetensors â”‚ file â”‚ 2024-09-23 12:31 GMT Standard Time â”‚ 4.6 GiB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>
</section>
<section id="main-application-structure" class="level2">
<h2 class="anchored" data-anchor-id="main-application-structure">Main Application Structure</h2>
<p>The application architecture consists of two primary components: the backend <code>serve_vllm()</code> and frontend <code>serve_fasthtml()</code>. The backend creates an OpenAI-compatible API endpoint for LLM inference using vLLM, handling model/tokenizer loading, initialization, completions endpoint with streaming support, and managing sampling parameters and response generation.</p>
<p>The frontend implements a web interface with WebSocket support for real-time chat, integrating RAG using FAISS for semantic search of advice columns. It manages chat sessions and history through SQLite, handles retrieval and incorporation of relevant context, implements UI with status indicators and source attribution, streams responses from the vLLM server to the web interface, and displays top sources with similarity scores.</p>
</section>
<section id="gpu-selection-and-pricing" class="level2">
<h2 class="anchored" data-anchor-id="gpu-selection-and-pricing">GPU Selection and Pricing</h2>
<p>Modal Labs currently offers several GPU options with varying <a href="https://modal.com/pricing">price points</a>:</p>
<p>At the time of writing the prices for GPUs on Modal Labs are:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Resource Type</th>
<th>Specification</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPU</td>
<td>Nvidia H100</td>
<td>4.56/h</td>
</tr>
<tr class="even">
<td>GPU</td>
<td>Nvidia A100, 80 GB</td>
<td>3.40/h</td>
</tr>
<tr class="odd">
<td>GPU</td>
<td>Nvidia A100, 40 GB</td>
<td>2.78/h</td>
</tr>
<tr class="even">
<td>GPU</td>
<td>Nvidia A10G</td>
<td>1.10/h</td>
</tr>
<tr class="odd">
<td>GPU</td>
<td>Nvidia L4</td>
<td>0.80/h</td>
</tr>
<tr class="even">
<td>GPU</td>
<td>Nvidia T4</td>
<td>0.59/h</td>
</tr>
<tr class="odd">
<td>CPU</td>
<td>Physical core (2 vCPU equivalent)</td>
<td>0.135/core/h</td>
</tr>
<tr class="even">
<td>Memory</td>
<td>RAM</td>
<td>0.024/GiB/h</td>
</tr>
</tbody>
</table>
<p>Note: Minimum of 0.125 cores per container</p>
<p>A typical configuration might look like this:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="at">@app.function</span>(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>image,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    gpu<span class="op">=</span>modal.gpu.A100(count<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="st">"40GB"</span>),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    container_idle_timeout<span class="op">=</span><span class="dv">10</span> <span class="op">*</span> <span class="dv">60</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    timeout<span class="op">=</span><span class="dv">24</span> <span class="op">*</span> <span class="dv">60</span> <span class="op">*</span> <span class="dv">60</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    allow_concurrent_inputs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    volumes<span class="op">=</span>{MODELS_DIR: volume},</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The configuration requires careful balance between cold boot wait time and GPU active time, considering both cost optimization and user experience. We implement a status indicator during model/GPU warmup to keep users informed of progress.</p>
</section>
<section id="api-integration" class="level2">
<h2 class="anchored" data-anchor-id="api-integration">API Integration</h2>
<p>Weâ€™ve adopted OpenAIâ€™s API standard, which is well-supported by open-source frameworks like vLLM. The implementation includes two key endpoints: <code>/completions</code> for single prompt completion (which we used) or <code>/chat/completions</code> for dialog responses with specific message history format. For those migrating from older implementations, a helpful guide is available at https://help.openai.com/en/articles/7042661-moving-from-completions-to-chat-completions-in-the-openai-api.</p>
</section>
<section id="rag-implementation" class="level2">
<h2 class="anchored" data-anchor-id="rag-implementation">RAG Implementation</h2>
<p>Our RAG implementation follows a straightforward process: it ranks similarity between user input and the â€˜Questionâ€™ column, selects top k similar questions, and passes the corresponding â€˜Answerâ€™ column content as context. The interface displays the top 2 sources in the GUI for transparency.</p>
<p>We use FAISS for dense retrieval, which retrieves documents based on vector representations and performs nearest neighbors search. Embeddings reduce the high-dimensional space of language (millions of possible words) into a dense vector space (typically hundreds or thousands of dimensions) while preserving semantic relationships. The system converts vectors to searchable code words and uses upside-down indexing for similar results, with support for reranking based on exact vector comparisons. Detailed documentation is available at https://huggingface.co/docs/datasets/v2.14.1/en/faiss_es#faiss.</p>
<p>The indexing process with FAISS involves indexing dense vectors for fast nearest neighbor searches, using vector quantization for efficient storage, and optimizing similarity computations. When considering model selection, traditional RAG often employs sequence-to-sequence models like BART or T5, which are specifically trained for input-to-output transformation and naturally fit the question-context-answer pattern. However, language models like GPT can be adapted for RAG with proper input formatting, despite being primarily trained for next-word prediction.</p>
</section>
<section id="future-rag-challenges-to-address" class="level2">
<h2 class="anchored" data-anchor-id="future-rag-challenges-to-address">Future RAG Challenges to Address</h2>
<p>Several challenges remain in our implementation. First, LLMs struggle with multiple retrieved documents, showing best accuracy when the answer is in the first document, decent accuracy when itâ€™s in the last document, but poor accuracy when itâ€™s in the middle. Additionally, LLMs may not effectively recognize irrelevant documents and tend to generate answers regardless of context quality.</p>
<p>To address these limitations, weâ€™re exploring information retrieval techniques including BM25 with boosting queries, recommender system ranking/filtering, and retrieval evaluation metrics. Additional improvements under consideration include two-stage retrieval, BM25 filtering, NDCG evaluation, relevance thresholds, and improved RAG training with irrelevant context.</p>
</section>
<section id="sqlite-databse-for-conversation-history-and-evaluation." class="level2">
<h2 class="anchored" data-anchor-id="sqlite-databse-for-conversation-history-and-evaluation.">SQLite Databse for Conversation History and Evaluation.</h2>
<p>Our database implementation focuses on storing conversation history for LLM context awareness, utilizing a <code>Conversation()</code> class in <a href="https://www.sqlalchemy.org/">SQLAlchemy</a> with a matching database schema and session management. Without passing the conversation history keeping it persistant, our model would forget after every user message (so not very helpful).</p>
<p>The conversation history implementation uses SQLite with SQLAlchemy ORM, structured to maintain persistent chat sessions while efficiently managing temporary request data. Hereâ€™s the database ouce created and used should look something like this:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 3%">
<col style="width: 25%">
<col style="width: 15%">
<col style="width: 25%">
<col style="width: 4%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th>Message ID</th>
<th>Session ID</th>
<th>Role</th>
<th>Content</th>
<th>Top Source Headline</th>
<th>Top Source URL</th>
<th>Cosine Sim Score</th>
<th>Created At</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1f498b5f-abe3-4237-b478-05757e94a373</td>
<td>25ee7bc3-227c-4db0-a8f4-a224fd7eeb83</td>
<td>user</td>
<td>well</td>
<td>None</td>
<td>None</td>
<td>NaN</td>
<td>2024-11-14 09:38:22.430858</td>
</tr>
<tr class="even">
<td>ede9d15f-4c26-4264-95b1-867bbd023f63</td>
<td>25ee7bc3-227c-4db0-a8f4-a224fd7eeb83</td>
<td>assistant</td>
<td>I sense some uncertainty about something going on without clear answers. Can any aspects come together?</td>
<td>Iâ€™m seeing a married man and his double life means my needs come last</td>
<td>https://www.thesun.co.uk/dear-deidre/21845342/married-man-double-life-needs-come-last/</td>
<td>0.388162</td>
<td>2024-11-14 09:38:23.177000</td>
</tr>
<tr class="odd">
<td>3e2172dd-eceb-4864-8d1e-91bf0e6b306f</td>
<td>25ee7bc3-227c-4db0-a8f4-a224fd7eeb83</td>
<td>user</td>
<td>ok</td>
<td>None</td>
<td>None</td>
<td>NaN</td>
<td>2024-11-14 09:38:27.179002</td>
</tr>
<tr class="even">
<td>18282b76-70eb-4934-b262-bcc810c6ccd1</td>
<td>25ee7bc3-227c-4db0-a8f4-a224fd7eeb83</td>
<td>assistant</td>
<td>Youâ€™ve shared there may have been issues within this particular situation but donâ€™t feel like sharing all details yet; Is anything bothering particularly enough so cannot focus at present?</td>
<td>My life has been torn apart by my son being accused of rape</td>
<td>https://www.thesun.co.uk/dear-deidre/29795950/son-accused-of-rape-scared/</td>
<td>0.370166</td>
<td>2024-11-14 09:38:27.908870</td>
</tr>
</tbody>
</table>
<p>The corresponding SQLAlchemy model:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Conversation(Base):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    __tablename__ <span class="op">=</span> <span class="st">'conversations_history_table_sqlalchemy_v2'</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    message_id <span class="op">=</span> Column(String, primary_key<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    session_id <span class="op">=</span> Column(String, nullable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    role <span class="op">=</span> Column(String, nullable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    content <span class="op">=</span> Column(String, nullable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    top_source_headline <span class="op">=</span> Column(String)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    top_source_url <span class="op">=</span> Column(String)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    cosine_sim_score <span class="op">=</span> Column(Float)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    created_at <span class="op">=</span> Column(DateTime, default<span class="op">=</span>datetime.datetime.utcnow)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Notice we match up the datatypes and column names to match the SQL table. We can inspect its contents via the <code>inspect_db.ipynb</code> notebook. Also, we also save the similarity score <code>Cosine Sim Score</code> used to gather the most relevant Answers using RAG. We could use this to evaluate how well the retrival of relavnt documents is going, perhaps also implement a cutoff so not ebvery article is passed to the LLM via the context parameter.</p>
<p>We generate a truley unique <code>Message ID</code> for every row (acts as primary key) while <code>Session ID</code> has a one-to-many relationship beacause it signifies the conversation (which can have multiple messages back and forth), changing only after a browser refresh (new conversation).</p>
<p>Just to be aware in the code there is another uuid <code>request_id</code>, this is used internally by vLLM.</p>
</section>
<section id="chat-template-or-lack-therof" class="level2">
<h2 class="anchored" data-anchor-id="chat-template-or-lack-therof">Chat Template, Or Lack Therof</h2>
<p><a href="https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#chat-template">Chat templates</a> are often a Jinja2 file that specifies how are roles, messages, and other chat-specific tokens are encoded in the input. vLLM community provides a set of chat templates for popular models.</p>
<p>But we are using vLLMâ€™s <code>OpenAIServingChat</code> with <code>chat_template=None</code>. When <code>chat_template</code> is not specified, the responsibility of formatting the prompt falls on your application code. For example were handling the prompt formatting ourselves through your <code>build_prompt()</code> function:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_prompt(system_prompt, context, conversation_history):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ss">f"""</span><span class="sc">{</span>system_prompt<span class="sc">}</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="ss">    Context Information:</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span><span class="sc">{</span>context<span class="sc">}</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="ss">    Conversation History:</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span><span class="sc">{</span>conversation_history<span class="sc">}</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="ss">    Assistant:"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <code>build_conversation</code> is responsible for manually formatting the conversation history.</p>
<p>Models are generally robust enough to handle variations in input formatting. The prompt structure (in our code ending with â€œAssistant:â€) provides enough context for the model to understand its role.</p>
</section>
<section id="model-integration-considerations" class="level2">
<h2 class="anchored" data-anchor-id="model-integration-considerations">Model Integration Considerations</h2>
<p>When integrating new models, pay special attention to max_position_embeddings, which determines fixed sequence length capacity. This often needs adjustment according to model specifications (typically 4096 or 8096). Additionally, verify Page Attention/vLLM compatibility by checking the supported models list, memory requirements, and performance implications for your specific use case.</p>
</section>
<section id="llm-output" class="level2">
<h2 class="anchored" data-anchor-id="llm-output">LLM Output</h2>
<p>There are a few approaches to take on generating LLM text output and how we display it to the user. I have another implementation <a href="https://github.com/c123ian/Irish_Tutor/blob/main/irish_llm_v2.py">Irish Tutor</a> which uses HTMX polling to constantly check from a complete message. For user experience this means the complete paragraph suddenly appears, while I rather have the text appear as its generated (the illusion of teh LLM writing). However one drawback is its much more difficult to ensure the output is clean, which is why we have the buffer.</p>
<p>The <code>chat_advan_v2_buffer.py</code> is the most recent version, incoporates a buffer system for cleaner LLM output streams. However, edge case still occurs where spacing is sometimes ommited. For example â€˜Whatâ€™s onyourmind?â€™ which should be â€˜Whatâ€™s on your mind?â€™.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>