{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an AI Podcast Generator: A Deep Technical Dive\n",
    "\n",
    "\n",
    "![Podcast Generator GUI](./images/pod.png)\n",
    "\n",
    "### 🚀 Try the Live app [HERE](https://c123ian--gen-podcast-serve.modal.run)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Creating podcasts is typically labor-intensive, requiring content preparation, recording sessions, and audio editing. But what if we could automate this process? This article explores the development of a podcast generator that transforms ordinary text into engaging audio conversations between two hosts. The system takes various inputs (PDFs, websites, text, even audio), generates a conversational script, and produces a natural-sounding podcast with minimal human intervention.\n",
    "\n",
    "This project builds upon the [Llama cookbook's NotebookLlama example](https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases/NotebookLlama), focusing on production-grade implementation with Modal Labs. The goal was to create a system that generates podcasts with natural speech patterns, consistent voices, and proper discussion flow.\n",
    "\n",
    "Please see the Github for samples of audio generated [HERE](https://github.com/c123ian/Podcast_Generator_App/blob/main/README.md) \n",
    "\n",
    "Let's dive into how it works, the challenges we faced, and how we solved them.\n",
    "\n",
    "## Architecture Overview: Why Three Separate Modules?\n",
    "\n",
    "The podcast generator follows a three-stage pipeline built on Modal Labs' cloud infrastructure:\n",
    "\n",
    "![Podcast Generator Diagram from NotebookLlama](./images/pod_diag.png)\n",
    "\n",
    "\n",
    "Each module handles a distinct part of the process:\n",
    "\n",
    "1. **Content Ingestion** (`input_gen.py`): Accepts various inputs, manages the web UI, and orchestrates the generation process\n",
    "2. **Script Generation** (`scripts_gen.py`): Transforms raw content into natural conversational scripts\n",
    "3. **Audio Generation** (`audio_gen.py`): Converts scripts to realistic audio using Bark TTS\n",
    "\n",
    "But why split the system into three modules instead of a single monolithic application? There are several compelling reasons:\n",
    "\n",
    "1. **Resource Optimization**: Each stage has different computational requirements. Input processing needs minimal resources, script generation benefits from smaller GPUs, while audio generation demands high-end GPUs. Separating these concerns allows us to allocate exactly what each stage needs.\n",
    "\n",
    "2. **Fault Tolerance**: If one stage fails, we don't need to restart the entire pipeline. For instance, if audio generation encounters an issue, we can retry just that stage without regenerating the script.\n",
    "\n",
    "3. **Development Flexibility**: Team members can work on different components simultaneously without conflicts. During development, we could iterate on the audio generation while the script generation remained stable.\n",
    "\n",
    "4. **Scalability**: We can scale each component independently based on demand. For instance, we might deploy multiple audio generation instances if that becomes the bottleneck.\n",
    "\n",
    "As Meta's NotebookLlama project demonstrated, this separation of concerns is crucial for building production-ready AI applications.\n",
    "\n",
    "Let's look at how we implemented this architecture using Modal Labs:\n",
    "\n",
    "```python\n",
    "import modal\n",
    "from input_gen import app as input_app\n",
    "from scripts_gen import app as script_app\n",
    "from audio_gen import app as audio_app\n",
    "\n",
    "app = modal.App(\"multi-file-podcast\")\n",
    "\n",
    "# Ensure all components share the same persistent storage\n",
    "shared_volume = modal.Volume.lookup(\"combined_volume\")\n",
    "\n",
    "# Compose the full application from individual modules\n",
    "app.include(input_app)\n",
    "app.include(script_app)\n",
    "app.include(audio_app)\n",
    "```\n",
    "\n",
    "This composition pattern allows us to maintain modular code while deploying a unified application. Modal Labs handles the container orchestration, network communication, and GPU allocation, letting us focus on application logic.\n",
    "\n",
    "## Building Blocks: Shared Resources and Infrastructure\n",
    "\n",
    "To maintain consistency across modules, we established common resources with a shared Docker image and persistent volume:\n",
    "\n",
    "```python\n",
    "import modal\n",
    "\n",
    "try:\n",
    "    shared_volume = modal.Volume.lookup(\"combined_volume\")\n",
    "except modal.exception.NotFoundError:\n",
    "    shared_volume = modal.Volume.persisted(\"combined_volume\")\n",
    "\n",
    "common_image = (\n",
    "    modal.Image.debian_slim(python_version=\"3.10\")\n",
    "    .apt_install(\"git\", \"ffmpeg\")  \n",
    "    .pip_install(\n",
    "        \"torch==2.5.1\",  # Specific version to avoid PyTorch security issues\n",
    "        \"PyPDF2\",\n",
    "        \"python-fasthtml==0.12.0\",\n",
    "        \"langchain\",\n",
    "        \"langchain-community\",\n",
    "        \"openai-whisper\",\n",
    "        \"beautifulsoup4\",\n",
    "        \"requests\",\n",
    "        \"pydub\",\n",
    "        \"nltk\",\n",
    "        \"tqdm\",\n",
    "        \"scipy\",\n",
    "        \"transformers==4.46.1\",\n",
    "        \"accelerate==1.2.1\",\n",
    "        \"sumy>=0.11.0\",  # For dedicated summarization\n",
    "        \"git+https://github.com/suno-ai/bark.git\"\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "Why pin specific versions like `torch==2.5.1`? We encountered an interesting challenge: PyTorch 2.6 introduced stricter security measures that prevented Bark from loading its models correctly. By pinning to 2.5.1, we avoided this issue while maintaining compatibility.\n",
    "\n",
    "The shared volume is crucial for inter-module communication, storing both intermediate results (like generated scripts) and final outputs (podcast audio files). It also maintains a database for tracking generation status across pipeline stages.\n",
    "\n",
    "## Content Ingestion: Flexible Input Processing\n",
    "\n",
    "The first challenge was handling diverse input formats. To solve this, we implemented a flexible class hierarchy with a common interface:\n",
    "\n",
    "```python\n",
    "class BaseIngestor:\n",
    "    \"\"\"Base class for all ingestors\"\"\"\n",
    "    def validate(self, source: str) -> bool:\n",
    "        pass\n",
    "    def extract_text(self, source: str, max_chars: int = MAX_CONTENT_CHARS) -> Optional[str]:\n",
    "        pass\n",
    "    \n",
    "    def truncate_with_warning(self, text: str, max_chars: int) -> str:\n",
    "        \"\"\"Intelligently truncate text with warning message\"\"\"\n",
    "        if len(text) <= max_chars:\n",
    "            return text\n",
    "            \n",
    "        truncated = text[:max_chars]\n",
    "        print(f\"⚠️ Content truncated from {len(text)} to {max_chars} characters\")\n",
    "        \n",
    "        # Add an explanatory note at the end\n",
    "        truncation_note = \"\\n\\n[Note: The original content was truncated due to length limitations.]\"\n",
    "        truncated = truncated[:max_chars - len(truncation_note)] + truncation_note\n",
    "        \n",
    "        return truncated\n",
    "```\n",
    "\n",
    "We then implemented specialized processors for each input type:\n",
    "\n",
    "```python\n",
    "class PDFIngestor(BaseIngestor):\n",
    "    \"\"\"PDF ingestion - core functionality\"\"\"\n",
    "    def validate(self, file_path: str) -> bool:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File not found at path: {file_path}\")\n",
    "            return False\n",
    "        if not file_path.lower().endswith('.pdf'):\n",
    "            print(\"Error: File is not a PDF\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def extract_text(self, file_path: str, max_chars: int = MAX_CONTENT_CHARS) -> Optional[str]:\n",
    "        if not self.validate(file_path):\n",
    "            return None\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                num_pages = len(pdf_reader.pages)\n",
    "                print(f\"Processing PDF with {num_pages} pages...\")\n",
    "                extracted_text = []\n",
    "                total_chars = 0\n",
    "                for page_num in range(num_pages):\n",
    "                    page_text = pdf_reader.pages[page_num].extract_text()\n",
    "                    if page_text:\n",
    "                        extracted_text.append(page_text)\n",
    "                        total_chars += len(page_text)\n",
    "                        print(f\"Processed page {page_num + 1}/{num_pages}, total chars: {total_chars}\")\n",
    "                        if total_chars > max_chars * 1.1:  # Read slightly more than needed\n",
    "                            print(f\"Reached character limit at page {page_num + 1}/{num_pages}\")\n",
    "                            break\n",
    "                full_text = \"\\n\".join(extracted_text)\n",
    "                return self.truncate_with_warning(full_text, max_chars)\n",
    "        except PyPDF2.PdfReadError:\n",
    "            print(\"Error: Invalid or corrupted PDF file\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {str(e)}\")\n",
    "            return None\n",
    "```\n",
    "\n",
    "We also implemented similar classes for websites (using LangChain's WebBaseLoader), audio files (using Whisper for transcription), and plain text files. \n",
    "\n",
    "To select the appropriate ingestor, we used the factory pattern:\n",
    "\n",
    "```python\n",
    "class IngestorFactory:\n",
    "    \"\"\"Factory to create appropriate ingestor based on input type\"\"\"\n",
    "    @staticmethod\n",
    "    def get_ingestor(input_type: str, **kwargs) -> Optional[BaseIngestor]:\n",
    "        input_type = input_type.lower()\n",
    "        if input_type == \"pdf\":\n",
    "            return PDFIngestor()\n",
    "        elif input_type == \"website\":\n",
    "            return WebsiteIngestor()\n",
    "        elif input_type == \"audio\":\n",
    "            return AudioIngestor(**kwargs)\n",
    "        elif input_type == \"text\":\n",
    "            return TextIngestor()\n",
    "        else:\n",
    "            print(f\"Unsupported input type: {input_type}\")\n",
    "            return None\n",
    "```\n",
    "\n",
    "This pattern provides several advantages:\n",
    "1. **Extensibility**: Adding new input types (like DOCX files) just requires implementing a new ingestor class\n",
    "2. **Encapsulation**: Each ingestor handles its own validation and processing logic\n",
    "3. **Separation of Concerns**: The core application doesn't need to understand how different formats are processed\n",
    "\n",
    "## The User Interface: Creating a Responsive Experience\n",
    "\n",
    "For the user interface, we chose FastHTML, a modern Python-based framework for building web applications. FastHTML combines the simplicity of HTML templates with the power of Python, making it perfect for rapid development.\n",
    "\n",
    "The UI includes:\n",
    "1. A file upload interface with support for various formats\n",
    "2. A progress tracking system with real-time updates\n",
    "3. An audio player for the finished podcast\n",
    "\n",
    "Here's how we implemented the home page:\n",
    "\n",
    "```python\n",
    "@rt(\"/\")\n",
    "def homepage():\n",
    "    \"\"\"Render upload form with status checker\"\"\"\n",
    "    # DaisyUI styled file input\n",
    "    upload_input = Input(\n",
    "        type=\"file\",\n",
    "        name=\"content\",\n",
    "        accept=\".pdf,.txt,.md,.mp3,.wav,.m4a,.flac\",\n",
    "        required=False,\n",
    "        cls=\"file-input file-input-secondary w-full\"\n",
    "    )\n",
    "    \n",
    "    # DaisyUI styled URL input with prefix label\n",
    "    url_input_container = Div(\n",
    "        Span(cls=\"bg-base-300 px-3 py-2 rounded-l-lg\"),\n",
    "        Input(\n",
    "            type=\"text\",\n",
    "            name=\"url\",\n",
    "            placeholder=\"https://\",\n",
    "            cls=\"grow px-3 py-2 bg-base-300 rounded-r-lg focus:outline-none\"\n",
    "        ),\n",
    "        cls=\"flex items-center w-full\"\n",
    "    )\n",
    "    \n",
    "    # Side-by-side layout with divider\n",
    "    side_by_side = Div(\n",
    "        # Left card - File upload\n",
    "        Div(\n",
    "            Div(\n",
    "                upload_input,\n",
    "                cls=\"grid place-items-center p-4\"\n",
    "            ),\n",
    "            cls=\"card bg-base-300 rounded-box grow\"\n",
    "        ),\n",
    "        # Divider\n",
    "        Div(\"OR\", cls=\"divider divider-horizontal\"),\n",
    "        # Right card - URL input\n",
    "        Div(\n",
    "            Div(\n",
    "                url_input_container,\n",
    "                cls=\"grid place-items-center p-4\"\n",
    "            ),\n",
    "            cls=\"card bg-base-300 rounded-box grow\"\n",
    "        ),\n",
    "        cls=\"flex w-full\"\n",
    "    )\n",
    "    \n",
    "    # Add loading spinner with JavaScript\n",
    "    loading_script = Script(\"\"\"\n",
    "    document.addEventListener('htmx:beforeRequest', function(evt) {\n",
    "        if (evt.target.matches('form')) {\n",
    "            // Find the submit button\n",
    "            var btn = evt.target.querySelector('button[type=\"submit\"]');\n",
    "            if (btn) {\n",
    "                // Save the original text\n",
    "                btn.dataset.originalText = btn.textContent;\n",
    "                // Replace with loading spinner\n",
    "                btn.innerHTML = '<span class=\"loading loading-spinner loading-lg text-secondary\"></span>';\n",
    "                btn.disabled = true;\n",
    "            }\n",
    "        }\n",
    "    });\n",
    "    \n",
    "    document.addEventListener('htmx:afterRequest', function(evt) {\n",
    "        if (evt.target.matches('form')) {\n",
    "            // Find the submit button\n",
    "            var btn = evt.target.querySelector('button[type=\"submit\"]');\n",
    "            if (btn && btn.dataset.originalText) {\n",
    "                // Restore original text\n",
    "                btn.innerHTML = btn.dataset.originalText;\n",
    "                btn.disabled = false;\n",
    "            }\n",
    "        }\n",
    "    });\n",
    "    \"\"\")\n",
    "\n",
    "    upload_form = Form(\n",
    "        side_by_side,\n",
    "        content_info,\n",
    "        process_button,\n",
    "        loading_script,\n",
    "        action=\"/inject\",\n",
    "        method=\"post\",\n",
    "        enctype=\"multipart/form-data\",\n",
    "        id=\"upload-form\",\n",
    "        hx_boost=\"true\",  # Use HTMX to enhance the form\n",
    "        hx_indicator=\"#process-button\",  # Show loading state on this element\n",
    "        cls=\"mb-6\"\n",
    "    )\n",
    "    \n",
    "    # Status checker form and other UI components...\n",
    "    # ...\n",
    "```\n",
    "\n",
    "Why use HTMX and JavaScript for a loading spinner instead of pure FastHTML? We found that direct DOM manipulation provided the most reliable user experience for dynamic elements like loading indicators. This approach ensures users receive immediate feedback when submitting large files, which can take time to upload and process.\n",
    "\n",
    "## Script Generation: The Heart of the System\n",
    "\n",
    "The script generation module converts raw content into engaging podcast-style conversations. This is perhaps the most critical part of the system, as it determines the quality and naturalness of the resulting podcast.\n",
    "\n",
    "### Why Two-Step Generation?\n",
    "\n",
    "Rather than generating the final script in one go, we implemented a two-step process:\n",
    "\n",
    "```python\n",
    "# Step 2: Generate initial script with first prompt\n",
    "print(\"Generating initial script...\")\n",
    "prompt_1 = SYSTEM_PROMPT + \"\\n\\n\" + source_text\n",
    "first_draft = generation_pipe(prompt_1)[0][\"generated_text\"]\n",
    "\n",
    "# Step 3: Rewrite with disfluencies using second prompt\n",
    "print(\"Adding natural speech patterns...\")\n",
    "prompt_2 = REWRITE_PROMPT + \"\\n\\n\" + first_draft\n",
    "final_text = generation_pipe(prompt_2)[0][\"generated_text\"]\n",
    "```\n",
    "\n",
    "This approach stemmed from experimentation with the NotebookLlama project, which showed that separating content generation from speech pattern enhancement produced more natural results. The first step focuses on organizing information into a coherent conversation, while the second step adds the natural speech patterns that make it sound like a real podcast.\n",
    "\n",
    "### Crafting the Perfect Prompts\n",
    "\n",
    "The quality of the generated script heavily depends on the prompts we provide. Here's the system prompt for the initial generation:\n",
    "\n",
    "```python\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a world-class podcast writer, having ghostwritten for top shows like Joe Rogan, Lex Fridman, and Tim Ferris.\n",
    "Your job is to write a lively, engaging script with two speakers based on the text I provide.\n",
    "Speaker 1 leads the conversation, teaching Speaker 2, giving anecdotes and analogies.\n",
    "Speaker 2 asks follow-up questions, gets excited or confused, and interrupts with umm, hmm occasionally.\n",
    "\n",
    "IMPORTANT LENGTH CONSTRAINTS:\n",
    "- Create a podcast script with EXACTLY 12-15 exchanges between speakers.\n",
    "- The entire podcast should be about 5-7 minutes when read aloud.\n",
    "- Keep the conversation focused and concise while maintaining engagement.\n",
    "- If the source text is very long, focus on the most important and interesting aspects.\n",
    "\n",
    "ALWAYS START YOUR RESPONSE WITH 'SPEAKER 1' and a colon.\n",
    "PLEASE DO NOT GIVE OR MENTION THE SPEAKERS BY NAME.\n",
    "Keep the conversation extremely engaging, welcome the audience with a fun overview, etc.\n",
    "Only create ONE EPISODE of the podcast. \n",
    "The speakers discussing the topic as external commentators.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "And here's the rewrite prompt that transforms it into natural speech:\n",
    "\n",
    "```python\n",
    "REWRITE_PROMPT = \"\"\"\n",
    "You are an Oscar-winning screenwriter rewriting a transcript for an AI Text-To-Speech Pipeline.\n",
    "\n",
    "Make it as engaging as possible, Speaker 1 and 2 will be using different voices.\n",
    "\n",
    "IMPORTANT LENGTH CONSTRAINTS:\n",
    "- The final script should be EXACTLY 12-15 exchanges between speakers.\n",
    "- The entire podcast should be about 5-7 minutes when read aloud.\n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "Please re-write to make it as characteristic as possible\n",
    "\n",
    "For both Speakers use the following disfluencies FREQUENTLY AS MUCH AS POSSIBLE, umm, hmm, [laughs], [sighs], [laughter], [gasps], [clears throat], — for hesitations, CAPITALIZATION for emphasis. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\n",
    "\n",
    "Do not use [excitedly], [trailing off)], [interrupting], [pauses] or anything else that is NOT an outlined above disfluency for expression.\n",
    "\n",
    "Return your final answer as a Python LIST of (Speaker, text) TUPLES ONLY, NO EXPLANATIONS, e.g.\n",
    "\n",
    "Dont add \"quotation marks\" within the script dialogue. \n",
    "\n",
    "IT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\n",
    "\n",
    "[\n",
    "    (\"Speaker 1\", \"Hello, and welcome...\"),\n",
    "    (\"Speaker 2\", \"Hmm, that is fascinating!\")\n",
    "]\n",
    "\n",
    "IMPORTANT Your response must be a valid Python list of tuples. STRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Why request a specific output format (list of tuples)? This structured format makes it easy to process in subsequent stages. It also allows us to associate each line of dialogue with the correct speaker, which is crucial for voice assignment in the audio generation phase.\n",
    "\n",
    "The prompt engineering was refined through extensive experimentation. We found that:\n",
    "1. Being specific about speech patterns yielded more natural results\n",
    "2. Explicitly requesting the list of tuples format made parsing more reliable\n",
    "3. Limiting the script length prevented excessive generation time\n",
    "\n",
    "### The Quote Normalization Problem\n",
    "\n",
    "One unexpected challenge we encountered was the inconsistent handling of quotation marks in the generated scripts. This seemingly minor issue caused significant problems in the audio generation, with Bark often inserting long pauses when encountering inconsistent quote formats.\n",
    "\n",
    "To address this, we implemented a thorough normalization function:\n",
    "\n",
    "```python\n",
    "def normalize_script_quotes(script):\n",
    "    \"\"\"Normalize quotes in the script while preserving contractions.\"\"\"\n",
    "    normalized_script = []\n",
    "    for speaker, text in script:\n",
    "        # Process text for each speaker-text pair\n",
    "        \n",
    "        # Temporarily replace contractions with a placeholder to protect them\n",
    "        text = re.sub(r'(\\w)\\'(\\w)', r'\\1APOSTROPHE\\2', text)\n",
    "        \n",
    "        # Convert all remaining quotes to one style\n",
    "        text = text.replace('\"', '\"').replace(\"'\", '\"')\n",
    "        \n",
    "        # Restore contractions\n",
    "        text = text.replace('APOSTROPHE', \"'\")\n",
    "        \n",
    "        # Apply disfluencies conversion\n",
    "        text = convert_disfluencies(text)\n",
    "        normalized_script.append((speaker, text))\n",
    "    return normalized_script\n",
    "```\n",
    "\n",
    "This function handles the delicate task of standardizing quotation marks while preserving apostrophes in contractions (like \"don't\" and \"can't\"). This subtle but crucial processing step dramatically improved audio quality by eliminating unexpected pauses.\n",
    "\n",
    "### Converting Disfluencies for TTS\n",
    "\n",
    "Another critical aspect was properly formatting speech disfluencies (like \"hmm,\" \"umm,\" and \"[laughs]\") for the TTS system:\n",
    "\n",
    "```python\n",
    "def convert_disfluencies(text):\n",
    "    \"\"\"\n",
    "    Convert parenthesized expressions like (laughs) to bracketed [laughs]\n",
    "    for proper TTS rendering.\n",
    "    \"\"\"\n",
    "    # List of common disfluencies to check for\n",
    "    disfluencies = [\n",
    "        \"laughs\", \"sighs\", \"laughter\", \"gasps\", \"clears throat\", \n",
    "        \"sigh\", \"laugh\", \"gasp\", \"chuckles\", \"snorts\",\n",
    "        \"hmm\", \"umm\", \"uh\", \"ah\", \"er\", \"um\"\n",
    "    ]\n",
    "    \n",
    "    # Convert (laughs) to [laughs]\n",
    "    for disfluency in disfluencies:\n",
    "        # Look for various formats and convert them\n",
    "        text = re.sub(r'\\((' + disfluency + r')\\)', r'[\\1]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'<(' + disfluency + r')>', r'[\\1]', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Also match when there's text inside\n",
    "        text = re.sub(r'\\(([^)]*' + disfluency + r'[^)]*)\\)', r'[\\1]', text, flags=re.IGNORECASE)\n",
    "        \n",
    "    return text\n",
    "```\n",
    "\n",
    "This function converts various formats like \"(laughs)\" or \"<sighs>\" to the bracketed format \"[laughs]\" that Bark understands best. This standardization was essential for ensuring that the TTS system properly voiced these expressions instead of trying to read them as literal text.\n",
    "\n",
    "## Audio Generation: Bringing Scripts to Life\n",
    "\n",
    "The final and most computationally intensive part of the pipeline is transforming the text script into audio. This presented several significant challenges:\n",
    "\n",
    "1. **Bark's 13-Second Limitation**: Bark TTS can only generate about 13 seconds of audio per invocation\n",
    "2. **Maintaining Voice Consistency**: Ensuring each speaker maintains the same voice throughout\n",
    "3. **Computational Efficiency**: Optimizing for faster generation without sacrificing quality\n",
    "4. **Distributed Processing**: Coordinating GPU resources across multiple containers\n",
    "\n",
    "### Overcoming Bark's 13-Second Limitation\n",
    "\n",
    "Inspired by Bark's [long-form generation notebook](https://github.com/suno-ai/bark/blob/main/notebooks/long_form_generation.ipynb), we implemented a sophisticated approach that splits text into sentences and processes them individually:\n",
    "\n",
    "```python\n",
    "def generate_speaker_audio(speaker_lines: List[Tuple[int, str]], \n",
    "                          speaker: str, \n",
    "                          injection_id: str = None) -> List[Tuple[int, np.ndarray, int]]:\n",
    "    \"\"\"Generate audio for all lines from a single speaker with progress updates\"\"\"\n",
    "    preload_models()  # Ensure Bark model is loaded\n",
    "\n",
    "    # Setup voice consistency with RNG seed\n",
    "    voice_state_key = f\"{injection_id}_{speaker}\" if injection_id else None\n",
    "    \n",
    "    if voice_state_key and voice_states.contains(voice_state_key):\n",
    "        seed = voice_states.get(voice_state_key)\n",
    "        print(f\"Using saved seed {seed} for {speaker}\")\n",
    "    else:\n",
    "        speaker_num = 1 if speaker == \"Speaker 1\" else 2\n",
    "        seed = np.random.randint(10000 * speaker_num, 10000 * (speaker_num + 1) - 1)\n",
    "        if voice_state_key:\n",
    "            voice_states[voice_state_key] = seed\n",
    "        print(f\"Created new seed {seed} for {speaker}\")\n",
    "    \n",
    "    # Set seed for consistent voice\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Process each line for this speaker\n",
    "    results = []\n",
    "    for i, (line_idx, text) in enumerate(tqdm(speaker_lines, desc=f\"Generating {speaker} audio\")):\n",
    "        # Process disfluencies in the text\n",
    "        text = convert_disfluencies(text)\n",
    "        sentences = sentence_splitter(preprocess_text(text))\n",
    "        all_audio = []\n",
    "        chunk_silence = np.zeros(int(0.1 * SAMPLE_RATE), dtype=np.float32)\n",
    "        \n",
    "        for i, sent in enumerate(sentences):\n",
    "            semantic_tokens = generate_text_semantic(\n",
    "                sent,\n",
    "                history_prompt=voice_preset,\n",
    "                temp=text_temp,\n",
    "                min_eos_p=0.05,\n",
    "            )\n",
    "            \n",
    "            audio_array = semantic_to_waveform(\n",
    "                semantic_tokens, \n",
    "                history_prompt=voice_preset,\n",
    "                temp=waveform_temp,\n",
    "            )\n",
    "            \n",
    "            all_audio.append(audio_array)\n",
    "            if i < len(sentences) - 1:  # Don't add silence after the last sentence\n",
    "                all_audio.append(chunk_silence)\n",
    "        \n",
    "        if not all_audio:\n",
    "            line_audio = np.zeros(24000, dtype=np.float32)\n",
    "        else:\n",
    "            line_audio = np.concatenate(all_audio, axis=0)\n",
    "        \n",
    "        # Store with original index for reassembly\n",
    "        results.append((line_idx, line_audio, SAMPLE_RATE))\n",
    "        \n",
    "    return results\n",
    "```\n",
    "\n",
    "Instead of using Bark's high-level `generate_audio()` function, we leveraged the lower-level `generate_text_semantic()` and `semantic_to_waveform()` functions. This gave us finer control over the generation process, particularly for managing voice consistency and preventing hallucinations at the end of sentences.\n",
    "\n",
    "We also carefully tuned parameters like `temp` (temperature) and `min_eos_p` (minimum end-of-sentence probability) based on experimentation:\n",
    "\n",
    "```python\n",
    "# Generation parameters\n",
    "text_temp = 0.6        # Lower value = more predictable text\n",
    "waveform_temp = 0.6    # Lower value = more consistent audio\n",
    "```\n",
    "\n",
    "### Speaker-Based Parallelization\n",
    "\n",
    "A key innovation was our parallel processing strategy, which assigns each speaker to a separate GPU:\n",
    "\n",
    "```python\n",
    "# Split by speaker, keeping original indices for later reassembly\n",
    "speaker1_lines = [(i, text) for i, (speaker, text) in enumerate(lines) if speaker == \"Speaker 1\"]\n",
    "speaker2_lines = [(i, text) for i, (speaker, text) in enumerate(lines) if speaker == \"Speaker 2\"]\n",
    "\n",
    "# Process each speaker's lines in parallel\n",
    "speaker1_results = []\n",
    "speaker2_results = []\n",
    "\n",
    "# Only process if there are lines for that speaker\n",
    "if speaker1_lines:\n",
    "    print(f\"  Sending {len(speaker1_lines)} Speaker 1 lines to GPU #1\")\n",
    "    update_injection_status(injection_id, \"processing\", f\"Generating voice for Speaker 1 ({len(speaker1_lines)} lines)...\")\n",
    "    speaker1_results = generate_speaker_audio.remote(speaker1_lines, \"Speaker 1\", injection_id)\n",
    "\n",
    "if speaker2_lines:\n",
    "    print(f\"  Sending {len(speaker2_lines)} Speaker 2 lines to GPU #2\")\n",
    "    update_injection_status(injection_id, \"processing\", f\"Generating voice for Speaker 2 ({len(speaker2_lines)} lines)...\")\n",
    "    speaker2_results = generate_speaker_audio.remote(speaker2_lines, \"Speaker 2\", injection_id)\n",
    "\n",
    "# --- Combine results in original script order ---\n",
    "all_results = speaker1_results + speaker2_results\n",
    "all_results.sort(key=lambda x: x[0])  # Sort by original script line index\n",
    "```\n",
    "\n",
    "This approach offers several advantages:\n",
    "1. **Efficiency**: Nearly doubles processing speed by using multiple GPUs concurrently\n",
    "2. **Voice Consistency**: Each speaker's lines are processed together, maintaining voice characteristics\n",
    "3. **Resource Optimization**: Modal's `.remote()` function automatically assigns each task to an available GPU\n",
    "\n",
    "The `sort` step ensures that lines appear in the correct order in the final podcast, regardless of generation order.\n",
    "\n",
    "### Voice Consistency Across Containers\n",
    "\n",
    "A particularly challenging aspect was maintaining consistent voices when processing lines in parallel across distributed containers. We solved this using Modal's distributed dictionary:\n",
    "\n",
    "```python\n",
    "voice_states = modal.Dict.from_name(\"voice-states\", create_if_missing=True)\n",
    "\n",
    "# For each speaker, create a deterministic RNG seed\n",
    "voice_state_key = f\"{injection_id}_{speaker}\"\n",
    "if voice_states.contains(voice_state_key):\n",
    "    seed = voice_states.get(voice_state_key)\n",
    "else:\n",
    "    # First-time generation creates a new seed (different per speaker)\n",
    "    speaker_num = 1 if speaker == \"Speaker 1\" else 2\n",
    "    seed = np.random.randint(10000 * speaker_num, 10000 * (speaker_num + 1) - 1)\n",
    "    voice_states[voice_state_key] = seed\n",
    "```\n",
    "\n",
    "This approach ensures that each speaker uses the same random seed across all containers, resulting in consistent voice characteristics throughout the podcast. \n",
    "\n",
    "Why use RNG seeds instead of voice samples? Bark doesn't provide a direct way to \"continue\" from a previously generated voice. Instead, by controlling the randomness with fixed seeds, we can achieve similar voice characteristics across segments.\n",
    "\n",
    "## Status Tracking and User Experience\n",
    "\n",
    "To provide a good user experience, we implemented robust status tracking with multiple fallbacks:\n",
    "\n",
    "```python\n",
    "def update_injection_status(injection_id, status, notes=None, max_retries=5):\n",
    "    \"\"\"Update database status with retry logic and file-based fallback\"\"\"\n",
    "    if not injection_id:\n",
    "        return\n",
    "    \n",
    "    # Always save to file-based status system for reliability\n",
    "    audio_path = get_audio_file_path(injection_id)\n",
    "    save_status_file(injection_id, status, notes, audio_path)\n",
    "        \n",
    "    # Then try database with retries\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            import sqlite3\n",
    "            DB_PATH = \"/data/injections_truncate.db\"\n",
    "            conn = sqlite3.connect(DB_PATH, timeout=10.0)  # Longer timeout\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Enable WAL mode for better concurrency\n",
    "            cursor.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "            cursor.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "            \n",
    "            # Update status and/or notes\n",
    "            if notes:\n",
    "                cursor.execute(\n",
    "                    \"UPDATE injections SET status = ?, processing_notes = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?\",\n",
    "                    (status, notes, injection_id)\n",
    "                )\n",
    "            else:\n",
    "                cursor.execute(\n",
    "                    \"UPDATE injections SET status = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?\",\n",
    "                    (status, injection_id)\n",
    "                )\n",
    "                \n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            print(f\"✅ Updated status to '{status}' for ID: {injection_id}\")\n",
    "            \n",
    "            if notes:\n",
    "                print(f\"📝 Notes: {notes}\")\n",
    "                \n",
    "            return\n",
    "            \n",
    "        except sqlite3.OperationalError as e:\n",
    "            # Handle database lock errors\n",
    "            if \"database is locked\" in str(e) and attempt < max_retries - 1:\n",
    "                wait_time = 0.1 * (2 ** attempt) + random.random() * 0.1  # Exponential backoff\n",
    "                print(f\"⚠️ Database locked, retrying in {wait_time:.2f} seconds (attempt {attempt+1}/{max_retries})...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"⚠️ Error updating injection status after {attempt+1} attempts: {e}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error updating injection status: {e}\")\n",
    "            break\n",
    "```\n",
    "\n",
    "Why implement both database and file-based status tracking? Early in development, we encountered database locking issues when multiple containers tried to update status simultaneously. The file-based fallback ensures that status information is always available, even if the database temporarily fails.\n",
    "\n",
    "We also implemented exponential backoff for retries, which is a best practice for distributed systems where resource contention can occur.\n",
    "\n",
    "For client-side status updates, we used HTMX polling with a smart backoff strategy:\n",
    "\n",
    "```javascript\n",
    "// Function to check status\n",
    "function checkStatus() {\n",
    "    console.log(\"Checking podcast status...\");\n",
    "    \n",
    "    fetch('/podcast-status-api/{injection_id}')\n",
    "        .then(response => response.json())\n",
    "        .then(data => {\n",
    "            console.log('Status update:', data);\n",
    "            failedPollCount = 0; // Reset failed poll counter\n",
    "            \n",
    "            // Update status display\n",
    "            const statusText = document.getElementById('status-text');\n",
    "            if (statusText) statusText.innerText = 'Status: ' + data.status;\n",
    "            \n",
    "            const statusNotes = document.getElementById('status-notes');\n",
    "            if (statusNotes) statusNotes.innerText = data.notes || 'Processing...';\n",
    "            \n",
    "            // Calculate progress percentage\n",
    "            let createdAt = data.timestamp;\n",
    "            if (typeof createdAt === 'string') {\n",
    "                createdAt = new Date(createdAt).getTime();\n",
    "            } else {\n",
    "                createdAt = new Date().getTime() - 60000; // Fallback: 1 minute ago\n",
    "            }\n",
    "            \n",
    "            const elapsed = (Date.now() - createdAt) / 1000; // seconds\n",
    "            const totalEstimated = 7 * 60; // 7 minutes in seconds\n",
    "            const progressPct = Math.min(95, Math.floor((elapsed / totalEstimated) * 100));\n",
    "            \n",
    "            // Update progress bar\n",
    "            const progressBar = document.getElementById('progress-bar');\n",
    "            if (progressBar) progressBar.style.width = progressPct + '%';\n",
    "            \n",
    "            // Update time estimate\n",
    "            const elapsedMin = Math.floor(elapsed / 60);\n",
    "            const elapsedSec = Math.floor(elapsed % 60);\n",
    "            const remainingMin = Math.max(0, Math.floor((totalEstimated - elapsed) / 60));\n",
    "            const remainingSec = Math.max(0, Math.floor((totalEstimated - elapsed) % 60));\n",
    "            \n",
    "            const timeInfo = document.getElementById('time-estimate');\n",
    "            if (timeInfo) {\n",
    "                timeInfo.innerText = `Elapsed: ${elapsedMin}m ${elapsedSec}s | Est. remaining: ${remainingMin}m ${remainingSec}s`;\n",
    "            }\n",
    "            \n",
    "            // If podcast is complete and audio exists, reload the page and stop polling\n",
    "            if (data.is_completed && data.audio_exists) {\n",
    "                console.log('Podcast is ready! Stopping polls and reloading page...');\n",
    "                \n",
    "                // Show completion message\n",
    "                const statusIcon = document.getElementById('status-icon');\n",
    "                if (statusIcon) statusIcon.innerHTML = '<span class=\"text-4xl\">✓</span>';\n",
    "                \n",
    "                if (statusNotes) statusNotes.innerText = 'Podcast is ready! Loading player...';\n",
    "                \n",
    "                // Very important: Clear the timeout to stop polling\n",
    "                if (pollTimeoutId) {\n",
    "                    console.log('Clearing timeout ID:', pollTimeoutId);\n",
    "                    clearTimeout(pollTimeoutId);\n",
    "                    pollTimeoutId = null;\n",
    "                }\n",
    "                \n",
    "                // Brief delay then reload\n",
    "                setTimeout(() => window.location.reload(), 1000);\n",
    "                return; // Exit function immediately\n",
    "            }\n",
    "            \n",
    "            // Only continue polling if not completed\n",
    "            if (!data.is_completed) {\n",
    "                console.log('Podcast not complete, continuing to poll...');\n",
    "                pollTimeoutId = setTimeout(checkStatus, poll_interval);\n",
    "            } else {\n",
    "                // If completed but no audio yet, poll at reduced frequency\n",
    "                console.log('Podcast completed but audio not ready, polling less frequently...');\n",
    "                pollTimeoutId = setTimeout(checkStatus, poll_interval * 2);\n",
    "            }\n",
    "        })\n",
    "        .catch(error => {\n",
    "            console.error('Error checking status:', error);\n",
    "            failedPollCount++;\n",
    "            console.log(`Poll attempt failed (${failedPollCount}/${MAX_FAILED_POLLS})`);\n",
    "            \n",
    "            // If too many failures, show error message\n",
    "            if (failedPollCount >= MAX_FAILED_POLLS) {\n",
    "                const statusNotes = document.getElementById('status-notes');\n",
    "                if (statusNotes) statusNotes.innerText = 'Connection issues. Please refresh the page.';\n",
    "                if (pollTimeoutId) {\n",
    "                    clearTimeout(pollTimeoutId);\n",
    "                    pollTimeoutId = null;\n",
    "                }\n",
    "                return;\n",
    "            }\n",
    "            \n",
    "            // Continue polling even on error, with longer interval\n",
    "            pollTimeoutId = setTimeout(checkStatus, poll_interval * 2);\n",
    "        });\n",
    "}\n",
    "```\n",
    "\n",
    "This polling system adjusts its frequency based on elapsed time and connection reliability. For new podcasts, it polls frequently to provide responsive updates. For longer-running generations, it reduces polling frequency to minimize server load.\n",
    "\n",
    "## Lessons Learned and Future Improvements\n",
    "\n",
    "Building this system taught us several valuable lessons:\n",
    "\n",
    "1. **Prompt Engineering is Crucial**: The quality of the generated podcast heavily depends on the prompts. Spending time refining them yields significant improvements.\n",
    "\n",
    "2. **Text Normalization Matters**: Small inconsistencies in text formatting can cause major issues in audio generation. Thorough normalization is essential.\n",
    "\n",
    "3. **Distributed State is Challenging**: Maintaining consistent state across distributed containers requires careful design and appropriate tools.\n",
    "\n",
    "4. **Handling Failures Gracefully**: Multiple fallback mechanisms are essential for production systems, especially when dealing with AI models that can sometimes produce unexpected results.\n",
    "\n",
    "5. **User Experience Requires Attention**: Even in AI systems, clear feedback and progress indicators make a huge difference to users.\n",
    "\n",
    "Based on our experience, several areas for future improvement are clear:\n",
    "\n",
    "1. **Voice Consistency**: While our RNG seed approach works well, occasional voice changes still occur. Exploring newer TTS models that better support voice continuity would be beneficial.\n",
    "\n",
    "2. **Speech Speed and Silence**: Fine-tuning silence duration could improve the natural flow of conversation. Current settings lean toward slower, more deliberate speech.\n",
    "\n",
    "3. **Music and Intro/Outro**: Adding theme music at the beginning and end would enhance the professional feel of the podcasts.\n",
    "\n",
    "4. **Display Estimated Time Earlier**: The system currently calculates processing time estimates but displays them later than ideal. Using Modal.Dict to share these estimates between modules would improve this.\n",
    "\n",
    "5. **Script Length Control**: Better enforcement of script length limits would prevent excessively long processing times.\n",
    "\n",
    "6. **Alternative TTS Models**: Testing newer models like MaskGCT could potentially improve quality and generation speed.\n",
    "\n",
    "7. **YouTube Integration**: Adding YouTube download capabilities would expand input options (though this would require a Modal Labs 'Team' subscription for IP Proxy functionality).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Building an AI podcast generator presented unique challenges at the intersection of natural language processing, text-to-speech technology, and distributed computing. By leveraging Modal Labs for infrastructure, implementing a robust three-stage pipeline, and carefully addressing each technical challenge, we created a system that produces surprisingly natural and engaging podcasts.\n",
    "\n",
    "The key ingredients for success were:\n",
    "1. A thoughtful architecture that separates concerns\n",
    "2. Careful prompt engineering for high-quality script generation\n",
    "3. Sophisticated audio generation that maintains voice consistency\n",
    "4. Robust status tracking and user experience design\n",
    "\n",
    "This project demonstrates that with the right approach, complex AI applications can be built in a modular, scalable, and maintainable way. The techniques and patterns we've shared can be applied to a wide range of AI applications beyond podcast generation.\n",
    "\n",
    "If you're interested in building similar systems, the Llama cookbook's NotebookLlama example and Bark's long-form generation notebook are excellent starting points. Combined with Modal Labs' serverless infrastructure, they provide a powerful foundation for creating production-ready AI applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
